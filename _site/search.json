[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nA Biostatistics PhD Application Notebook [with Statement of Purpose]\n\n\n\n\n\nAug 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnnotated Forest Plots using ggplot2\n\n\n\n\n\nDec 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Statistical Intuition for Optimal Treatment Rules\n\n\n\n\n\nAug 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nJust my type! A beginner’s guide to building a custom mechanical keyboard\n\n\n\n\n\nJun 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing ggplot2 to create Treatment Timelines with Multiple Variables\n\n\n\n\n\nJun 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn Illustrated Guide to Modified Treatment Policies, Part 1: Introduction and Motivation\n\n\n\n\n\nApr 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow many ways are there to get a SOFA Score of 10?\n\n\n\n\n\nFeb 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nBecoming a Biostatistician: FAQs for ‘A Day in the Life of a Biostatistician’\n\n\n\n\n\nJan 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn Illustrated Guide to TMLE, Part III: Properties, Theory, and Learning More\n\n\n\n\n\nDec 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn Illustrated Guide to TMLE, Part II: The Algorithm\n\n\n\n\n\nDec 11, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn Illustrated Guide to TMLE, Part I: Introduction and Motivation\n\n\n\n\n\nDec 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nBecome a Superlearner! An Illustrated Guide to Superlearning\n\n\n\n\n\nOct 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Sidelines: NYC’s COVID-19 Outbreak from the Eyes of a Pulmonary and Critical Care Team’s Biostatistician\n\n\n\n\n\nOct 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomizable correlation plots in R\n\n\n\n\n\nAug 24, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nRethinking Conditional and Iterated Expectations as Linear Regression Models\n\n\n\n\n\nAug 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nSilver Linings: five coding tricks learned during Lockdown\n\n\n\n\n\nJul 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nPatient Treatment Timelines for Longitudinal Survival Data\n\n\n\n\n\nNov 28, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nStats on Drugs: An Interview with a Pharmaceutical CRO Biostatistician\n\n\n\n\n\nOct 6, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing {sl3} for superlearning\n\n\n\n\n\nSep 12, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips and Tricks from the 2019 New York R Conference\n\n\n\n\n\n\nJun 10, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Day in the Life of a Biostatistician\n\n\n\n\n\n\nApr 16, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Condensed Key for A Visual Guide to Targeted Maximum Likelihood Estimation (TMLE)\n\n\n\n\n\n\nJan 9, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pubs/pubs_methods.html",
    "href": "pubs/pubs_methods.html",
    "title": "Methods",
    "section": "",
    "text": "Statistical and Epidemiological Research Methods Manuscripts and Pre-prints\n\n1. Katherine L. Hoffman, Edward J. Schenck, Michael J. Satlin, William Whalen, Di Pan, Nicholas Williams, Iván Díaz. Comparison of a Target Trial Emulation Framework vs Cox Regression to Estimate the Association of Corticosteroids With COVID-19 Mortality. JAMA Network Open. 2022;5(10):e2234425-e2234425. doi:10.1001/jamanetworkopen.2022.34425\n\n\n2. Katherine L Hoffman, Diego Salazar-Barreto, Nicholas Williams, Kara E Rudolph, Iván Dı́az. Introducing longitudinal modified treatment policies: A unified framework for studying complex exposures. arXiv preprint arXiv:230409460. Published online 2023. https://arxiv.org/pdf/2304.09460.pdf\n\n\n3. Iván Díaz, Katherine L Hoffman, Nima S. Hejazi. Causal survival analysis under competing risks using longitudinal modified treatment policies. Published online 2022. doi:10.48550/ARXIV.2202.03513\n\n\n4. Iván Díaz, Nicholas Williams, Katherine L. Hoffman, Edward J. Schenck. Nonparametric Causal Effects Based on Longitudinal Modified Treatment Policies. Journal of the American Statistical Association. Published online September 2021:1-16. doi:10.1080/01621459.2021.1955691\n\n\n5. Brian Gilbert, Katherine L. Hoffman, Nicholas Williams, Kara E. Rudolph, Edward J. Schenck, Iván Díaz. Identification and estimation of mediational effects of longitudinal modified treatment policies. Published online 2024. https://arxiv.org/abs/2403.09928",
    "crumbs": [
      "Statistics Methods"
    ]
  },
  {
    "objectID": "blog/trt-timelines/multiple-vars.html",
    "href": "blog/trt-timelines/multiple-vars.html",
    "title": "Using ggplot2 to create Treatment Timelines with Multiple Variables",
    "section": "",
    "text": "This post walks through code to create a timeline in R using ggplot2. These types of plots can help visualize treatment or measurement patterns, time-varying covariates, outcomes, and loss to follow-up in longitudinal data settings.\nYou can view a corresponding slide deck I made with {flipbookr} for the 2022 R/Medicine [here], or you can skip to the end of this post to just see the code.\n\nBackground\nTreatment timelines, or “swimmer plots”, are a visualization technique I find useful in exploring longitudinal data structures. A few years ago I shared how I make treatment timelines for a single treatment (categorical or continuous) in the post Patient Treatment Timelines for Longitudinal Survival Data.\nSometimes when I share these plots with collaborators, they ask me to add additional variables to the timelines. This post shows how to do that.\nI’ll use a toy dataset on hospitalized COVID-19 patients, available to download on this Github repository. It is derived from a dataset from Electronic Health Record data during Spring 2020. This is a time period when there was large variation in provider practice in administering steroids, a type of drug that combats hyper-inflammation. Steroids are usually given to patients which exhibit an inflammatory profile; we will identify this using a threshold for low oxygen levels (severe hypoxia).\nWe will look at the treatment patterns of steroids as it relates to the timing of patients (1) reaching severe hypoxia and (2) being put on a ventilator (intubation). We will also include whether patients died. I used a similar figure in a recent manuscript on this topic, if you’re interested in learning more!\n\n\nExploring the data\nThe data set is in long format with one row per patient. Let’s first load the data set and libraries we’ll need, then look at the first 20 rows:\n# install.packages(c(\"tidyverse\",\"gt\",\"RCurl\",\"rmarkdown\"))\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(rmarkdown)\n\ndat_long &lt;- read_csv(\"https://raw.githubusercontent.com/kathoffman/steroids-trial-emulation/main/data/dat_trt_timeline.csv\", col_types = list(id  = \"c\", steroids = \"c\", death = \"c\", severe = \"c\"))\nIf we look at the first patient (id = 797), we can see they were in the hospital for 17 days, never intubated, never receive steroids, and ultimately die (death is 1 on the last day).\nhead(dat_long) |&gt;\n  paged_table()\n\n\n  \n\n\nWe can plot all patients’ hospital length of stay, colored by intubation status using ggplot2’s geom_line():\ndat_long |&gt;\n  ggplot(aes(x=day, y=id, col = intubation_status, group=id)) +\n  geom_line() +\n  theme_bw()\n\nWe can add our steroids column to the plot by adding a point designating whether steroids exposure was 1 (yes) or 0 (no) that day. We can see this results in points of two different colors on the lines of our plot. This can work just fine! …unless you want to add another variable to the timeline.\ndat_long |&gt;\n  ggplot(aes(x=day, y=id, col = intubation_status, group=id)) +\n  geom_point(aes(day, id, col = steroids)) +\n  geom_line() +\n  theme_bw() \n\n\n\nModify the data\nWe could edit the colors of the dots we don’t want so that they’re transparent (using NA), but when you have other non-mutually exclusive dots you want to show, it’s simpler to just edit the data instead. So, we will now edit our data so that our three binary columns are turned into three *_this_day column, where:\n\nThe value is NA if the observation did not experience that exposure/outcome that day (remember each day is a new row)\nThe value is the day if the observation did experience the exposure/outcome. This is to make our x axis easy to specify in ggplot2.\n\ndat_swim &lt;-\n  dat_long |&gt;\n  mutate(severe_this_day = case_when(severe == 1 ~ day),\n         steroids_this_day = case_when(steroids == 1 ~ day),\n         death_this_day = case_when(death == 1 ~ day))\nWhile we’re at it, let’s modify the patient’s IDs so that we can rearrange our plot by length of each individual’s timeline. To do this, we will reorder the factored id variable by a new variable max_day, or the length of time that patients are in the study.\ndat_swim &lt;- \n  dat_swim |&gt;\n  group_by(id) |&gt;\n  mutate(max_day = max(day)) |&gt;\n  ungroup() |&gt;\n  mutate(id = fct_reorder(factor(id), max_day))\n\nhead(dat_swim) |&gt; paged_table()\n\n\n  \n\n\n\n\nBack to plotting\nNow, we can re-plot the steroids and intubation statuses using our new data. This time without all the 0 values for steroids showing.\ndat_swim |&gt; \n  ggplot() +\n  geom_line(aes(x=day, y=id, col = intubation_status, group=id)) +\n  geom_point(aes(x=steroids_this_day, y=id, col=\"Steroids\", shape=\"Steroids\")) +\n  theme_bw()\nWarning: Removed 387 rows containing missing values (`geom_point()`).\n\nFrom this point on I’ll save the plot as p and just keep adding onto it so you can see the new step.\nWe’ll see why we’re doing this in a second, but in creating this first iteration of p using geom_line() and geom_point(), we also want to set the col to match how we want the marker for steroids to appear in the legend. I’m also going to make minor edits to the size and shape of the point geometry right now, as well as the width of each timeline itself (also using the size argument).\np &lt;- \n  dat_swim |&gt; \n  ggplot() +\n  geom_line(aes(x=day, y=id, col = intubation_status, group=id),\n            size=1.8) +\n  geom_point(aes(x=steroids_this_day, y=id, col=\"Steroids\"), stroke=2, shape=15) +\n  theme_bw()\np\nWarning: Removed 387 rows containing missing values (`geom_point()`).\n\nLet’s add hypoxia and death to the figure. We’ll use geom_point() again, and again specify legend names for the col and shape arguments, and modify the size and stroke of our point geometries.\np &lt;- p +\n  geom_point(aes(x=severe_this_day, y=id, col=\"Severe hypoxia\"), size=2, stroke=1.5, shape=21) +\n  geom_point(aes(x=death_this_day, y=id, col=\"Death\"), size=2, stroke=1.5, shape=4) \np\nWarning: Removed 387 rows containing missing values (`geom_point()`).\nWarning: Removed 403 rows containing missing values (`geom_point()`).\nWarning: Removed 414 rows containing missing values (`geom_point()`).\n\nNote that we get warning messages that values with NA are removed. This is fine since we just created all those NAs! I’m going to set my options so that warnings are suppressed for future code outputs to keep this post tidy.\nknitr::opts_chunk$set(message=F, warning=F)\n\n\nModify the colors and shapes\nNext let’s start changing our color and shape scales. We can change colors using scale_color_manual() and and filling in the values argument with a vector where the names of the vector match the names in the col in our geom_point() aesthetics.\nI define my cols in a vector outside the plotting code to keep everything cleaner. Note that the order we’re specifying here will continue throughout the rest of the plotting code!\n# define colors for all geometries with a color argument\ncols &lt;- c(\"Severe hypoxia\" = \"#b24745\", # red\n          \"Intubated\" = \"#483d8b\", # navy\n          \"Not intubated\" = \"#74aaff\", # lighter blue\n          \"Steroids\"=\"#ffd966\", # gold\n          \"Death\" = \"#000000\") # black \nAfter we set values = cols, the name argument is simply the title we want for our legend (I chose “Patient Status”).\np &lt;- p +\n  scale_color_manual(values = cols, name=\"Patient Status\") \np\n\n\n\nFix the Legend\nYou’ll notice that our legend does not match the changes we made to the shapes, size, or linetype right now. This is because our legend only contains information on the colors, because that’s all we’re mapping to the aesthetics of our geometries right now (in geom_point and geom_line. We can override the color legend aesthetics and still create a plot that shows correct and useful information.\nWe will do this by using the guides() function. We can control each aesthetic here. We will first override the colors legend with the code guide_legend(override.aes = list(...)).\nThis allows us to change the shapes of the color legend by specifying a vector with the shapes, size, and line types we want in the order the labels appear in the legend. If we don’t want a characteristic to appear on the legend, we will use NA.\nI only want to show shapes for certain statuses (severe hypoxia, steroid administration, death), and not the intubation status of a patient, so I’ll set up my shape override vector accordingly. Note that the order in the legend follows the order of my color specification vector (cols).\nshape_override &lt;- c(21, NA, NA, 15, 4) # order matches `cols`:severe, intubation (yes/no), steroids, death\n\np +\n  guides(color = guide_legend(\n                      override.aes = list(\n                          shape = shape_override) # modify the color legend to include shapes\n                      )\n         ) \n\nTo remove the line through Death, Severe hypoxia, and Steroids in our legend, we can override the aesthetics for linetype with NA’s for those three labels. We will specify the default, linetype=1, for our intubation status color labels.\nWe can additionally override the stroke and size arguments to correspond to our point geometries.\nline_override &lt;- c(NA,1,1,NA,NA) # order matches `cols`:severe, intubation (yes/no), steroids, death\nstroke_override &lt;- c(1.2,1,1,1,1.4) # order matches `cols`:severe, intubation (yes/no), steroids, death\nsize_override &lt;- c(2.5,2.5,2.6,2.5,2) # order matches `cols`:severe, intubation (yes/no), steroids, death\n\np &lt;-\n  p +\n    guides(color = guide_legend(\n                        override.aes = list(\n                                stroke = stroke_override,\n                                shape = shape_override,\n                                linetype = line_override,\n                                size = size_override)\n                                )\n             )\np\n\nOk, the challenging parts are done! Now we can make some minor aesthetic edits using labs, scale_x_continuous(), and theme(). I won’t go into detail on these edits because they’re fairly self-explanatory, but check out the help files if you’re unsure what these arguments in theme do!\np &lt;- p +\n  labs(x=\"Days since hospitalization\",y=\"Patient\\nnumber\",title=\"Treatment Timeline for N=30 Patients\") +\n  scale_x_continuous(expand=c(0,0)) + # remove extra white space \n  theme(# text=element_text(family=\"Poppins\", size=11),\n        title = element_text(angle = 0, vjust=.5, size=12, face=\"bold\"),\n        axis.title.y = element_text(angle = 0, vjust=.5, size=12, face=\"bold\"),\n        axis.title.x = element_text(size=15, face=\"bold\", vjust=-0.5, hjust=0),\n        axis.text.y = element_text(size=6, hjust=1.5),\n        axis.ticks.y = element_blank(),\n        legend.position = c(0.8, 0.3),\n        legend.title = element_text(colour=\"black\", size=13, face=4),\n        legend.text = element_text(colour=\"black\", size=10),\n        legend.background = element_rect(size=0.5, linetype=\"solid\", colour =\"gray30\"),\n        panel.grid.minor = element_blank(),\n        panel.grid.major.x = element_blank()\n  ) \np\n\nHope this is helpful! As always let me know if you have any feedback or suggestions. If you’d like to copy-paste the code, here it is:\n\n\nJust the Code\nlibrary(tidyverse)\n\ndat_long &lt;- read_csv(\"https://raw.githubusercontent.com/kathoffman/steroids-trial-emulation/main/data/dat_trt_timeline.csv\", col_types = list(id  = \"c\", steroids = \"c\", death = \"c\", severe = \"c\"))\n\n# define colors for all geometries with a color argument\ncols &lt;- c(\"Severe hypoxia\" = \"#b24745\", # red\n          \"Intubated\" = \"#483d8b\", # navy\n          \"Not intubated\" = \"#74aaff\", # lighter blue\n          \"Steroids\"=\"#ffd966\", # gold\n          \"Death\" = \"#000000\") # black \n\nshape_override &lt;- c(21, NA, NA, 15, 4) # order matches `cols`:severe, intubation (yes/no), steroids, death\nline_override &lt;- c(NA,1,1,NA,NA) # order matches `cols`:severe, intubation (yes/no), steroids, death\nstroke_override &lt;- c(1.2,1,1,1,1.4) # order matches `cols`:severe, intubation (yes/no), steroids, death\nsize_override &lt;- c(2.5,2.5,2.6,2.5,2) # order matches `cols`:severe, intubation (yes/no), steroids, death\n\n# modify swimmer data to 1) only show events if yes 2) have an id ordered by max follow up\ndat_swim &lt;- \n   dat_long |&gt;\n  mutate(severe_this_day = case_when(severe == 1 ~ day),\n         steroids_this_day = case_when(steroids == 1 ~ day),\n         death_this_day = case_when(death == 1 ~ day)) |&gt;\n  group_by(id) |&gt;\n  mutate(max_day = max(day)) |&gt;\n  ungroup() |&gt;\n  mutate(id = fct_reorder(factor(id), max_day))\n\ndat_swim |&gt;\n  ggplot() +\n  geom_line(aes(x=day, y=id, col = intubation_status, group=id),\n            size=1.8) +\n  geom_point(aes(x=steroids_this_day, y=id, col=\"Steroids\"), shape=15, stroke=2) +\n  geom_point(aes(x=severe_this_day, y=id, col=\"Severe hypoxia\"), size=2, stroke=1.5, shape=21) +\n  geom_point(aes(x=death_this_day, y=id, col=\"Death\"), size=2, stroke=1.5, shape=4) +\n  theme_bw() +\n  scale_color_manual(values = cols, name=\"Patient Status\") +\n  guides(color = guide_legend(\n                        override.aes = list(\n                                stroke = stroke_override,\n                                shape = shape_override,\n                                linetype = line_override,\n                                size = size_override)\n                                )\n             )+\n  labs(x=\"Days since hospitalization\",y=\"Patient\\nnumber\",title=\"Treatment Timeline for N=30 Patients\") +\n  scale_x_continuous(expand=c(0,0)) + # remove extra white space \n  theme(# text=element_text(family=\"Poppins\", size=11),\n        title = element_text(angle = 0, vjust=.5, size=12, face=\"bold\"),\n        axis.title.y = element_text(angle = 0, vjust=.5, size=12, face=\"bold\"),\n        axis.title.x = element_text(size=15, face=\"bold\", vjust=-0.5, hjust=0),\n        axis.text.y = element_text(size=6, hjust=1.5),\n        axis.ticks.y = element_blank(),\n        legend.position = c(0.8, 0.3),\n        legend.title = element_text(colour=\"black\", size=13, face=4),\n        legend.text = element_text(colour=\"black\", size=10),\n        legend.background = element_rect(size=0.5, linetype=\"solid\", colour =\"gray30\"),\n        panel.grid.minor = element_blank(),\n        panel.grid.major.x = element_blank()\n  ) \n\n\nSession Info\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16)\nPlatform: x86_64-apple-darwin20 (64-bit)\nRunning under: macOS Big Sur 11.7\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Detroit\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] rmarkdown_2.23  gt_0.9.0        lubridate_1.9.2 forcats_1.0.0  \n [5] stringr_1.5.0   dplyr_1.1.2     purrr_1.0.1     readr_2.1.4    \n [9] tidyr_1.3.0     tibble_3.2.1    ggplot2_3.4.2   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.3        generics_0.1.3    xml2_1.3.5        stringi_1.7.12   \n [5] hms_1.1.3         digest_0.6.33     magrittr_2.0.3    evaluate_0.21    \n [9] grid_4.3.1        timechange_0.2.0  fastmap_1.1.1     jsonlite_1.8.7   \n[13] fansi_1.0.4       scales_1.2.1      cli_3.6.1         rlang_1.1.1      \n[17] crayon_1.5.2      bit64_4.0.5       munsell_0.5.0     withr_2.5.0      \n[21] yaml_2.3.7        tools_4.3.1       parallel_4.3.1    tzdb_0.4.0       \n[25] colorspace_2.1-0  curl_5.0.1        vctrs_0.6.3       R6_2.5.1         \n[29] lifecycle_1.0.3   htmlwidgets_1.6.2 bit_4.0.5         vroom_1.6.3      \n[33] pkgconfig_2.0.3   pillar_1.9.0      gtable_0.3.3      glue_1.6.2       \n[37] xfun_0.39         tidyselect_1.2.0  rstudioapi_0.14   knitr_1.43       \n[41] farver_2.1.1      htmltools_0.5.5   labeling_0.4.2    compiler_4.3.1"
  },
  {
    "objectID": "blog/keyboard/index.html",
    "href": "blog/keyboard/index.html",
    "title": "Just my type! A beginner’s guide to building a custom mechanical keyboard",
    "section": "",
    "text": "This blog post details a DIY project I did last summer – building a mechanical keyboard. I’m no expert, but I enjoyed the process of building the keyboard so much that I thought I would briefly walk through the steps and share helpful resources. Even if you have no interest in building your own, I hope it encourages you to learn something new that you may have initially felt intimidated by!"
  },
  {
    "objectID": "blog/keyboard/index.html#case",
    "href": "blog/keyboard/index.html#case",
    "title": "Just my type! A beginner’s guide to building a custom mechanical keyboard",
    "section": "Case",
    "text": "Case\nThe case is the outside of your keyboard. The size of the case you choose will limit how many keys you can have on your keyboard. Case sizes are described in percentages, i.e. 100%, 80%, 75%, 60%, 40%, which corresponds to how many keys fit on the board."
  },
  {
    "objectID": "blog/keyboard/index.html#pcb",
    "href": "blog/keyboard/index.html#pcb",
    "title": "Just my type! A beginner’s guide to building a custom mechanical keyboard",
    "section": "PCB",
    "text": "PCB\nThe Printed Circuit Board (PCB) is the electronic circuitry of your keyboard. You will need to choose a PCB that fits into whatever size case you’ve picked. One option with a PCB is whether it has RGB underglow on it, which means it has tiny lights on it that you can program to display different colors. If you want these lights to show, you should find a case and/or key caps with some aspect of transparency.\nAnother option for the PCB is whether you attach the keys via “hot swap” or “soldering”. If you choose hot swap, you just click the keys into place. However, most PCBs require you to solder the key switches (discussed below) to the circuitry on the board."
  },
  {
    "objectID": "blog/keyboard/index.html#keyboard-plate",
    "href": "blog/keyboard/index.html#keyboard-plate",
    "title": "Just my type! A beginner’s guide to building a custom mechanical keyboard",
    "section": "Keyboard plate",
    "text": "Keyboard plate\nKeyboard plates are what you put on top of PCBs to hold your keys in place. You can choose from aluminum, brass, or steel. If you’re feeling overwhelmed, this isn’t something you should stress about. You can barely see it below the keys, so color doesn’t matter much, and the differences in sound/material from the material seem minimal to me. You should make sure to get one that matches the layout of the keys that you want, though."
  },
  {
    "objectID": "blog/keyboard/index.html#key-switches",
    "href": "blog/keyboard/index.html#key-switches",
    "title": "Just my type! A beginner’s guide to building a custom mechanical keyboard",
    "section": "Key switches",
    "text": "Key switches\nThis is a fun choice! Key switches are what go below the keys to provide the majority of sound and feel. You can choose from many different brands, e.g. Cherry MX or Gateron, and types of switch feels, e.g. tactile, linear, clicky. I won’t go into all the differences here, but you should know that switch colors correspond to certain sounds/feels. The color of the switch won’t be seen unless you have translucent key caps. I discovered the world of typing test videos while I was picking out key switches."
  },
  {
    "objectID": "blog/keyboard/index.html#key-stabilizers",
    "href": "blog/keyboard/index.html#key-stabilizers",
    "title": "Just my type! A beginner’s guide to building a custom mechanical keyboard",
    "section": "Key stabilizers",
    "text": "Key stabilizers\nKey stabilizers are necessary but a bit boring. They go beneath the wider keys like space bar and shift to help distribute the weight of your finger to press the button. It is common to lubricate the key stabilizers before you install them so that these keys are easier to press."
  },
  {
    "objectID": "blog/keyboard/index.html#key-caps",
    "href": "blog/keyboard/index.html#key-caps",
    "title": "Just my type! A beginner’s guide to building a custom mechanical keyboard",
    "section": "Key caps",
    "text": "Key caps\nKey caps are, in my opinion, the most exciting part! They go on top of the switches and are the most visible part of the keyboard. You can choose the material and colors, and either buy as a set or pick out unique custom keycaps."
  },
  {
    "objectID": "blog/keyboard/index.html#soldering-equipment",
    "href": "blog/keyboard/index.html#soldering-equipment",
    "title": "Just my type! A beginner’s guide to building a custom mechanical keyboard",
    "section": "Soldering equipment",
    "text": "Soldering equipment\nUnless you get a hot swap PCB, you’ll need a soldering iron, solder wire, something to remove excess solder (i.e. solder wick or sucker)."
  },
  {
    "objectID": "blog/keyboard/index.html#test-your-pcb",
    "href": "blog/keyboard/index.html#test-your-pcb",
    "title": "Just my type! A beginner’s guide to building a custom mechanical keyboard",
    "section": "1. Test your PCB",
    "text": "1. Test your PCB\nFirst you’ll want to download a keyboard programming app like VIA, plug your keyboard into your computer, take a pair of tweezers, and test each point of key circuitry on your PCB plate."
  },
  {
    "objectID": "blog/keyboard/index.html#lubricate-the-stabilizers",
    "href": "blog/keyboard/index.html#lubricate-the-stabilizers",
    "title": "Just my type! A beginner’s guide to building a custom mechanical keyboard",
    "section": "2. Lubricate the stabilizers",
    "text": "2. Lubricate the stabilizers\nIf your PCB plate is working properly, go ahead and add lubricant and put together your stabilizers. I found the whole process a bit tricky, but Youtube helped a lot. There are special lubricants made for keyboards, but I was impatient and used silicone lubricant that I already had."
  },
  {
    "objectID": "blog/keyboard/index.html#place-your-stabilizers-on-the-pcb",
    "href": "blog/keyboard/index.html#place-your-stabilizers-on-the-pcb",
    "title": "Just my type! A beginner’s guide to building a custom mechanical keyboard",
    "section": "3. Place your stabilizers on the PCB",
    "text": "3. Place your stabilizers on the PCB\nTime to click the stabilizers into place! You can put the respective keycaps on (space bar, shift, backspace, etc.) and make sure you like the feel at this point."
  },
  {
    "objectID": "blog/keyboard/index.html#add-the-plate-and-click-the-key-switches-into-the-pcb",
    "href": "blog/keyboard/index.html#add-the-plate-and-click-the-key-switches-into-the-pcb",
    "title": "Just my type! A beginner’s guide to building a custom mechanical keyboard",
    "section": "4. Add the plate and click the key switches into the PCB",
    "text": "4. Add the plate and click the key switches into the PCB\nYou can now overlay your plate on the PCB and start clicking each key switch’s metal prongs into the small holes on the PCB. You should feel a click when it locks into place, and the switches should not fall out if you flip the board over."
  },
  {
    "objectID": "blog/keyboard/index.html#soldering",
    "href": "blog/keyboard/index.html#soldering",
    "title": "Just my type! A beginner’s guide to building a custom mechanical keyboard",
    "section": "5. Soldering",
    "text": "5. Soldering\nThis was hands-down my favorite part of the entire process. Soldering (pronounced saw-der-ing) is SO much fun. There’s a ton of videos online about it, and you can buy a kit to practice if you want, but honestly I just went for it. You basically melt a piece of metal (solder) and then place the soldering iron right next to the key switch prongs and PCB ring of metal to create an electrical conduction.\n\n\nOne important note with soldering is that you should wait a day and retest your keys to make sure the conduction stayed. I had an issue where I would successfully test the keys about five minutes after soldering, but the next day the same keys would no longer work. I finally realized this was because I was soldering outside on a summer night (to avoid the fumes). As the sun went down, my soldering iron was no longer as hot as it said it was (~400° Fahrenheit) because the wind/outside air was cooling it down.\nThis is what my final soldered PCB and switches looked like. Each of the keys has two small mounds of solder creating an electrical conduction between the switch and the PCB. These look like small silver balls."
  },
  {
    "objectID": "blog/keyboard/index.html#stack-and-screw",
    "href": "blog/keyboard/index.html#stack-and-screw",
    "title": "Just my type! A beginner’s guide to building a custom mechanical keyboard",
    "section": "6. Stack and screw",
    "text": "6. Stack and screw\nOnce all your keys’ electrical connections work, you can add your key caps. Finally, stack all the pieces together (case, foam plate, plate/PCB/switches/keys) and tighten the necessary screws."
  },
  {
    "objectID": "blog/keyboard/index.html#type-away",
    "href": "blog/keyboard/index.html#type-away",
    "title": "Just my type! A beginner’s guide to building a custom mechanical keyboard",
    "section": "7. Type away",
    "text": "7. Type away\nEnjoy the click-clack of your brand new custom mechanical keyboard!"
  },
  {
    "objectID": "blog/corr-plots/index.html",
    "href": "blog/corr-plots/index.html",
    "title": "Customizable correlation plots in R",
    "section": "",
    "text": "Updated to include gt package August 23, 2022.\nIf you’re ever felt limited by correlogram packages in R, this post will show you how to write your own function to tidy the many correlations into a ggplot2-friendly form for plotting.\n\nBy the end of this post, you will be able to run one function to get a tidied data frame of correlations:\nformatted_cors(mtcars) %&gt;% head() %&gt;% gt()\n\n\n\n\n\n\n\nmeasure1\nmeasure2\nr\nn\np\nsig_p\np_if_sig\nr_if_sig\n\n\n\n\nmpg\nmpg\n1.0000000\n32\nNA\nNA\nNA\nNA\n\n\nmpg\ncyl\n-0.8521620\n32\n6.112688e-10\nTRUE\n6.112688e-10\n-0.8521620\n\n\nmpg\ndisp\n-0.8475514\n32\n9.380328e-10\nTRUE\n9.380328e-10\n-0.8475514\n\n\nmpg\nhp\n-0.7761684\n32\n1.787835e-07\nTRUE\n1.787835e-07\n-0.7761684\n\n\nmpg\ndrat\n0.6811719\n32\n1.776240e-05\nTRUE\n1.776240e-05\n0.6811719\n\n\nmpg\nwt\n-0.8676594\n32\n1.293958e-10\nTRUE\n1.293958e-10\n-0.8676594\n\n\n\n\n\n\nYou can then run ggplot2 code on this data to make your own correlation heat maps.\n\n\n\n\n\n\n\nIf you just want the code, skip to the end.\n\n\nLess-customizable options\nI really appreciate some of the packages and functions that allow me to make correlation plots super quickly using R. Here are a few examples:\ncorrplot::corrplot(cor(mtcars))\n\n\n\n\n\n\n\ncorrgram::corrgram(mtcars)\n\n\n\n\n\n\n\nggcorrplot::ggcorrplot(cor(mtcars))\n\n\n\n\n\n\n\nAll of these are nice, but none of them are ultimately as customizable as I need them to be. I’ll next show how you can bypass using someone else’s function constraints to prepare correlations in your data in a ggplot2-friendly format.\n\n\nGetting the correlations\nWe could use the base R function cor() to get our correlations, but I do not like the defaults for missing data. Instead, I use Frank Harrell’s Hmisc::rcorr() function for two reasons:\n\nit drops missing pairs as the default\nit returns p-values, so you only need one function to get both the correlation coefficient and matching p-value\n\nLet’s load the libraries we’ll need for this, which are knitr for showing tables using gt, and tidyverse (we’ll specifically use tidyr, dplyr, ggplot2, tibble and purrr).\nlibrary(knitr)\nlibrary(gt)\nlibrary(tidyverse, warn.conflict=F)\nFirst, let’s look at our output from our correlation function we’ll use, Hmisc::rcorr(). It requires the input to be a matrix, and outputs a list of three matrices.\nmtcars_cor &lt;- Hmisc::rcorr(as.matrix(mtcars))\nThese three matrices include the correlation coefficient (default is Pearson’s), r, the p-value, P, and the number of observations used for each correlation, n. Let’s turn each matrix into a data frame and look at the top six rows with head and gt.\nThe correlation coefficients, r:\ndata.frame(mtcars_cor$r) %&gt;% head() %&gt;% gt()\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n1.0000000\n-0.8521620\n-0.8475514\n-0.7761684\n0.6811719\n-0.8676594\n0.41868403\n0.6640389\n0.5998324\n0.4802848\n-0.5509251\n\n\n-0.8521620\n1.0000000\n0.9020329\n0.8324475\n-0.6999381\n0.7824958\n-0.59124207\n-0.8108118\n-0.5226070\n-0.4926866\n0.5269883\n\n\n-0.8475514\n0.9020329\n1.0000000\n0.7909486\n-0.7102139\n0.8879799\n-0.43369788\n-0.7104159\n-0.5912270\n-0.5555692\n0.3949769\n\n\n-0.7761684\n0.8324475\n0.7909486\n1.0000000\n-0.4487591\n0.6587479\n-0.70822339\n-0.7230967\n-0.2432043\n-0.1257043\n0.7498125\n\n\n0.6811719\n-0.6999381\n-0.7102139\n-0.4487591\n1.0000000\n-0.7124406\n0.09120476\n0.4402785\n0.7127111\n0.6996101\n-0.0907898\n\n\n-0.8676594\n0.7824958\n0.8879799\n0.6587479\n-0.7124406\n1.0000000\n-0.17471588\n-0.5549157\n-0.6924953\n-0.5832870\n0.4276059\n\n\n\n\n\n\nThe p-values, P:\ndata.frame(mtcars_cor$P) %&gt;% head() %&gt;% gt()\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nNA\n6.112688e-10\n9.380328e-10\n1.787835e-07\n1.776240e-05\n1.293958e-10\n1.708199e-02\n3.415937e-05\n2.850207e-04\n5.400948e-03\n1.084446e-03\n\n\n6.112688e-10\nNA\n1.803002e-12\n3.477861e-09\n8.244636e-06\n1.217567e-07\n3.660533e-04\n1.843018e-08\n2.151207e-03\n4.173297e-03\n1.942340e-03\n\n\n9.380328e-10\n1.803002e-12\nNA\n7.142679e-08\n5.282022e-06\n1.222311e-11\n1.314404e-02\n5.235012e-06\n3.662114e-04\n9.635921e-04\n2.526789e-02\n\n\n1.787835e-07\n3.477861e-09\n7.142679e-08\nNA\n9.988772e-03\n4.145827e-05\n5.766253e-06\n2.940896e-06\n1.798309e-01\n4.930119e-01\n7.827810e-07\n\n\n1.776240e-05\n8.244636e-06\n5.282022e-06\n9.988772e-03\nNA\n4.784260e-06\n6.195826e-01\n1.167553e-02\n4.726790e-06\n8.360110e-06\n6.211834e-01\n\n\n1.293958e-10\n1.217567e-07\n1.222311e-11\n4.145827e-05\n4.784260e-06\nNA\n3.388683e-01\n9.798492e-04\n1.125440e-05\n4.586601e-04\n1.463861e-02\n\n\n\n\n\n\nThe number of observations, n. There are no missing data in the mtcars data set so there are 32 pairs used for all correlations.\ndata.frame(mtcars_cor$n) %&gt;% head(n=3) %&gt;% gt()\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n\n\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n\n\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\n\n\n\n\n\n\nNext we can write a function that formats a data frame correctly for Hmisc::rcorr() and then turns each of the three elements of the list (r,n and P)\ncors &lt;- function(df) {\n  M &lt;- Hmisc::rcorr(as.matrix(df))\n  # turn all three matrices (r, n, and P into a data frame)\n  Mdf &lt;- map(M, ~data.frame(.x))\n  # return the three data frames in a list\n  return(Mdf)\n}\nNothing too crazy happened in this function. Now we just have a list of three data frames. We can look at the the first element of our list using first(), which shows us the correlations between all our variables:\ncors(mtcars) %&gt;% first() %&gt;% head() %&gt;% gt()\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n1.0000000\n-0.8521620\n-0.8475514\n-0.7761684\n0.6811719\n-0.8676594\n0.41868403\n0.6640389\n0.5998324\n0.4802848\n-0.5509251\n\n\n-0.8521620\n1.0000000\n0.9020329\n0.8324475\n-0.6999381\n0.7824958\n-0.59124207\n-0.8108118\n-0.5226070\n-0.4926866\n0.5269883\n\n\n-0.8475514\n0.9020329\n1.0000000\n0.7909486\n-0.7102139\n0.8879799\n-0.43369788\n-0.7104159\n-0.5912270\n-0.5555692\n0.3949769\n\n\n-0.7761684\n0.8324475\n0.7909486\n1.0000000\n-0.4487591\n0.6587479\n-0.70822339\n-0.7230967\n-0.2432043\n-0.1257043\n0.7498125\n\n\n0.6811719\n-0.6999381\n-0.7102139\n-0.4487591\n1.0000000\n-0.7124406\n0.09120476\n0.4402785\n0.7127111\n0.6996101\n-0.0907898\n\n\n-0.8676594\n0.7824958\n0.8879799\n0.6587479\n-0.7124406\n1.0000000\n-0.17471588\n-0.5549157\n-0.6924953\n-0.5832870\n0.4276059\n\n\n\n\n\n\n\n\nPrep the correlations for ggplot2\nThe next step is to get the data ready for plotting with ggplot2. We can keep the data in a list for now and use the map() function from purrr.\nFirst, we need to move the rownames to their own column using tibble::rownames_to_column(). The output of that looks like:\ncors(mtcars) %&gt;%\n  map(~rownames_to_column(.x, var=\"measure1\")) %&gt;%\n  # look at the first element of the list (r)\n  first() %&gt;%\n  head() %&gt;%\n  gt()\n\n\n\n\n\n\n\nmeasure1\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nmpg\n1.0000000\n-0.8521620\n-0.8475514\n-0.7761684\n0.6811719\n-0.8676594\n0.41868403\n0.6640389\n0.5998324\n0.4802848\n-0.5509251\n\n\ncyl\n-0.8521620\n1.0000000\n0.9020329\n0.8324475\n-0.6999381\n0.7824958\n-0.59124207\n-0.8108118\n-0.5226070\n-0.4926866\n0.5269883\n\n\ndisp\n-0.8475514\n0.9020329\n1.0000000\n0.7909486\n-0.7102139\n0.8879799\n-0.43369788\n-0.7104159\n-0.5912270\n-0.5555692\n0.3949769\n\n\nhp\n-0.7761684\n0.8324475\n0.7909486\n1.0000000\n-0.4487591\n0.6587479\n-0.70822339\n-0.7230967\n-0.2432043\n-0.1257043\n0.7498125\n\n\ndrat\n0.6811719\n-0.6999381\n-0.7102139\n-0.4487591\n1.0000000\n-0.7124406\n0.09120476\n0.4402785\n0.7127111\n0.6996101\n-0.0907898\n\n\nwt\n-0.8676594\n0.7824958\n0.8879799\n0.6587479\n-0.7124406\n1.0000000\n-0.17471588\n-0.5549157\n-0.6924953\n-0.5832870\n0.4276059\n\n\n\n\n\n\nNext, we can turn move of the columns to a single column called measure2 using tidyr::pivot_longer()\ncors(mtcars) %&gt;%\n  map(~rownames_to_column(.x, var=\"measure1\")) %&gt;%\n  # format each data set (r,P,n) long\n  map(~pivot_longer(.x, -measure1, \"measure2\")) %&gt;%\n  # look at the first element of the list (r)\n  first() %&gt;%\n  head() %&gt;%\n  gt()\n\n\n\n\n\n\n\nmeasure1\nmeasure2\nvalue\n\n\n\n\nmpg\nmpg\n1.0000000\n\n\nmpg\ncyl\n-0.8521620\n\n\nmpg\ndisp\n-0.8475514\n\n\nmpg\nhp\n-0.7761684\n\n\nmpg\ndrat\n0.6811719\n\n\nmpg\nwt\n-0.8676594\n\n\n\n\n\n\nNow, we’re ready to unlist our data by using bind_rows(). This will turn our correlations into a very long data frame with all the rows from r, then n, then P.\ncors(mtcars) %&gt;%\n  map(~rownames_to_column(.x, var=\"measure1\")) %&gt;%\n  # format each data set (r,P,n) long\n  map(~pivot_longer(.x, -measure1, \"measure2\")) %&gt;%\n  # merge our three list elements by binding the rows\n  bind_rows(.id = \"id\") %&gt;%\n  head() %&gt;%\n  gt()\n\n\n\n\n\n\n\nid\nmeasure1\nmeasure2\nvalue\n\n\n\n\nr\nmpg\nmpg\n1.0000000\n\n\nr\nmpg\ncyl\n-0.8521620\n\n\nr\nmpg\ndisp\n-0.8475514\n\n\nr\nmpg\nhp\n-0.7761684\n\n\nr\nmpg\ndrat\n0.6811719\n\n\nr\nmpg\nwt\n-0.8676594\n\n\n\n\n\n\nFor ggplot2, we’ll need to have r, n, and p as their own column. We can use pivot_longer() to do this.\ncors(mtcars) %&gt;%\n  map(~rownames_to_column(.x, var=\"measure1\")) %&gt;%\n  # format each data set (r,P,n) long\n  map(~pivot_longer(.x, -measure1, \"measure2\")) %&gt;%\n  # merge our three list elements by binding the rows\n  bind_rows(.id = \"id\") %&gt;%\n  pivot_wider(names_from = id, values_from = value) %&gt;%\n  head() %&gt;%\n  gt()\n\n\n\n\n\n\n\nmeasure1\nmeasure2\nr\nn\nP\n\n\n\n\nmpg\nmpg\n1.0000000\n32\nNA\n\n\nmpg\ncyl\n-0.8521620\n32\n6.112688e-10\n\n\nmpg\ndisp\n-0.8475514\n32\n9.380328e-10\n\n\nmpg\nhp\n-0.7761684\n32\n1.787835e-07\n\n\nmpg\ndrat\n0.6811719\n32\n1.776240e-05\n\n\nmpg\nwt\n-0.8676594\n32\n1.293958e-10\n\n\n\n\n\n\nFinally, we can add a few columns that will potentially be useful later for making our correlation plots more informative. Let’s add columns that tell us whether the p-value was less than 0.05, and if so, give us back 1) the p-value and 2) the correlation coefficient, in case we want to label our plot with these values.\ncors(mtcars) %&gt;%\n  map(~rownames_to_column(.x, var=\"measure1\")) %&gt;%\n  # format each data set (r,P,n) long\n  map(~pivot_longer(.x, -measure1, \"measure2\")) %&gt;%\n  # merge our three list elements by binding the rows\n  bind_rows(.id = \"id\") %&gt;%\n  pivot_wider(names_from = id, values_from = value) %&gt;%\n  # change so everything is lower case\n  rename(p = P) %&gt;%\n  mutate(sig_p = ifelse(p &lt; .05, T, F),\n           p_if_sig = ifelse(sig_p, p, NA),\n           r_if_sig = ifelse(sig_p, r, NA)) %&gt;%\n  head() %&gt;% \n  gt()\n\n\n\n\n\n\n\nmeasure1\nmeasure2\nr\nn\np\nsig_p\np_if_sig\nr_if_sig\n\n\n\n\nmpg\nmpg\n1.0000000\n32\nNA\nNA\nNA\nNA\n\n\nmpg\ncyl\n-0.8521620\n32\n6.112688e-10\nTRUE\n6.112688e-10\n-0.8521620\n\n\nmpg\ndisp\n-0.8475514\n32\n9.380328e-10\nTRUE\n9.380328e-10\n-0.8475514\n\n\nmpg\nhp\n-0.7761684\n32\n1.787835e-07\nTRUE\n1.787835e-07\n-0.7761684\n\n\nmpg\ndrat\n0.6811719\n32\n1.776240e-05\nTRUE\n1.776240e-05\n0.6811719\n\n\nmpg\nwt\n-0.8676594\n32\n1.293958e-10\nTRUE\n1.293958e-10\n-0.8676594\n\n\n\n\n\n\nThis seems like everything I think I’ll ever ever want to plot. Of course you could add more. At this point I turned my formatted correlations into a function:\nformatted_cors &lt;- function(df){\n  cors(df) %&gt;%\n    map(~rownames_to_column(.x, var=\"measure1\")) %&gt;%\n    map(~pivot_longer(.x, -measure1, \"measure2\")) %&gt;%\n    bind_rows(.id = \"id\") %&gt;%\n    pivot_wider(names_from = id, values_from = value) %&gt;%\n    rename(p = P) %&gt;%\n    mutate(sig_p = ifelse(p &lt; .05, T, F),\n           p_if_sig = ifelse(sig_p, p, NA),\n           r_if_sig = ifelse(sig_p, r, NA)) \n}\nWe can test the function works as expected:\nformatted_cors(mtcars) %&gt;% head() %&gt;% gt()\n\n\n\n\n\n\n\nmeasure1\nmeasure2\nr\nn\np\nsig_p\np_if_sig\nr_if_sig\n\n\n\n\nmpg\nmpg\n1.0000000\n32\nNA\nNA\nNA\nNA\n\n\nmpg\ncyl\n-0.8521620\n32\n6.112688e-10\nTRUE\n6.112688e-10\n-0.8521620\n\n\nmpg\ndisp\n-0.8475514\n32\n9.380328e-10\nTRUE\n9.380328e-10\n-0.8475514\n\n\nmpg\nhp\n-0.7761684\n32\n1.787835e-07\nTRUE\n1.787835e-07\n-0.7761684\n\n\nmpg\ndrat\n0.6811719\n32\n1.776240e-05\nTRUE\n1.776240e-05\n0.6811719\n\n\nmpg\nwt\n-0.8676594\n32\n1.293958e-10\nTRUE\n1.293958e-10\n-0.8676594\n\n\n\n\n\n\n\n\nPlotting\nWe’re finally ready to plot our correlation heat maps in ggplot2.\nThe simplest form of this plot only requires us to specify measure1 and measure2 on the x and y-axis, respectively. Then we can map the correlation r to the fill aesthetic, and add a tile as the geometry.\nformatted_cors(mtcars) %&gt;%\n  ggplot(aes(x = measure1, y = measure2, fill = r)) +\n  geom_tile()\n\n\n\n\n\n\n\nWe can make some minor aesthetic changes, such as the fill coloring scale, titles, and font family.\nformatted_cors(mtcars) %&gt;%\n  ggplot(aes(x = measure1, y = measure2, fill = r)) +\n  geom_tile() +\n  labs(x = NULL, y = NULL, fill = \"Pearson's\\nCorrelation\", title=\"Correlations in Mtcars\") +\n  # map a red, white and blue color scale to correspond to -1:1 sequential gradient\n  scale_fill_gradient2(mid=\"#FBFEF9\",low=\"#0C6291\",high=\"#A63446\", limits=c(-1,1)) +\n  theme_classic() +\n  # remove excess space on x and y axes\n  scale_x_discrete(expand=c(0,0)) +\n  scale_y_discrete(expand=c(0,0)) +\n  # change global font to roboto\n  theme(text=element_text(family=\"Roboto\"))\n\n\n\n\n\n\n\nWe can add the correlations for extra information. For this particular plot, I only added significant (p-value less than 0.05) correlations, using the column r_if_sig that outputs from formatted_cors().\nformatted_cors(mtcars) %&gt;%\n  ggplot(aes(measure1, measure2, fill=r, label=round(r_if_sig,2))) +\n  geom_tile() +\n  labs(x = NULL, y = NULL, fill = \"Pearson's\\nCorrelation\", title=\"Correlations in Mtcars\",\n       subtitle=\"Only significant Pearson's correlation coefficients shown\") +\n  scale_fill_gradient2(mid=\"#FBFEF9\",low=\"#0C6291\",high=\"#A63446\", limits=c(-1,1)) +\n  geom_text() +\n  theme_classic() +\n  scale_x_discrete(expand=c(0,0)) +\n  scale_y_discrete(expand=c(0,0)) +\n  theme(text=element_text(family=\"Roboto\"))\n\n\n\n\n\n\n\nAnother version of this could involve squares with different sizes to denote strength of correlation using geom_point with shape set to a value from these available geom_shapes. Make sure you take the absolute value of the correlation so that strong negative correlations can also be denoted larger.\nformatted_cors(mtcars) %&gt;%\n  ggplot(aes(measure1, measure2, col=r)) + ## to get the rect filled\n  geom_tile(col=\"black\", fill=\"white\") +\n  geom_point(aes(size = abs(r)), shape=15) +\n  labs(x = NULL, y = NULL, col = \"Pearson's\\nCorrelation\", title=\"Correlations in Mtcars\") +\n  theme_classic()+\n  scale_color_gradient2(mid=\"#FBFEF9\",low=\"#0C6291\",high=\"#A63446\", limits=c(-1,1))  +\n  scale_x_discrete(expand=c(0,0)) +\n  scale_y_discrete(expand=c(0,0)) +\n  theme(text=element_text(family=\"Roboto\")) +\n  scale_size(range=c(1,11), guide=NULL) \n\n\n\n\n\n\n\nPlease feel free to reach out with questions or suggestions. Thank you to Elena Leib for spotting a minor bug in a previous version of this post!\n\n\nJust the code\ncors &lt;- function(df) {\n  M &lt;- Hmisc::rcorr(as.matrix(df))\n  Mdf &lt;- map(M, ~data.frame(.x))\n  return(Mdf)\n}\n\nformatted_cors &lt;- function(df){\n  cors(df) %&gt;%\n    map(~rownames_to_column(.x, var=\"measure1\")) %&gt;%\n    map(~pivot_longer(.x, -measure1, \"measure2\")) %&gt;%\n    bind_rows(.id = \"id\") %&gt;%\n    pivot_wider(names_from = id, values_from = value) %&gt;%\n    rename(p = P) %&gt;%\n    mutate(sig_p = ifelse(p &lt; .05, T, F),\n           p_if_sig = ifelse(sig_p, p, NA),\n           r_if_sig = ifelse(sig_p, r, NA)) \n}\n\nformatted_cors(mtcars) %&gt;%\n  ggplot(aes(measure1, measure2, fill=r, label=round(r_if_sig,2))) +\n  geom_tile() +\n  labs(x = NULL, y = NULL, fill = \"Pearson's\\nCorrelation\", title=\"Correlations in Mtcars\",\n       subtitle=\"Only significant Pearson's correlation coefficients shown\") +\n  scale_fill_gradient2(mid=\"#FBFEF9\",low=\"#0C6291\",high=\"#A63446\", limits=c(-1,1)) +\n  geom_text() +\n  theme_classic() +\n  scale_x_discrete(expand=c(0,0)) +\n  scale_y_discrete(expand=c(0,0)) +\n  theme(text=element_text(family=\"Roboto\"))\n\n\n\n\n\n\n\n\nSession info\nsessionInfo()\nR version 4.1.3 (2022-03-10) Platform: x86_64-apple-darwin17.0 (64-bit) Running under: macOS Catalina 10.15.7\nMatrix products: default BLAS: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\nlocale: [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\nattached base packages: [1] stats graphics grDevices utils datasets methods base\nother attached packages: [1] gt_0.6.0 knitr_1.38 forcats_0.5.1 stringr_1.4.1\n[5] dplyr_1.0.9 purrr_0.3.4 readr_2.1.2 tidyr_1.2.0\n[9] tibble_3.1.8 ggplot2_3.3.6 tidyverse_1.3.1\nloaded via a namespace (and not attached): [1] fs_1.5.2 lubridate_1.8.0 RColorBrewer_1.1-3 [4] httr_1.4.2 tools_4.1.3 backports_1.4.1\n[7] utf8_1.2.2 R6_2.5.1 rpart_4.1.16\n[10] Hmisc_4.7-0 DBI_1.1.2 colorspace_2.0-3\n[13] nnet_7.3-17 withr_2.5.0 tidyselect_1.1.2\n[16] gridExtra_2.3 cabinets_0.6.0 compiler_4.1.3\n[19] cli_3.3.0 rvest_1.0.2 htmlTable_2.4.0\n[22] xml2_1.3.3 labeling_0.4.2 sass_0.4.1\n[25] scales_1.2.1 checkmate_2.0.0 digest_0.6.29\n[28] foreign_0.8-82 rmarkdown_2.13 base64enc_0.1-3\n[31] jpeg_0.1-9 pkgconfig_2.0.3 htmltools_0.5.2\n[34] dbplyr_2.1.1 fastmap_1.1.0 htmlwidgets_1.5.4\n[37] rlang_1.0.4 readxl_1.4.0 rstudioapi_0.13\n[40] generics_0.1.3 farver_2.1.1 jsonlite_1.8.0\n[43] magrittr_2.0.3 Formula_1.2-4 Matrix_1.4-0\n[46] Rcpp_1.0.9 munsell_0.5.0 fansi_1.0.3\n[49] lifecycle_1.0.1 stringi_1.7.8 yaml_2.3.5\n[52] plyr_1.8.7 grid_4.1.3 crayon_1.5.1\n[55] lattice_0.20-45 haven_2.5.0 splines_4.1.3\n[58] hms_1.1.1 pillar_1.8.1 reshape2_1.4.4\n[61] reprex_2.0.1 glue_1.6.2 evaluate_0.15\n[64] latticeExtra_0.6-29 data.table_1.14.2 modelr_0.1.8\n[67] png_0.1-7 vctrs_0.4.1 tzdb_0.3.0\n[70] cellranger_1.1.0 gtable_0.3.0 assertthat_0.2.1\n[73] xfun_0.32 broom_0.8.0 ggcorrplot_0.1.3\n[76] survival_3.2-13 corrgram_1.14 cluster_2.1.2\n[79] corrplot_0.92 ellipsis_0.3.2"
  },
  {
    "objectID": "blog/ditl/ditl-followup.html",
    "href": "blog/ditl/ditl-followup.html",
    "title": "Becoming a Biostatistician: FAQs for ‘A Day in the Life of a Biostatistician’",
    "section": "",
    "text": "To my surprise since writing it, I’ve received hundreds of emails about my first blog post, “Day in the Life of a Biostatistician”. I really enjoy hearing from everyone who reaches out, but I thought I’d give detailed answers to the most common questions I’ve received in this follow up post. I first published this in January 2021 and last updated it in August 2022.\n\nCaveat all my answers with “this is just one early-career biostatistician’s opinion,” please!\n\nWhat do you recommend majoring in to become a (bio)statistician?\nIf you’re still early in your undergraduate education, I think it is best to major in statistics or math and supplement those classes with specific applications of statistics that interest you. Courses in biological science (e.g. genetics) or issues in healthcare (e.g. medical ethics, health economics) are all beneficial if you’re specifically interested in medical and biological applications of statistics. Any computer science courses you’re able to fit into your schedule will also help.\nIf you’re too far along in your current degree or you already finished college, not to worry! Depending on how much time you have before you apply to school, you should take Calculus I-III and Linear Algebra equivalents. This is a pretty standard requirement for most M.S. programs. I wish I had also taken a class in mathematical proofs.\nDid you find the transition from biological sciences to statistics to be difficult? (For those who haven’t read my DITL post, I majored in Biochemistry and had not taken the prerequisite math courses since my freshman year of college.)\nYes, absolutely. I don’t want to scare anyone away from going into biostatistics, but it was a difficult transition for me and required a lot of effort to catch up. I underestimated the transition to learn graduate level statistics after a years-long hiatus from math. I had to relearn many concepts from calculus and teach my brain to learn high-level math after years of studying biology and chemistry. I say this not to discourage you(!), but so that you understand you may feel very behind when you first begin your journey in biostatistics. It doesn’t mean you cannot or will not be successful.\nDid you know how to code before you started school and did you find learning to code to be difficult?\nNo, I didn’t know how to code at all before starting graduate school (you might be reading this thinking, why did anyone let her in?! and I honestly don’t know, either). I did not find learning to code particularly easy at first, but once I got over the initial learning curve after a few months in school, I found learning to code much easier than statistical theory. This might be because there’s a ton of resources to learn to code online, whereas math is usually harder to find good resources to learn independently.\nDo you have advice on [insert school name here]?\nI don’t feel like i’m in a position to give advice on specific programs as I only applied to one program, and my reason was purely financially-based. If I had applied to multiple schools I would’ve still tried to minimize post-school debt. Also, grad school can be very stressful, so pick a school that is in either a location you think you’ll enjoy living, and/or where you will have a good support system around you. I would also recommend trying to talk to current students beforehand if you’re deciding between multiple schools.\nI’m starting a Masters in Biostatistics soon. Do you have advice for grad school?\nHere’s what worked for me… take from it what you want! I have a lot of advice because I learned a lot of things the hard way. :-)\n\nDon’t be afraid to ask a lot of questions or to be a regular presence at office hours. I found that even when I was struggling quite a bit, professors wanted to help me when they saw I was putting in the effort to understand. It can be stressful to ask questions that you think you probably should know the answer to, but I always thought, “I’d rather ask this to my professor than not know it in front of my future boss.” Plus, you might need letters of recommendations or references, so you want your professors to be able to speak to your personality.\nTry to strike a good balance between learning to code and learning the theory. I attended a theory-heavy program (a one-credit SAS coding class comprised my entire formal programming education) so I spent a lot of time learning to code on my own, especially in the summer between my first and second years. It seems like more schools are shifting to an applied curriculum, so if you attend one of those programs, I would put independent effort into understanding the underlying theory.\nI found it helpful to get to know everyone in my cohort. Being on friendly terms with classmates benefited me on countless occasions throughout my Masters and afterwards. I understand it can be tough to put yourself out there, but it really will make your life easier when you’re stuck on a homework question or just generally feeling overwhelmed by school. My motto was always, “everyone here is likely awkward because we’ve all decided to study statistics, so I really have nothing to lose by ‘making the first move’ towards friendly conversation.”\nDon’t obsess over grades. I did this a little when I was in school and it was a complete waste of time. No one asked for my grades when I applied for jobs. Focus on learning the concepts and being able to explain what you’ve learned to audiences with varying degrees of technical background. Grades are an imperfect measure of your learning (and to be completely honest, they are often skewed due to a portion of students having materials from previously taught courses) so use them as feedback rather than an absolute measure of success. I TA now and it drives me crazy when students obsess over a less-than-perfect grade rather than their less-than-perfect understanding of a concept.\nI cannot emphasize this enough: don’t question your abilities if concepts don’t seem to come as “naturally” as they do for your classmates, especially if they’ve taken more quantitative courses than you (e.g. math, stats, computer science, engineering majors). They’ve probably already seen the foundation of what you’re learning in various forms before, and that matters so much! It took me a while to realize this, and I wasted a lot of time worrying I’d never be a good statistician because I had to study so much longer to understand concepts at even a passing level.\n\nShould I do an internship during grad school? Do you have any advice on getting an internship?\nSure, apply to internships! I applied to several for the summer between my first and second year of my Masters. There is a downside that they often don’t pay that well, and it can be costly to live in a new city for just 2-3 months. I ended up not taking any of my internship offers for that reason, and ultimately doing summer research in a lab at my university. I don’t think this hurt my final job prospects at all. My advice for getting internships is to review your CV/cover letter with your school’s career counselors, and then apply early and often. If you don’t have any relevant work experience (I didn’t), highlight in your cover letter and CV how your projects in school thus far have prepared you to do good work as an intern. For example, I discussed the dataset we used in my first semester regression course as a way of highlighting applied skills (exploratory data analysis, assumptions of regression, etc.).\nAs for summer research, it worked for me to email professors explaining that I was interested in their research and asking if we could set up a time to briefly chat. I used to get really nervous for these meetings before someone informed me that pretty much all professors love to talk about their research. Even if they don’t have any research opportunities at the moment, it’s a good way to learn about aspects of biostatistics you didn’t know existed.\nDo you feel intellectually stimulated as a biostatistician?\nAbsolutely. This is, hands down, the reason I like biostatistics so much. I will never know all there is to know about biostatistics (not even close!) and there are so many directions to grow.\nDo you feel financially rewarded as a biostatistician?\nAlso yes. I’ve been able to live comfortably and save money while working in academic research right out of school (we notoriously make lower salaries than positions in pharmaceutical or tech companies). If you’re curious about specific salaries, you might like this recent ASA survey of (bio)statistics graduates. Note that when you’re looking at salaries online, you can also look at jobs with the title “data scientist,” which you should have the base qualifications for after you receive a Masters in (Bio)statistics.\nDo you think a PhD is necessary for your career growth?\nI’m four years into my career and don’t yet feel stunted, so that’s a good sign, I think! No, I don’t think a PhD is necessary to have a stable, rewarding career. This is not to say I’ll never apply, because I do really love learning new statistical theory, but I do not think it is necessary and can see many routes to having a “successful” career without one.\nUpdate, August 2023, I did apply! I am starting the University of Washington’s Biostatistics PhD program soon, and I wrote about my motivation and application process here.\nWhat’s your favorite part of your job?\nI love when I am able to focus on one project and optimize my code to be really well documented and efficient. I also like improving my understanding of the biology or clinical domain knowledge for specific projects. Interestingly, one of my least favorite parts of the job used to be writing and editing manuscripts, but as I’ve gained experience reading and writing scientific papers, I enjoy this much more.\nWhat’s your least favorite part of your job?\nIt is working with individuals who either believe they have equivalent knowledge to me in statistics after taking an introductory course, or who know very little about statistics, do not desire to learn more, and yet do not trust my knowledge. Both types of people tend to want to be very hands-on in my analyses and this usually results in me explaining (to little avail) why we cannot do the analysis they think we should do. I don’t think this is unique to my job or even academia, however, it is definitely my least favorite part.\n\nThat’s all for now! I hope it is helpful. As always feel free to reach out if you have other questions; I try to answer my messages to this email once a week. You can also read about my friend Kim’s career as a biostatistician at a pharmaceutical CRO in this previous blog post.\nCheers,\nKat"
  },
  {
    "objectID": "blog/ditl/pharma-cro.html",
    "href": "blog/ditl/pharma-cro.html",
    "title": "Stats on Drugs: An Interview with a Pharmaceutical CRO Biostatistician",
    "section": "",
    "text": "An interview with my MS Biostatistics classmate Kim Hirschhorn about her experience as a biostatistician at a Contract Research Organization (CRO) for pharmaceutical companies.\n\n\nEarlier this year I wrote my first blog post, “A Day in the Life of a Biostatistician,” documenting the granular details of my work as an early career academic research biostatistician. I’m excited to announce I am turning that post into a “day in the life” series in which I interview other biostatisticians with differing roles. My hope is that it will enlighten anyone interested in the field of biostatistics, and especially help undergraduate and current biostatistics Masters students make informed decisions about their careers.\nMy first interviewee is Kim Hirschhorn, a former classmate and good friend from my time at the University of Michigan School of Public Health. Kim is an avid reality TV show watcher, loves Eggo waffles and Hostess Ho-Hos, and is a great statistician; so great, professors were often confused as to why she wasn’t planning to pursue a PhD. Kim, however, has always been adamant that she doesn’t need a PhD to have the career she wants.\nKim’s determination to find a job which allowed for ample career growth with “only” an MS in Biostatistics paid off. After graduating in 2018, she chose to work as a biostatistician at PRA Health Sciences, a Contract Research Organization (CRO) for pharmaceutical companies. Biostatistician roles at CROs offer excellent career trajectory for Masters-level statisticians and don’t have a job title ceiling as some careers in statistics notoriously do for those without a PhD. Additionally, it’s a lucrative job option for a Masters-level biostatistician. (🤑!) For more information on salaries of early career statisticians you can check out the American Statistical Association’s recent survey of 2018 Masters in Statistics graduates. But, to learn more about what Kim actually does as a biostatistician at a pharmaceutical CRO, read on!\nKat: Thanks for taking the time to chat, Kim! Can you tell me where you work and what the mission of your company is?\nKim: I work at the Chicago office of a large CRO called PRA Health Sciences. PRA stands for Pharmaceutical Research Associates, and our primary responsibility is to run clinical trials for pharmaceutical companies. This includes everything from planning the study at an administrative level – such as identifying physicians to recruit patients and administer drug therapies – to collecting data like patients’ disease progression and adverse events, analyzing and summarizing the results, and submitting requests for the successful trials to be approved by the Food and Drug Administration (FDA). After a pharmaceutical company has researched and developed a potential new drug, we partner with them to implement the various phases of clinical trials necessary to test that drug.\nWhoa, sometimes I forget about all the steps in a clinical trial. Before we go any further, can you give us a quick explanation about what those phases of clinical trials mean?\nThere are three main phases: Phase I, II, and III. In general, Phase I clinical trials check whether a drug is safe to administer. Sometimes this also involves testing which dosage of a drug is safe – or at least not overwhelmingly toxic – to the body. Phase I trials are usually done on a small group of patients. Phase II trials are done on a larger group of patients and focus on whether the drug is effective, or in other words, whether it can actually fix whatever health problem the pharmaceutical company had in mind when developing it. Most drugs fail in Phase II.\nIf a drug does move onto a Phase III trial, its performance in comparison to the current standard of care (which is often another drug on the market) is evaluated. In both Phase II and III, patients are randomized into treatment and control groups and typically neither physicians nor patients know which treatment is administered. Even patients who are not receiving a treatment will receive a sugar pill, or placebo drug. Oftentimes a clinical trial’s aims will fall somewhere between two phases. For example, I’ve worked on Phase 1b/2a trials that collect information on safety, determine the recommended dose, and then evaluate the drug’s efficacy.\nI see, so which of those phases do you work on?\nI have been a Biostatistician I for a bit over a year now, so that means I am the lead biostatistician on a few simple Phase I trials. There is usually not much statistical testing occurring here; it’s all about how many patients have an adverse response to the drug. Since I am still in the most junior biostatistician role, a lot of my time is spent doing biostatistical support for Phase II and III trials, where more senior biostatisticians oversee my work and pre-specify the analyses. The longer I stay at my job, the more responsibilities I will have and the more complex studies I will lead.\nWhat’s involved in leading a study, even a “small and simple” phase I? How is that different from the work you do as “biostatistical support” on the Phase II and III trials?\nWhen I am the lead of a study, I am meeting with members of the pharmaceutical company and working with them to develop a statistical analysis plan. Once the clinical trial starts and I begin to receive data, I work with programmers to ensure everything that needs to be done for the analysis is completed in the time allotted by that trial’s budget. Since the studies I am leading now are early phase, I am in charge of doing all the statistical analyses and reporting the results back to the pharmaceutical company. \nIn contrast, when I am biostatistical support, I am mostly executing analyses already specified in a statistical analysis plan. This typically involves writing macros in the statistical programming language SAS to ultimately produce what is dozens of outputs. A large portion of my time as a junior biostatistician is spent figuring out the best way to present that huge amount of data and results in a comprehensible way. Even before I have data from a clinical trial, I have to plan out detailed tables and prepare a shell document for when the data is ready for analysis.\nWho are you working with in your day-to-day roles?\nI get assignments from my manager, and then work with the lead biostatisticians on each trial. It is standard in the pharmaceutical industry that all analyses must be independently reproduced. This usually means the lead biostatistician is redoing my analysis without looking at my code, so sometimes I have to work closely with him or her to figure out why our results are not matching. Besides that, I frequently collaborate with the data programmers and statistical programmers in charge of the data for the trials I am working on.\nData programmers are in charge of taking raw data from the trials, checking it for inconsistencies, and putting it into a standardized form for statistical programmers. Statistical programmers then take those standardized data sets, derive many new variables from those initially collected for each patient, and produce hundreds of tables and figures summarizing the data. In some cases they also help prepare the data for my analyses, for example, they might write code to calculate the number of days or months until each patient experienced disease progression so that I can use that information for statistical models.\nIf there is a problem with the data when I am running a model, I ask the statistical programmer to check his or her code, and if there is no issue there, the data programmer looks at their code. If necessary, the data programmer may go back to the clinical sites who are collecting the data from patients and ask them to correct any missing information or inconsistencies.\nSounds like your data is in pretty good shape when it arrives to you, then! Earlier you mentioned writing macros in SAS. Does everyone use SAS? Could you use R or another programming language if you wanted?\nI have used R for a few side analyses, such as a data mining analysis. However, the industry standard for pharmaceuticals has always been SAS. I don’t foresee that changing anytime soon. Although a huge benefit of R is that it is free and open source software, meaning anyone can contribute code in a package, SAS has the advantage that if you run a certain type of statistical model, you can be absolutely sure that is what ran on your data. Because SAS is a paid software, it is liable for what it says it is doing, and there is always support on hand if you have questions about errors or results you are getting from any given data procedure.\nWhat are your hours like? What sort of flexibility do you have for vacation, or working remotely?\nI usually get in at 8:15 and leave somewhere between 4:45 and 5:15. I do get a lot of vacation days, with the caveat that if there is a deliverable deadline for a project I’m working on, I may not be able to take my vacation during that time. As for working remotely, it’s an option I have as-needed, such as if I have a doctor’s appointment. The job can definitely be done 100% remotely, and in fact many members of the company work remotely, but it’s not really an option for a Biostatistician I to do all the time because we rely so heavily on our senior biostatisticians. I prefer that, though - it’s easier for me to work in the office!\nHow will your roles and responsibilities change as you progress as a biostatistician at a CRO?\nWhen I said a lot of my time is spent working on table shells and report planning, I think it’s worth acknowledging that that is not a very exciting activity. However, as a Biostatistician I progresses to Biostatistician II, Senior Biostatistician, Principal Biostatistician, Senior Principal Biostatistician, etc., a lot of those more mundane (in my opinion) tasks decrease. You are in charge of much more complex studies and specifying the statistical analyses for that clinical trial. The work also naturally becomes more managerial.\nBeing in charge of a trial means meeting with both external clients and internal company leadership, setting timelines for data/statistical programmers and junior biostatisticians, answering questions that come up about data and analyses, maintaining oversight of the hours each team member is spending on the project, and tracking the corresponding budget carefully. As biostatisticians progress in the career ladder at a CRO, they have some flexibility in how much data they actually analyze on a day-to-day basis.\nWhat made you decide to take this position at a CRO over your other offers?\nWell of course it’s not all about money, but this was one of the highest offers I received, so that helped. The potential for career growth was also very appealing to me. Typically after about 4-5 years you are a Senior Biostatistician and qualified to lead any study. But, you can lead studies long before this if the pharmaceutical company agrees you are qualified to do so. I really like that career progression at a pharmaceutical CRO is very much about experience and skills, as opposed to whether or not you hold a PhD. This is often not the case if you work directly for a pharmaceutical company.\nFinally, what made you choose to enter the field of Biostatistics?\nI have always been drawn to a career in public health. During my first year at the University of Illinois, I was a stereotypical freshman pre-med student. I ultimately changed my mind about medical school and got a Bachelor’s degree in statistics, but I still took several epidemiology and biology classes just because I found them interesting. Math has always been my strongest subject, but I wanted to find a way to combine it with my other interests. After doing a fair share of Googling, I found the field of Biostatistics. I met with the head of U of I’s Statistics MS program, and he shared his experience of working in industry as a biostatistician before becoming a professor. He spoke very highly of the field and gave me a lot of great information on the types of careers I could have with a Masters in Statistics or Biostatistics. I was pretty much sold after that meeting.\nLooking back, are you happy with your decision to do a Masters in Biostatistics, rather than Statistics or Applied Statistics?\nYes, I’m really happy I decided on a Masters in Biostatistics. Throughout my program, our applied problem sets were always related to medicine or healthcare in some way, and it was easy for me to take health-specific statistics classes, such as a clinical trials methodology course. Since I was pretty sure I wanted to go into healthcare, I think biostatistics was a great decision for me, because a Masters in Biostatistics shows my passion for the field and may give me an edge over similar applicants who have a more general Masters in Statistics or Applied Statistics. I still find public health to be the most interesting sector to practice statistics in, so I think the degree will always be to my advantage.\nThanks so much for chatting, Kim!\nIf you have other questions for Kim, or if there are careers within biostatistics you’d like to hear about, feel free to email me and let me know.\nAll the best,\nKat"
  },
  {
    "objectID": "blog/phd-apps/index.html",
    "href": "blog/phd-apps/index.html",
    "title": "A Biostatistics PhD Application Notebook [with Statement of Purpose]",
    "section": "",
    "text": "A minor life update – I applied to Biostatistics PhD programs last fall! And, a major life update– I’m moving to Seattle to attend the University of Washington (UW)’s program next month. I’m super excited (and nervous) to begin. Since applications are opening up for next year, I thought I’d share what the process of deciding to apply, actually applying, and deciding on a program was like for me."
  },
  {
    "objectID": "blog/phd-apps/index.html#assessing-which-schools-to-apply-to",
    "href": "blog/phd-apps/index.html#assessing-which-schools-to-apply-to",
    "title": "A Biostatistics PhD Application Notebook [with Statement of Purpose]",
    "section": "Assessing which schools to apply to",
    "text": "Assessing which schools to apply to\nI gauged my competitiveness for applying to PhD programs by asking biostatistics faculty I knew from working in the field and/or who taught me during my MS. I also searched Reddit for relevant content (r/biostatistics) and used Gradcafe. (Be wary of anonymous forums on the internet, though!)\nFrom these sources, I gathered that my strengths were probably:\n\nhaving a MS in Biostatistics already\nfive years of full-time work as a biostatistical consultant\nexperience as the lead data analyst/statistician on many applied projects\nleading some of my own research papers\nparticipating in statistical methodology papers\n\nI decided the main weakness of my application would be my lack of mathematics background. Even though I had good grades in my Biostatistics MS program, I had only the minimum math requirements to apply to that program originally (Calc I-III and linear algebra), and my Calc III and linear algebra grades were mediocre, albeit from 10 years ago.\nOf note, when I read posts on GradCafe, the general consensus was that U.S. citizens (which I am) have a higher chance of being admitted to programs in the U.S. I don’t have much insight on this, but I think it depends program-to-program and has to do with funding opportunities. There are many government sponsored funding opportunities which are unfortunately only available to domestic students. This is also program-dependent, but oftentimes there are also many more international applicants, making it inherently more competitive to be an international applicant.\nIn the end, I applied to eight schools: two schools in New York City (where I currently live) and six other schools which are consistently considered to be top programs for biostatistics. Although I’m well-aware that rankings are imperfect measures of the quality of programs and there are many excellent biostatistics programs that are not top-ranked, I also knew I would only consider leaving NYC and my full-time salary for a few opportunities."
  },
  {
    "objectID": "blog/phd-apps/index.html#application-organization",
    "href": "blog/phd-apps/index.html#application-organization",
    "title": "A Biostatistics PhD Application Notebook [with Statement of Purpose]",
    "section": "Application organization",
    "text": "Application organization\nI kept track of all my applications and notes on a Notion page. I made several tables with information about each school’s requirements and created to-do lists for various tasks (e.g. send transcripts). I also wrote out my letter of recommendation (LOR) writers’ names, emails, and titles so that I would have an easier time copying and pasting."
  },
  {
    "objectID": "blog/phd-apps/index.html#application-components",
    "href": "blog/phd-apps/index.html#application-components",
    "title": "A Biostatistics PhD Application Notebook [with Statement of Purpose]",
    "section": "Application Components",
    "text": "Application Components\n\nTranscripts\nAll the schools I applied to required me to submit unofficial transcripts and then manually enter all relevant (science, math, statistics, etc.) coursework into their own application system. I had to enter the course name, course number, number of credits, semester I took it, and grade for each course. This is super time consuming, and I recommend beginning to work on this as soon as applications open. Many of the application portals were glitchy, and this would have been hard to complete at the last minute.\n\n\nGRE Scores\nMy GRE scores expired a few years back, but thankfully all the schools I applied to haven’t required them since the pandemic, so I didn’t retake the test. Of note, a few schools said they required them on their website, but did not actually when I inquired with admissions. For one school I only had to self-report my old scores.\n\n\nLetters of Recommendation\nAll schools required three LORs, and most accepted up to four or five. The people I asked to write my LORs were:\n\nA long-time colleague and mentor who could speak to my research potential for both methods and applied work. They are mid-career and known within the field of causal inference statistical estimation methods, which is what I want to continue studying.\nA long-time physician collaborator who I’d also worked with on applied projects for 4 years who could talk about my skill set in applied projects.\nMy current boss, an academic epidemiologist with strong training in statistical methods. At the time I’d only worked with them for a few months, but they seemed comfortable writing about my scientific potential.\n(Extra letter) My former professor1 from my applied capstone course during my MS. They are late-career and well-known within the field of biostatistics. They confirmed they could speak to my discipline and aptitude for completing coursework.\n\n1 Some schools explicitly request a LOR from a former professor.I think this is general LOR advice, but I only asked individuals who I was pretty sure would write strong letters on my behalf. I tried to strike a balance in people who were recognizable names within the field of biostatistics and who worked with me enough to write about me. Remember to ask your letter-writers early, as well as register early with the schools’ application systems so the writers have plenty of time to upload their letters.\n\n\nCurriculum Vitae\nAll of the schools required me to submit a Curriculum Vitae (CV) document. This is the version I submitted for applications. Depending on your background, sections will look different. I recommend having someone within academia, preferably (bio)statistics or related, review your CV. If you are a student, you should also take advantage of your university’s career center resources to review.\nAdditional feedback I received for this which may be relevant to someone else:\n\nList out all details/roles for classes you served as Teaching Assistant.\nList out blog posts under “Scientific Communication” and try to illustrate their impact. I’ve been blogging for years and have a Google Analytics attached to my site, so I was told to add the number of views.\n\n\n\nThe Statement of Purpose (SOP)\nThis was by far the hardest part of the application for me! There’s a lot of opinions surrounding the statement of purpose for Biostatistics PhDs, from, “it’s very important and the only way to set yourself apart to the application committee,” to, “nobody reads it and it won’t affect your application.” I opted to believe the first set of opinions and took my SOP seriously.\nI received a lot of advice on my statement. The most helpful piece of advice I received was that the SOP is not about highlighting qualifications – that’s what the CV does – and qualifications alone do not equate to success in academia. You need drive and motivation, and your SOP is the chance to show that you have it. It is more about your philosophy and research goals than stating what you’ve done so far. Every time you bring up an accomplishment, you should explain to the committee exactly why that’s relevant to your overall goal of pursuing a PhD in biostatistics. If something is not directly relevant to why you want to pursue a PhD or why you’ll be a successful researcher, you should not include it.\nI ended up receiving so much advice for this that I decided to publicly post my UW SOP on a Google Doc with comments. Some other resources I found helpful include these California State Example Essays and Lucy Lai’s Personal Statement for her Neuroscience PhD applications. These tweet threads were also useful:\n\n\nWriting a statement of purpose (SOP) for PhD admissions – please do not make me read another “as a kid, when I looked at the sky…” a thread.\n\n— Hadas Kress-Gazit (@HadasKressGazit) November 15, 2021\n\n\n\n\nEvery year I read a lot of grad school applications from accomplished people that don't give me the info I'm looking for. It feels like a major hidden curriculum thing. So here's (my opinion on) how to write a great Statement of Purpose/Research for a PhD program. 🧵 1/\n\n— Roman Feiman (@RomanFeiman) October 27, 2022\n\n\nIf you take only one thing away from my SOP advice: start writing your SOP early and ask at least one person who has served on an academic application committee, preferably for Biostatistics PhDs, to read your draft to make sure you’re on the right track. This is the easiest part of your application to control!\n\n\nThe Personal Statement\nOnly a few schools required this, and the prompts were related to why your background uniquely adds to your scientific potential. This statement is, of course, very personal to your own background! I wrote about how growing up in a rural Midwest town with my family in blue-collar jobs shaped my understanding of public health and access to education. I also wrote about my work and volunteer experience in low income areas and with underrepresented groups, and how my motivations for improving diversity in the field are driven by my experiences as an underrepresented gender in STEM. This will obviously look very different for any given applicant. I am not posting my personal statement publicly, but if you have a reason you think it’d be helpful to see my personal statement, please email me.\n\n\nApplication Fees\nAlmost every school had an $80-130 application fee, paid upon the time of submitting. Make sure to reach out to schools if you have any justification for receiving a fee waiver!"
  },
  {
    "objectID": "blog/phd-apps/index.html#pre-application-review-service-pars",
    "href": "blog/phd-apps/index.html#pre-application-review-service-pars",
    "title": "A Biostatistics PhD Application Notebook [with Statement of Purpose]",
    "section": "Pre Application Review Service (PARS)",
    "text": "Pre Application Review Service (PARS)\nI sent all my application materials in November to UW’s Pre Application Review Service (PARS) for review by current students. This is an excellent service available to underrepresented genders and minority groups. Not only was I able to get feedback on my application, but I made connections with a statistics PhD student who reviewed my application and a biostatistics PhD student who he subsequently introduced me to via email.\n\n\nOur department is offering a pre-application review service (PARS) initiative to provide support and mentorship to PhD applicants from historically marginalized groups. See details here: https://t.co/0evhEigqrm pic.twitter.com/xZ0B8LA8Gt\n\n— UW Statistics (@UWStat) September 27, 2022"
  },
  {
    "objectID": "blog/phd-apps/index.html#admission-rates",
    "href": "blog/phd-apps/index.html#admission-rates",
    "title": "A Biostatistics PhD Application Notebook [with Statement of Purpose]",
    "section": "Admission rates",
    "text": "Admission rates\nI found it difficult to find admission rates online, but the numbers given at some of my interview/admit days (if I remember correctly) were approximately:\n\n250 or so applicants\n15-25 interview spots\n7-20 spots in the cohort offered\n\nThe final number of spots in the cohort and process for obtaining that number varied quite a bit by school. A few schools ranked candidates and could only offer a spot to the top 6-7 candidates. Once someone rejected their offer, they moved down the list to offer the next candidate. Other schools accepted a large (~20) number of applicants with the expectation that only a certain percentage would accept their offer. Finally, at least one school I applied to could only offer a fixed number of spots (12), and could not re-offer to another applicant if someone turned down their offer. That school was careful to only give offers to those they really thought might attend."
  },
  {
    "objectID": "blog/phd-apps/index.html#funding",
    "href": "blog/phd-apps/index.html#funding",
    "title": "A Biostatistics PhD Application Notebook [with Statement of Purpose]",
    "section": "Funding",
    "text": "Funding\nMost Biostatistics PhD programs will only admit students if they know they can fund them, i.e. pay for tuition and a stipend, for 4-6 years. For the programs I was admitted to, the stipend offers ranged from $36-46,000 per year, pre-tax. A PhD stipend is often described as “enough to live, but not enough to save,” although this will obviously vary by the city’s cost-of-living and the student’s personal financial situation.\nI said earlier that a PhD is a huge financial commitment, and the stipend is the main reason why. Even though the amount of money might seem like a lot (it did to me when I was going through my MS degree!), the time you’ll spend earning your PhD is undoubtedly a short-term loss of potential earnings. If you have a strong quantitative background (as most Biostatistics PhD applicants do), a conservative estimate is that this loss could accumulate to over $400,000 in pre-tax income.3 This estimate is not accounting for the compounding interest you will miss out on in retirement savings (assuming you would put money towards retirement if working full-time). Although the earning potential is higher with a PhD than with an MS, it will still take some time to counteract the short-term loss.\n3 My calculation for this is (potential salary - stipend) * expected years in PhD.On that note, if you have multiple funded offers, it is worth asking each program what their policies are regarding internships, part-time work, and freelance consulting work, because all of these are supplemental sources of income. Are any of these types of work allowed or encouraged, and does participating in them affect the stipend amount you receive (beyond potential differences in tax brackets)? The answers vary by program, and sometimes even by student due to differences in funding sources.\n[Edit August 2024] Tidbits on funding I learned about funding after beginning my PhD program:\n\nI learned in some cases you can actually negotiate your offer with schools if you have other offers! I have no advice to offer on this, only that I’ve heard it is possible.\nThe National Science Foundation (NSF) Graduate Student Research Fellowship is an excellent opportunity for U.S. citizens to secure their own funding prior to beginning a PhD program. If you’re returning to school after prior time in graduate school, like I was, you can only apply for the scholarship before beginning your PhD program. The deadline is in October. If you’d like to apply, Simon Couch wrote a blog post about his experience. One of my classmates came into UW with this fellowship.\nAnother opportunity for U.S. students is to apply for a National Institutes of Health F31 Fellowship. This is another way to secure your own funding, and there is no limit on the number of years of prior schooling to apply."
  },
  {
    "objectID": "blog/phd-apps/index.html#reaching-out-to-professors-in-advance",
    "href": "blog/phd-apps/index.html#reaching-out-to-professors-in-advance",
    "title": "A Biostatistics PhD Application Notebook [with Statement of Purpose]",
    "section": "Reaching out to professors in advance",
    "text": "Reaching out to professors in advance\nI did not email any professors before applying, so I unfortunately don’t have much to share on this topic. I doubt it would’ve helped me get into any additional programs, but who knows! It definitely has the potential to be informative and a good networking experience. Lucy Lai includes a template for reaching out to professors in her blog post, as does John Muschelli in his post, “Some things I wish I knew about Grad School”."
  },
  {
    "objectID": "blog/sofa/index.html",
    "href": "blog/sofa/index.html",
    "title": "How many ways are there to get a SOFA Score of 10?",
    "section": "",
    "text": "The Serial Organ Failure Assessment (SOFA) is a measure used in hospitals to define and assign severity of illness in acutely ill patients. The score measures general dysfunction in six vital organ systems: the lung, brain, heart, liver, kidney, and blood. Each of these organs receive an integer subscore, ranging from 0 to 4, using routinely collected labs and vital signs. The sum of the six subscores, ranging from 0 to 24, creates a patient’s SOFA score. The higher a patient’s SOFA score, the sicker they are. There are plenty of online calculators for SOFA scores that you can check out you’re curious about exactly what goes into each of the subscores.\nSOFA scores are useful as a quick live-check for physicians of a patient’s overall well-being, but they’re moreso used extensively in critical care observational research. We use SOFA scores to adjust for severity of illness as a confounder in observational studies, and they’re often used as inclusion criteria for deciding which patients are eligible for therapies in a Randomized Control Trial (RCT). There are plenty of pros and cons of SOFA, but one aspect of SOFA scores that is perhaps non-intuitive for many physicians is just how many ways there are for a patient with a drastically different clinical profile to have the same SOFA score as another patient.\nRecently a pulmonologist who I work closely with, Dr. Edward Schenck, sent me an email with the subject line “Help!!” I clicked on his email, and found an interesting question:\n\nThere is only one way for a patient to have a SOFA score of 0 (all six subscores are 0). There are six ways a patient could have a SOFA score of 6 (one of the six subscores is 1). But how many ways are there for a patient to get a SOFA score of 10?\n\nIt was a serious question from him, but a beautiful mid-day brainteaser break for me. What are the number of possible combinations to get every possible SOFA score, from 0 to 24?\n\nWe can solve this problem with only a few lines of code in R by using expand.grid() to make all the combinations of the 6 scores, which can take values 0, 1, 2, 3, or 4.\n\ncombos &lt;- expand.grid(\"lung\" = 0:4, \"liver\" = 0:4, \"heart\" = 0:4, \"blood\" = 0:4, \"brain\" = 0:4, \"kidney\" = 0:4)\nhead(combos,n=20)\n\n   lung liver heart blood brain kidney\n1     0     0     0     0     0      0\n2     1     0     0     0     0      0\n3     2     0     0     0     0      0\n4     3     0     0     0     0      0\n5     4     0     0     0     0      0\n6     0     1     0     0     0      0\n7     1     1     0     0     0      0\n8     2     1     0     0     0      0\n9     3     1     0     0     0      0\n10    4     1     0     0     0      0\n11    0     2     0     0     0      0\n12    1     2     0     0     0      0\n13    2     2     0     0     0      0\n14    3     2     0     0     0      0\n15    4     2     0     0     0      0\n16    0     3     0     0     0      0\n17    1     3     0     0     0      0\n18    2     3     0     0     0      0\n19    3     3     0     0     0      0\n20    4     3     0     0     0      0\n\ncombos$total_sofa &lt;- rowSums(combos)\nhead(combos,n=20)\n\n   lung liver heart blood brain kidney total_sofa\n1     0     0     0     0     0      0          0\n2     1     0     0     0     0      0          1\n3     2     0     0     0     0      0          2\n4     3     0     0     0     0      0          3\n5     4     0     0     0     0      0          4\n6     0     1     0     0     0      0          1\n7     1     1     0     0     0      0          2\n8     2     1     0     0     0      0          3\n9     3     1     0     0     0      0          4\n10    4     1     0     0     0      0          5\n11    0     2     0     0     0      0          2\n12    1     2     0     0     0      0          3\n13    2     2     0     0     0      0          4\n14    3     2     0     0     0      0          5\n15    4     2     0     0     0      0          6\n16    0     3     0     0     0      0          3\n17    1     3     0     0     0      0          4\n18    2     3     0     0     0      0          5\n19    3     3     0     0     0      0          6\n20    4     3     0     0     0      0          7\n\nsofa_counts &lt;- table(combos$total_sofa)\nsofa_counts\n\n\n   0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15 \n   1    6   21   56  126  246  426  666  951 1246 1506 1686 1751 1686 1506 1246 \n  16   17   18   19   20   21   22   23   24 \n 951  666  426  246  126   56   21    6    1 \n\n\nWe can plot those numbers using ggplot2:\n\nlibrary(tidyverse)\n\ncombos %&gt;%\n  count(total_sofa) %&gt;%\n  ggplot(aes(total_sofa, n, label=formatC(n, format=\"d\", big.mark=\",\"))) +\n  geom_point(size=2) +\n  geom_segment(aes(x=total_sofa, xend=total_sofa, y=0, yend=n)) +\n  geom_text(nudge_y = 60) +\n  labs(x=\"Total SOFA Score\", y=\"Number of Combinations\") +\n  scale_y_continuous(expand=c(0,0), limits=c(-20,2000), labels=scales::comma_format()) +\n  scale_x_continuous(breaks = 0:24) +\n  guides(fill=F) +\n  scale_fill_gradient(low=\"navy\",high=\"firebrick2\") +\n  theme_classic() +\n  theme(axis.line = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title.x = element_blank(),\n         plot.title = element_text(hjust = 0.5,\n                                  face = \"bold\", size = 20),\n        axis.text.x = element_text(size=12, face=\"bold\", vjust=2.5)\n        ) +\n  labs(title=\"Number of subscore combinations per SOFA score\") + \n  annotate(\"segment\", x = 6, xend = 9, y = 1800, yend = 1600, colour = \"dodgerblue2\", size=2, arrow=arrow())"
  },
  {
    "objectID": "blog/tmle/visual-key.html",
    "href": "blog/tmle/visual-key.html",
    "title": "A Condensed Key for A Visual Guide to Targeted Maximum Likelihood Estimation (TMLE)",
    "section": "",
    "text": "A helper post to TMLE Part II: The Algorithm containing only formulas and graphics.\n\n\nA condensed key for my corresponding TMLE tutorial blog post.\n\n\nInitial set up\n Estimand of interest:\n\\[ATE = \\Psi = E_W[\\mathrm{E}[Y|A=1,\\mathbf{W}] - \\mathrm{E}[Y|A=0,\\mathbf{W}]]\\]\n\n\n\nStep 1: Estimate the Outcome\n\n\n\n\nFirst, estimate the expected value of the outcome using treatment and confounders as predictors.\n\\[Q(A,\\mathbf{W}) = \\mathrm{E}[Y|A,\\mathbf{W}]\\] \nThen use that fit to obtain estimates of the expected outcome under varying three different treatment conditions:\n1. If every observation received the treatment they actually received.\n\\[\\hat{Q}(A,\\mathbf{W}) = \\mathrm{\\hat{E}}[Y|A,\\mathbf{W}]\\]\n\n2. If every observation received the treatment.\n\\[\\hat{Q}(1,\\mathbf{W}) = \\mathrm{\\hat{E}}[Y|A=1,\\mathbf{W}]\\]\n\n3. If every observation received the control.\n\\[\\hat{Q}(0,\\mathbf{W}) = \\mathrm{\\hat{E}}[Y|A=0,\\mathbf{W}]\\]\n\n\n\n\nStep 2: Estimate the Probability of Treatment\n\n\n\n\nThe next step is to estimate the probability of treatment, given confounders.\n\\[g(\\mathbf{W}) = \\mathrm{Pr}(A=1|\\mathbf{W})\\] \nThen we need to compute three different quantities from this model fit:\n1. The inverse probability of receiving treatment.\n\\[H(1,\\mathbf{W}) = \\frac{1}{g(\\mathbf{W})} = \\frac{1}{\\mathrm{Pr}(A=1|\\mathbf{W})}\\]\n\n2. The negative inverse probability of not receiving treatment.\n\\[H(0,\\mathbf{W}) = -\\frac{1}{1-g(\\mathbf{W})}= -\\frac{1}{\\mathrm{Pr}(A=0|\\mathbf{W})}\\]\n\n3. If the observation was treated, the inverse probability of receiving treatment, and if they were not treated, the negative inverse probability of not receiving treatment.\n\\[H(A,\\mathbf{W}) = \\frac{\\mathrm{I}(A=1)}{\\mathrm{Pr}(A=1|\\mathbf{W})}-\\frac{\\mathrm{I}(A=0)}{\\mathrm{Pr}(A=0|\\mathbf{W})}\\]\n\n\n\n\nStep 3: Estimate the Fluctuation Parameter\n\n\n\n\nEstimating equation we need to solve:\n\\[logit(\\mathrm{E}[Y|A,\\mathbf{W}]) = logit(\\mathrm{\\hat{E}}[Y|A,\\mathbf{W}]) + \\epsilon H(A,\\mathbf{W})\\] Two technical points for application: we use qlogis to transform the probabilities \\(\\mathrm{\\hat{E}}[Y|A,\\mathbf{W}]\\) to the \\(logit\\) scale. Also, the R code for a fixed intercept is -1 + offset(fixed_intercept).\n\nNext we need to save the coefficient from that logistic regression, which we will call \\(\\hat{\\epsilon}\\):\n\n\n\n\nStep 4: Update the Initial Estimates of the Expected Outcome\n\n\n\n\nNote we can use \\(expit\\) to show the inverse of the \\(logit\\) function, and we will denote updates to the outcome regressions as \\(\\hat{\\mathrm{E}}^*\\).\n1. Update the expected outcomes of all observations, given the treatment they actually received and their baseline confounders.\n\\[\\hat{\\mathrm{E}}^*[Y|A,\\mathbf{W}] = expit(logit(\\mathrm{\\hat{E}}[Y|A,\\mathbf{W}]) + \\hat{\\epsilon}H(A,\\mathbf{W}))\\]\n\n2. Update the expected outcomes, conditional on baseline confounders and everyone receiving the treatment.\n\\[\\hat{\\mathrm{E}}^*[Y|A=1,\\mathbf{W}] = expit(logit(\\mathrm{\\hat{E}}[Y|A=1,\\mathbf{W}]) + \\hat{\\epsilon}H(A,1))\\] \n3. Update the expected outcomes, conditional on baseline confounders and no one receiving the treatment.\n\\[\\hat{\\mathrm{E}}^*[Y|A=0,\\mathbf{W}] = expit(logit(\\mathrm{\\hat{E}}[Y|A=0,\\mathbf{W}]) + \\hat{\\epsilon}H(A,0))\\] \n\n\n\nStep 5: Compute the Statistical Estimand of Interest\n\n\n\n\nWe now have updated expected outcomes estimates, so we can compute the ATE as the mean difference in the updated outcome estimates under treatment and no treatment:\n\\[\\hat{ATE}_{TMLE} = \\hat{\\Psi}_{TMLE} = \\sum_{i=1}^{n}[\\hat{E^*}[Y|A=1,\\mathbf{W}] - \\hat{E^*}[Y|A=0,\\mathbf{W}]]\\]\n\n\n\n\nStep 6: Calculate the Standard Errors, Confidence Intervals, and P-values\n\n\n\n\nTo obtain the standard errors, we first need to compute the Influence Curve (IC). The equation for the IC looks like this:\n\\[\\hat{IC} = (Y-\\hat{E^*}[Y|A,\\mathbf{W}])H(A,\\mathbf{W}) + \\hat{E^*}[Y|A=1,\\mathbf{W}] - \\hat{E^*}[Y|A=0,\\mathbf{W}] - \\hat{ATE}\\]\nOnce we have the IC, we can take the square-root of its variance divided by the number of observations to get the standard error of our estimate.\n\\[\\hat{SE} = \\sqrt{\\frac{var(\\hat{IC})}{N}} \\]\n Once we have that standard error, we can easily get the 95% confidence interval and p-value of our estimate.\n\nA visual guide designed as a printable reference is available on my Github:"
  },
  {
    "objectID": "blog/tmle/tutorial-pt3.html",
    "href": "blog/tmle/tutorial-pt3.html",
    "title": "An Illustrated Guide to TMLE, Part III: Properties, Theory, and Learning More",
    "section": "",
    "text": "The is the third and final post in a three-part series to help beginners and/or visual learners understand Targeted Maximum Likelihood Estimation (TMLE). In this section, I discuss more statistical properties of TMLE, offer a brief explanation for the theory behind TMLE, and provide resources for learning more.\n\n\n\nProperties of TMLE 📈\nTo reiterate a point from Parts I and II, a main motivation for TMLE is that it allows the use of machine learning algorithms while still yielding asymptotic properties for inference. This is notably not true for many estimators.\nFor example, in Part II we walked through TMLE for the Average Treatment Effect (ATE). Two frequently used alternatives to estimating the ATE are G-computation and Inverse Probability of Treatment Weighting (see Part II, Step 1 and references). In general, neither yield valid standard errors unless a-priori specified parametric models are used, and this reliance on parametric assumptions can bias results. There are many simulation studies that show this.\nAnother beneficial property of TMLE is that it is a doubly robust estimator. This means that if either the regression to estimate the expected outcome, or the regression to estimate the probability of treatment, are correctly specified (formally, their bias goes to zero as sample size grows large, meaning they are consistent), the final TMLE estimate will be consistent.\nIf both regressions are consistent, the final estimate will reach the smallest possible variance at a rate of \\(\\sqrt{n}\\), which is the fastest possible rate of convergence and equivalent to parametric maximum likelihood estimation. The reason we use superlearning for estimating the outcome and treatment regressions is to give us the best possible chance of having two correctly specified models and obtaining an efficient estimate.\n\nEven among other doubly robust estimators, TMLE is appealing because its estimates will always stay within the bounds of the original outcome. This is because it is part of a class of substitution estimators. There is another class of doubly robust, semiparametric estimation methods frequently used in causal inference that are referred to as one-step estimators, but they can sometimes yield final estimates that are outside the original outcome scale. The one-step estimator for the ATE is called Augmented Inverse Probability Weighting (AIPW).\n\n\nWhy does TMLE work? ✨\nTruly understanding why TMLE works requires semiparametric theory that falls far outside the scope of this tutorial. However, the theory is interesting, so I’ll give a brief, high-level explanation, and then you can look at the references if you’re curious to learn more. Importantly, the explanation I outline here is more than sufficient and certainly not necessary to appropriately implement TMLE as an analyst.\nTMLE relies on the following ideas:\n\nSome estimands allow for asymptotically linear estimation. This means that estimators can be represented as sample averages (plus a term that converges to zero).\nThe quantities being averaged for asymptotically linear estimators are called influence functions. An influence function is a function that quantifies how much influence each observation has on the estimator. For this reason, it is very useful to characterize the variance of the estimator. In parametric maximum likelihood estimation, the influence function is related the score function.\nThe efficient influence function (EIF) is the influence function that achieves the efficiency bound (think Cramer Rao Lower Bound from parametric maximum likelihood estimation) and can be used to create efficient estimators.\nIf we want to construct an estimator that is efficient, we can take advantage of the EIF to endow the estimator with useful asymptotic properties.\n\nThis is the reason TMLE allows us to use machine learning models “under the hood” while still obtaining asymptotic properties for inference: our estimand of interest admits asymptotically linear estimation, and we are using properties of the EIF to construct an estimator with optimal statistical properties (e.g. double robustness).\n\n\nResources to learn more\nI could only cover so much in this post, but here are the resources I’ve used the most to learn about TMLE, semiparametric estimation, and causal inference. If you are new to any or all of it, there is a good chance it will take several reads of these materials before the concepts begin to make any sense. Don’t get discouraged!\n\nTMLE\n\nThe paper I referred to most often while learning TMLE was Targeted Maximum Likelihood Estimation for Causal Inference in Observational Studies by Megan S. Schuler and Sherri Rose. It has a nice step-by-step written explanation and details the statistical advantages of TMLE for an applied thinker.\nI also really like the written explanations in the Targeted Learning book (Chapters 4 and 5) by Mark van der Laan and Sherri Rose. The notation was often too difficult for me to follow, but the words themselves make a lot of sense.\nMiguel Angel Luque-Fernandez wrote an excellent bookdown tutorial on TMLE, also with step-by-step R code. It is more technical and thorough than my post, but still aimed at an applied audience. He also has a tutorial on the functional delta method which is part of the theory behind the way we compute the standard errors (see Part II, Step 6).\nOther code-based web-based tutorials on TMLE that are more recent (or recently discovered by me!) include:\n\nDavid Benkeser and Antoine Chambaz’ A Ride in Targeted Learning Territory\nThe authors of the R package suite tlverse’s Hitchhiker’s Guide to Targeted Learning: The TMLE Framework.\nStitch Fix’s Jasmine Nettiksimmons AND Molly Davies’ blog post: “Gimme a robust estimator - and make it a double!”\n\n\n\n\nSemiparametric Theory and Influence Functions\n\nEdward Kennedy has several well-written pieces on semiparametric estimation in causal inference. I recommend starting with:\n\nHis introductory paper on Semiparametric Theory\nHis slideshow tutorial Nonparametric efficiency theory and machine learning in causal inference\n\nMy favorite resource so far for learning specifically about influence functions has been Visually Communicating Influence Functions by Aaron Fisher and Edward Kennedy. However, this paper didn’t make sense to me until I worked through this interactive tutorial by Herb Susmann. I suggest playing around with the interactive examples first, and then trying to work through the paper.\nAdditonal walkthroughs to learn about EIFs include the following:\n\nAlejandro Schuler’s Modern Causal Inference online book which includes a section on deriving EIFs. His tutorial is a similar, shorter version of the next two resources:\nOliver Hines et al.’s Demystifying statistical learning based on efficient influence functions\nEdward Kennedy’s comprehensive review Semiparametric Doubly Robust Targeted Double Machine Learning: A Review\n\n\nAlthough I did not personally use these last few when I was initially learning about EIFs (since they came out after I wrote this post in Fall 2020), they seem like great resources and intended to be approachable for beginner-to-intermediate learners.\n\n\n\nCausal Inference\n\nAs emphasized in Part I, TMLE is an estimation technique which can be used for causal inference. If you want to learn about the foundations of causal inference, I suggest two different introductory texts below. Note that these provide fairly different frameworks (notation, descriptions of assumptions) to reach the same conclusions, but both provide useful perspectives.\n\nCausal Inference in Statistics: A Primer by Judea Pearl. Pearl does not discuss estimation methods, but rather focuses on the assumptions, or identification, side of causal inference. Thus, you will not find TMLE mentioned in this text.\nWhat If by Miguel Hernan and James Robins. Notably, Hernan and Robins only discuss parametric estimation methods, so you will also not find TMLE or AIPW in this text.\n\nI also think the introductory chapters of the previously mentioned Targeted Learning book (Chapters 1 and 2) do an excellent job of setting up the “roadmap” of causal inference.\n\nI’ll continue to update this page with beginner’s resources as I discover them.\nFeedback or clarifications on this post is welcome, either from the new learners of TMLE or experts in causal inference. The best way to reach me is through email.\n\n\n\nAcknowledgements\nThis tutorial would not have been possible without my colleague Iván Díaz patiently answering many, many questions on TMLE. I am also very appreciative of Miguel Angel Luque-Fernandez’s helpful feedback on the visual guide.\nLastly, many thanks to Axel Martin, Nick Williams, Anjile An, Adam Peterson, Alan Wu, and Will Simmons for providing suggestions on various drafts of this art project!"
  },
  {
    "objectID": "blog/covid-functions/index.html",
    "href": "blog/covid-functions/index.html",
    "title": "Silver Linings: five coding tricks learned during Lockdown",
    "section": "",
    "text": "Tips for using {tidylog}, {gtsummary}, {labelled}, {snakecase}, and more.\n\nJuly 10, 2020.\nIn non-coronavirus times, I am the biostatistician for a team of NYC pulmonologists and intensivists. When the pandemic hit NYC in mid-March, I immediately became a 100% 200% COVID-19 statistician. I received many analysis requests, though not all of them from official investigators:\n\n\nMy family recently learned I am the statistician for my hospital’s pulmonologists and now I get COVID-19 analysis requests from them, too pic.twitter.com/wlHmUaBh6Y\n\n— Kat Hoffman (@rkatlady) April 10, 2020\n\n\nJokes aside, I was really busy during the Spring 2020 outbreak. While doing work for both hospital operations and rapidly moving COVID-19 academic research, it was especially important to code efficiently and accurately while working with a deluge of Electronic Health Record (EHR) data.\nThis post contains my favorite (because they were either the most useful or most used) lines of R code for turning very raw-form EHR data into analytical files for descriptive reporting and statistical inference. For the examples, I’ve created data that is a simplified version of my actual data sets – just in case you too would like to pretend you are a COVID-19 biostatistician!\nThe first data set contains some basic demographic and clinical outcome information for all COVID-19 positive patients who arrived at a hospital. The second data set contains laboratory results for all patients who arrived at the hospital during a certain window of time. In theory, the first data set (patients) should be a subset of the second (labs).\n# load tidyverse for data creation and set seed for reproducible data\nlibrary(tidyverse)\nset.seed(7)\n\n# data set of basic patient demographics\npatients &lt;-\n  tribble(\n    ~id, ~admit_dt, ~death_or_discharge_dt,\n    ~age, ~sex, ~height, ~weight, ~current_smoker, ~immunosuppressed, \n    100, \"2020-03-21 00:10\", \"2020-05-13 12:10\",\n64, \"Male\", 68, 199, \"Yes\", \"No\", \n    104, \"2020-04-03 12:15\", \"2020-04-29 18:34\",\n25, \"Male\", 72, NA, \"Yes\", \"No\", \n    106, \"2020-03-28 12:22\", \"2020-04-05 19:18\",\n49, \"Female\", 64, 189, \"No\", \"Yes\", \n     107, \"2020-04-10 18:15\",\"2020-04-14 19:12\",\n 88, \"Male\", 62, 111, \"No\", \"Yes\", \n    111, \"2020-04-18 00:49\", \"2020-04-25 19:18\",\n61, \"Female\", 67, 156, \"No\", \"Yes\"\n  ) %&gt;%\n  # set time zone for date time variables\n  mutate_at(vars(ends_with(\"_dt\")), ~as.POSIXct(., tz=\"America/New_York\"))\n\n# generate labs data\nlabs &lt;- map_dfr(100:110, function(x){\n  lab_time &lt;- sample(seq(as.POSIXct(\"2020-03-01 00:00\"), as.POSIXct(\"2020-05-30 00:00\"), by=\"hours\"), runif(1, 50, 200))\n  id &lt;- rep(x, length.out=length(lab_time))\n  lab_name &lt;- sample(c(\"D-Dimer\",\"Platelet Count\",\"C-Reactive Protein\",\"Lactate Dehydrogenase\",\"LYMPHOCYTE PERC\",\"Absolute Lymphocyte Count\"), size = length(lab_time), replace = T)\n  lab_value &lt;- runif(length(lab_time), 100, 1200)\n  lab_value &lt;- ifelse(lab_value &gt; 1000, \"&gt;1000\", as.character(round(lab_value)))\n  df &lt;- tibble(id, lab_time, lab_name, lab_value)\n  return(df)\n}) \npatients\n\nA tibble: 5 × 9\n id admit_dt            death_or_discharge_dt   age sex    height weight\n       1 100 2020-03-21 00:10:00 2020-05-13 12:10:00 64 Male 68 199 2 104 2020-04-03 12:15:00 2020-04-29 18:34:00 25 Male 72 NA 3 106 2020-03-28 12:22:00 2020-04-05 19:18:00 49 Female 64 189 4 107 2020-04-10 18:15:00 2020-04-14 19:12:00 88 Male 62 111 5 111 2020-04-18 00:49:00 2020-04-25 19:18:00 61 Female 67 156 # … with 2 more variables: current_smoker , immunosuppressed \nlabs\n\n\nA tibble: 1,623 × 4\n  id lab_time            lab_name                  lab_value\n   \n1 100 2020-05-02 03:00:00 Platelet Count 245\n2 100 2020-03-20 20:00:00 C-Reactive Protein &gt;1000\n3 100 2020-05-15 07:00:00 C-Reactive Protein 620\n4 100 2020-04-29 00:00:00 Absolute Lymphocyte Count 937\n5 100 2020-05-02 08:00:00 Platelet Count 984\n6 100 2020-04-13 14:00:00 Absolute Lymphocyte Count 177\n7 100 2020-04-30 15:00:00 LYMPHOCYTE PERC 227\n8 100 2020-03-24 19:00:00 LYMPHOCYTE PERC 405\n9 100 2020-03-12 04:00:00 C-Reactive Protein 878\n10 100 2020-04-09 11:00:00 Platelet Count 711\n# … with 1,613 more rows\n\n\n1. tidylog\nThe first line of R code I found most useful was actually just loading an entire package. It sounds crazy, but that’s all you have to do! You simply load the package, tidylog, after tidyverse (or dplyr or tidyr):\nlibrary(tidylog, warn.conflicts = F)\nThen, whenever you use one of the previously mentioned packages to wrangle data, tidylog will give you super helpful information about what just happened. For example, if you use mutate on a column, it will tell you how many new NA values were created, if any. It will also remind you which variables you removed, and will give you feedback after you’ve grouped or ungrouped by a certain variable.\npatients &lt;-\n  patients %&gt;%\n  # compute BMI\n  mutate(bmi = weight / height^2 * 703) %&gt;%\n  # remove the patients height and weight from the data frame\n  select(-height, -weight)\nmutate: new variable 'bmi' (double) with 5 unique values and 20% NA\nselect: dropped 2 variables (height, weight)\nIt’s especially helpful for joining data, because it will tell you how many rows matched in the right and left hand side of your data. In this example, we can see that about 50% of the lab values in labs have a match in patients – as expected. However, one patient in patients does not have any labs in labs – not good! This would be a scenario I would need to follow up with my data source (the Informatics team I work with) to figure out.\npatient_labs &lt;-\n  patients %&gt;%\n  left_join(labs)\nJoining, by = \"id\"\nleft_join: added 3 columns (lab_time, lab_name, lab_value)\n&gt; rows only in x 1\n&gt; rows only in y (994)\n&gt; matched rows 629 (includes duplicates)\n&gt; =====\n&gt; rows total 630\npatient_labs\n\n\nA tibble: 630 × 11\n  id admit_dt            death_or_discharge_dt   age sex   current…¹ immun…²\n      \n1 100 2020-03-21 00:10:00 2020-05-13 12:10:00 64 Male Yes No\n2 100 2020-03-21 00:10:00 2020-05-13 12:10:00 64 Male Yes No\n3 100 2020-03-21 00:10:00 2020-05-13 12:10:00 64 Male Yes No\n4 100 2020-03-21 00:10:00 2020-05-13 12:10:00 64 Male Yes No\n5 100 2020-03-21 00:10:00 2020-05-13 12:10:00 64 Male Yes No\n6 100 2020-03-21 00:10:00 2020-05-13 12:10:00 64 Male Yes No\n7 100 2020-03-21 00:10:00 2020-05-13 12:10:00 64 Male Yes No\n8 100 2020-03-21 00:10:00 2020-05-13 12:10:00 64 Male Yes No\n9 100 2020-03-21 00:10:00 2020-05-13 12:10:00 64 Male Yes No\n10 100 2020-03-21 00:10:00 2020-05-13 12:10:00 64 Male Yes No\n# … with 620 more rows, 4 more variables: bmi , lab_time , # lab_name , lab_value , and abbreviated variable names # ¹​current_smoker, ²​immunosuppressed\nlibrary(tidylog) has singlehandedly helped me detect countless errors while working with rapidly changing COVID-19 data from many different sources. My coworker once summed it up perfectly by saying, “tidylog isn’t just a package, it’s a lifestyle.”\n\n\n2. gtsummary::tbl_summary() + labelled::add_variable_labels + snakecase::to_title_case\n\n\nefficiently labeling variables: a useful skill if you make a lot of #rstats tables👩🏼‍💻feat. labelled, snakecase, stringr & gtsummary 📦s pic.twitter.com/YmiYZEjVKS\n\n— Kat Hoffman (@kat_hoffman_) September 13, 2020\n\n\ngtsummary’s tbl_summary() is hands down my favorite function for making summary tables. It is so smooth and flexible to use, and it works seamlessly with the new gt package for making tables. You simply input a dataset containing the variables you want to summarize, and, optionally, a grouping variable to stratify those summary statistics by, and you’ll immediately have a clean and clear descriptive table!\nIn this example, I’ll use tbl_summary() to summarize the median and IQR or number and percent of all the demographic variables in our patients data set. We could use this code: the barebones function truly only needs a data set containing the variables of interest, and then gtsummary does all the formatting work:\nlibrary(gtsummary)\n\nAttaching package: 'gtsummary'\nThe following objects are masked from 'package:tidylog':\n\n    mutate, select\npatients %&gt;% \n  # select vars of interest for tables\n  select(age, sex, bmi, current_smoker, immunosuppressed) %&gt;%\n  tbl_summary(\n    # make sure all numeric variables are reported as continuous\n    type = list(all_numeric() ~ \"continuous\")\n  ) \nThat’d be fine, but what I really found to be useful during the NYC outbreak was the labelled function. Since I was constantly presenting data to clinicians, it was important that the tables and figures I showed were clear and concise. I try to always eliminate the “ugly” variable names from my presentations and reports, as a rule… but while doing that as automatically as possible.\nIt was incredibly helpful to use set_variable_labels() from the labelled package to make the variable names ready for reporting. My favorite trick was to combine this function with the to_title_case() function from the snakecase package. The latter will take any variable of the format snake_case (i.e. all lowercase, with underscores between words), remove the underscores, and capitalize the first letter of each word – just like a title.\nIf you use to_title_case(names(.)) in the .labels global argument of the set_variable_names() function, it’ll clean up most variables in an extremely intuitive and readable way. Then if there are variables (such as acronyms) that are still not labelled the way you’d prefer them to be, you can directly change them by listing the variable name and the character string you’d like it to say instead. We can do this for BMI, so it does not read “Bmi” when to_title_case transforms the label.\npatients %&gt;% \n  # select vars of interest for tables\n  select(age, sex, bmi, current_smoker, immunosuppressed) %&gt;%\n  # edit variable names using labelled package\n  labelled::set_variable_labels(\n    # change all variable labels to \"Title Case\"\n    .labels = snakecase::to_title_case(names(.)),\n    # change any extra variables that are not title case, like BMI\n    bmi = \"BMI\"\n    ) %&gt;%\n  tbl_summary(\n    # don't show missing (unknown) values\n    missing = \"no\",\n    # make sure all numeric variables are reported as continuous\n    type = list(all_numeric() ~ \"continuous\")\n  ) %&gt;%\n  # bold the labels\n  bold_labels()\nWarning: `all_numeric()` was deprecated in gtsummary 1.3.6.\nThe {tidyselect} and {dplyr} packages have implemented functions to select variables by class and type, and the {gtsummary} version is now deprecated.\n\nUse `where(is.numeric)` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\nWarning: The `fmt_missing()` function is deprecated and will soon be removed\n* Use the `sub_missing()` function instead\n\n\n\n\n\n\n\nCharacteristic\nN = 51\n\n\n\n\nAge\n61 (49, 64)\n\n\nSex\n\n\n\nFemale\n2 (40%)\n\n\nMale\n3 (60%)\n\n\nBMI\n27.3 (23.4, 30.8)\n\n\nCurrent Smoker\n2 (40%)\n\n\nImmunosuppressed\n3 (60%)\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\nSo clean and readable, with such little effort! A few other lines of code I usually add to make the tables nicer with very little effort are missing = \"no\" to the main tbl_summary() argument, and bold_labels() after.\n\n\n3. dplyr::filter() + stringr::str_detect() + tolower()\nThis has been such a life saver when searching through very long-form COVID-19 data for a particular lab, vital sign, medication, or order of interest. I often don’t know the exact word, much less capitalization, of the string I’m looking for in a data set, but this combination of functions really saves the day.\nHere we can use it to figure out the name of patients’ absolute lymphocyte count labs. Of course we could just do something like:\nsort(unique(labs$lab_name))\n[1] “Absolute Lymphocyte Count” “C-Reactive Protein”\n[3] “D-Dimer” “Lactate Dehydrogenase”\n[5] “LYMPHOCYTE PERC” “Platelet Count”\nBut in my actual data sets of upwards of 50 million rows, this would return thousands of results! I discovered the easiest way to find what I was looking for was to first convert the lab_name to all lowercase using tolower(lab_name), and then search for strings using str_detect(), only keeping rows that matched using filter() and then looking at those unique values using pull()* and unique().\nlabs %&gt;%\n  filter(str_detect(tolower(lab_name), \"lymph\")) %&gt;%\n  pull(lab_name) %&gt;%\n  unique() \nfilter: removed 1,078 rows (66%), 545 rows remaining\n[1] “Absolute Lymphocyte Count” “LYMPHOCYTE PERC”\nAlthough you don’t have to use pipes to do this, I often used those filtered rows for other exploratory data checks, such as looking at the units or distributions of the tests, so it was helpful to have it in the “tidy” format.\n*pull() is a somewhat lesser known function of dplyr that extracts a column as a dimensionless vector from a data frame, rather than selecting a single column, which R still treats as a data frame.\n\n\n4. readr::parse_number()\nThis is handy whenever I’m dealing with test results that can contain values above or below a detection range. Instead of using as.numeric() on a vector, I’ve switched to always using readr’s parse_number() function. This is because as.numeric() will turn values with meaningful information (such as “&gt;1,000”) into NA.\nLet’s look at this with the D-Dimer values in patients_labs. If you’ve been keeping up with any of the cytokine storm in COVID-19 headlines, you’ll know that D-Dimers are often sky-high in severely ill COVID-19 patients. So high, that they’re sometimes out of detectable ranges*!\npatient_labs %&gt;%\n  filter(str_detect(tolower(lab_name), \"dimer\")) %&gt;%\n  select(lab_name, lab_value) %&gt;%\n  mutate(lab_value_numeric = as.numeric(lab_value),\n         lab_value_parsed_number = readr::parse_number(lab_value))\nfilter: removed 536 rows (85%), 94 rows remaining\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n\n\nA tibble: 94 × 4\nlab_name lab_value lab_value_numeric lab_value_parsed_number     1 D-Dimer 642 642 642 2 D-Dimer 531 531 531 3 D-Dimer 674 674 674 4 D-Dimer 751 751 751 5 D-Dimer 436 436 436 6 D-Dimer 181 181 181 7 D-Dimer 107 107 107 8 D-Dimer &gt;1000 NA 1000 9 D-Dimer 251 251 251 10 D-Dimer 148 148 148 # … with 84 more rows\nWhen we use as.numeric() to switch the values from strings, we lose those out of range values. Those are our sickest patients, so if we continue with the analysis, we’ll definitely bias our results. However, if we use parse_number() we can at least evaluate those patients conservatively, using the upper bound of the test detection range.\n*Detectable range is usually &gt;55,000 ng/mL, but for the sake of demonstration, let’s pretend it’s &gt;1,000 of the mystery units in my fake data set.\n\n\n5. lubridate’s %within% + interval() + hours()\nLast but not least is two beautiful functions from the lubridate package. If you ever find yourself working with time-stamped data, you should definitely check it out. I tried multiple functions and packages at the beginning of the outbreak, and in the end, nothing compared to lubridate, at least for my use-cases.\nThe %within% function allows you to determine whether a time value (stored in R as a POSIXct object) falls within a window of time. You can specify this window of time using the interval() function. If you have only the start or the end time of the window of interest, you can add or subtract using supplemental functions like days() or hours().\nFor my COVID-19 research, the pulmonologists were often interested in snapshots of patients relative to a certain time in their disease course, for example, within the first 24 hours after intubation. The lubridate functions made it super easy to extract the labs, vital signs, or other information that happened relative to another date.\nHere’s how we could use the aforementioned functions in the patient_labs dataset we made previously. We can extract all the labs relative to 24 hours after the hospital admission date.\nlibrary(lubridate)\n\nAttaching package: 'lubridate'\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\npatient_labs %&gt;%\n  filter(lab_time %within% interval(admit_dt, admit_dt + days(1)))\nfilter: removed 623 rows (99%), 7 rows remaining\n\n\nA tibble: 7 × 11\n id admit_dt            death_or_discharge_dt   age sex    current…¹ immun…²\n      \n1 100 2020-03-21 00:10:00 2020-05-13 12:10:00 64 Male Yes No\n2 104 2020-04-03 12:15:00 2020-04-29 18:34:00 25 Male Yes No\n3 106 2020-03-28 12:22:00 2020-04-05 19:18:00 49 Female No Yes\n4 106 2020-03-28 12:22:00 2020-04-05 19:18:00 49 Female No Yes\n5 106 2020-03-28 12:22:00 2020-04-05 19:18:00 49 Female No Yes\n6 106 2020-03-28 12:22:00 2020-04-05 19:18:00 49 Female No Yes\n7 107 2020-04-10 18:15:00 2020-04-14 19:12:00 88 Male No Yes\n# … with 4 more variables: bmi , lab_time , lab_name , # lab_value , and abbreviated variable names ¹​current_smoker, # ²​immunosuppressed\nPerfect! I could go on to informatively join this data to another using tidylog, make tables with clean labels using tbl_summary(), set_variable_labels(), and to_title_case(), find further labs of interest with filter(), str_detect(), and tolower(), and make the lab values numeric with parse_number()… the possibilities are endless!\nI hope you enjoyed a quick insight into the data side of COVID-19 research, and/or perhaps picked up some useful function combinations for your own applied work with R."
  },
  {
    "objectID": "blog/sl/superlearning.html",
    "href": "blog/sl/superlearning.html",
    "title": "Become a Superlearner! An Illustrated Guide to Superlearning",
    "section": "",
    "text": "HTML Image as link"
  },
  {
    "objectID": "blog/sl/superlearning.html#initial-set-up-load-libraries-set-seed-simulate-data",
    "href": "blog/sl/superlearning.html#initial-set-up-load-libraries-set-seed-simulate-data",
    "title": "Become a Superlearner! An Illustrated Guide to Superlearning",
    "section": "Initial set-up: Load libraries, set seed, simulate data",
    "text": "Initial set-up: Load libraries, set seed, simulate data\nFor simplicity I’ll show the concept of superlearning using only four variables (AKA features or predictors) to predict a continuous outcome. Let’s first simulate a continuous outcome, y, and four potential predictors, x1, x2, x3, and x4.\n\nlibrary(tidyverse)\nlibrary(gt)\nset.seed(7)\n\n\nn &lt;- 5000\nobs &lt;- tibble(\n  id = 1:n,\n  x1 = rnorm(n),\n  x2 = rbinom(n, 1, plogis(10*x1)),\n  x3 = rbinom(n, 1, plogis(x1*x2 + .5*x2)),\n  x4 = rnorm(n, mean=x1*x2, sd=.5*x3),\n  y = x1 + x2 + x2*x3 + sin(x4)\n)\ngt(head(obs)) %&gt;%\n  tab_header(\"Simulated data set\") %&gt;%\n  fmt_number(everything(),decimals=3)\n\n\n\n\n\n\n\nSimulated data set\n\n\nid\nx1\nx2\nx3\nx4\ny\n\n\n\n\n1.000\n2.287\n1.000\n1.000\n1.385\n5.270\n\n\n2.000\n−1.197\n0.000\n0.000\n0.000\n−1.197\n\n\n3.000\n−0.694\n0.000\n0.000\n0.000\n−0.694\n\n\n4.000\n−0.412\n0.000\n1.000\n−0.541\n−0.928\n\n\n5.000\n−0.971\n0.000\n0.000\n0.000\n−0.971\n\n\n6.000\n−0.947\n0.000\n1.000\n−0.160\n−1.107\n\n\n\n\n\n\n\n\n\n\nStep 1: Split data into K folds\n\n\n\n\n The superlearner algorithm relies on K-fold cross-validation (CV) to avoid overfitting. We will start this process by splitting the data into 10 folds. The easiest way to do this is by creating indices for each CV fold.\n\nk &lt;- 10 # 10 fold cv\ncv_index &lt;- sample(rep(1:k, each = n/k)) # create indices for each CV fold. We need each fold K to contain n (all the rows of our data set) divided by k rows. in our example this is 5000/10 = 500 rows in each fold\n\n\n\n\nStep 2: Fit base learners for first CV-fold\n\n\n\n\n\nRecall that in K-fold CV, each fold serves as the validation set one time. In this first round of CV, we will train all of our base learners on all the CV folds (k = 1,2,…,9) except for the very last one: cv_index == 10.\nThe individual algorithms or base learners that we’ll use here are three linear regressions with differently specified parameters:\n\nLearner A: \\(Y=\\beta_0 + \\beta_1 X_2 + \\beta_2 X_4 + \\epsilon\\)\nLearner B: \\(Y=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_3 + \\beta_4 sin(X_4) + \\epsilon\\)\nLearner C: \\(Y=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_1 X_2 + \\beta_5 X_1 X_3 + \\beta_6 X_2 X_3 + \\beta_7 X_1 X_2 X_3 + \\epsilon\\)\n\n\ncv_train_1 &lt;- obs[-which(cv_index == 10),] # make a data set that contains all observations except those in k=1\nfit_1a &lt;- glm(y ~ x2 + x4, data=cv_train_1) # fit the first linear regression on that training data\nfit_1b &lt;- glm(y ~ x1 + x2 + x1*x3 + sin(x4), data=cv_train_1) # second LR fit on the training data\nfit_1c &lt;- glm(y ~ x1*x2*x3, data=cv_train_1) # and the third LR\n\nI am only using the linear regressions so that code for running more complicated regressions does not take away from understanding the general superlearning algorithm.\nSuperlearning actually works best if you use a diverse set, or superlearner library, of base learners. For example, instead of three linear regressions, we could use a least absolute shrinkage estimator (LASSO), random forest, and multivariate adaptive splines (MARS). Any parametric or non-parametric supervised machine learning algorithm can be included as a base learner.\n\n\n\nStep 3: Obtain predictions for first CV-fold\n\n\n\n\n\nWe can then get use our validation data, cv_index == 10, to obtain our first set of cross-validated predictions.\n\ncv_valid_1 &lt;- obs[which(cv_index == 10),] # make a data set that only contains observations except in k=10\npred_1a &lt;- predict(fit_1a, newdata = cv_valid_1) # use that data set as the validation for all the models in the SL library\npred_1b &lt;- predict(fit_1b, newdata = cv_valid_1) \npred_1c &lt;- predict(fit_1c, newdata = cv_valid_1)\n\nSince we have 5000 observations, that gives us three vectors of length 500: a set of predictions for each of our Learners A, B, and C.\n\nlength(pred_1a) # double check we only have n/k predictions ...we do :-)\n\n[1] 500\n\nhead(cbind(pred_1a, pred_1b, pred_1c)) %&gt;%\n  as.data.frame() %&gt;% gt() %&gt;%\n  fmt_number(everything(), decimals = 2) %&gt;%\n  tab_header(\"First CV round of predictions\") \n\n\n\n\n\n\n\nFirst CV round of predictions\n\n\npred_1a\npred_1b\npred_1c\n\n\n\n\n−0.77\n−0.81\n−0.69\n\n\n2.54\n1.89\n1.63\n\n\n0.19\n0.60\n−0.32\n\n\n2.46\n2.69\n2.98\n\n\n3.63\n4.04\n3.73\n\n\n3.59\n3.23\n3.15\n\n\n\n\n\n\n\n\n\n\nStep 4: Obtain CV predictions for entire data set\n\n\n\n\n\nWe’ll want to get those predictions for every fold. So, using your favorite for loop, apply statement, or mapping function, fit the base learners and obtain predictions for each of them, so that there are 1000 predictions – one for every point in observations.\nThe way I chose to code this was to make a generic function that combines Step 2 (base learners fit to the training data) and Step 3 (predictions on the validation data), then use map_dfr() from the purrr package to repeat over all 10 CV folds. I saved the results in a new data frame called cv_preds.\n\ncv_folds &lt;- as.list(1:k)\nnames(cv_folds) &lt;- paste0(\"fold\",1:k)\n\nget_preds &lt;- function(fold){   # function that does the same procedure as step 2 and 3 for any CV fold\n  cv_train &lt;- obs[-which(cv_index == fold),]  # make a training data set that contains all data except fold k\n  fit_a &lt;- glm(y ~ x2 + x4, data=cv_train)  # fit all the base learners to that data\n  fit_b &lt;- glm(y ~ x1 + x2 + x1*x3 + sin(x4), data=cv_train)\n  fit_c &lt;- glm(y ~ x1*x2*x3, data=cv_train)\n  cv_valid &lt;- obs[which(cv_index == fold),]  # make a validation data set that only contains data from fold k\n  pred_a &lt;- predict(fit_a, newdata = cv_valid)  # obtain predictions from all the base learners for that validation data\n  pred_b &lt;- predict(fit_b, newdata = cv_valid)\n  pred_c &lt;- predict(fit_c, newdata = cv_valid)\n  return(data.frame(\"obs_id\" = cv_valid$id, \"cv_fold\" = fold, pred_a, pred_b, pred_c))  # save the predictions and the ids of the observations in a data frame\n}\n\ncv_preds &lt;- purrr::map_dfr(cv_folds, ~get_preds(fold = .x)) # map_dfr loops through every fold (1:k) and binds the rows of the listed results together\n\ncv_preds %&gt;% arrange(obs_id) %&gt;% head() %&gt;% as.data.frame() %&gt;% gt() %&gt;%\n  fmt_number(cv_fold:pred_c, decimals=2) %&gt;%\n  tab_header(\"All CV predictions for all three base learners\") \n\n\n\n\n\n\n\nAll CV predictions for all three base learners\n\n\nobs_id\ncv_fold\npred_a\npred_b\npred_c\n\n\n\n\n1\n7.00\n3.74\n5.42\n5.28\n\n\n2\n6.00\n−0.77\n−1.19\n−1.20\n\n\n3\n10.00\n−0.77\n−0.81\n−0.69\n\n\n4\n8.00\n−1.39\n−0.77\n−0.41\n\n\n5\n2.00\n−0.78\n−1.02\n−0.97\n\n\n6\n9.00\n−0.96\n−1.04\n−0.94\n\n\n\n\n\n\n\n\n\n\nStep 5: Choose and compute loss function of interest via metalearner\n\n\n\n\n\n\nThis is the key step of the superlearner algorithm: we will use a new learner, a metalearner, to take information from all of the base learners and create that new algorithm.\n\nNow that we have cross-validated predictions for every observation in the data set, we want to merge those CV predictions back into our main data set…\n\nobs_preds &lt;- \n  full_join(obs, cv_preds, by=c(\"id\" = \"obs_id\"))\n\n…so that we can minimize a final loss function of interest between the true outcome and each CV prediction. This is how we’re going to optimize our overall prediction algorithm: we want to make sure we’re “losing the least” in the way we combine our base learners’ predictions to ultimately make final predictions. We can do this efficiently by choosing a new learner, a metalearner, which reflects the final loss function of interest.\nFor simplicity, we’ll use another linear regression as our metalearner. Using a linear regression as a metalearner will minimize the Cross-Validated Mean Squared Error (CV-MSE) when combining the base learner predictions. Note that we could use a variety of parametric or non-parametric regressions to minimize the CV-MSE.\nNo matter what metalearner we choose, the predictors will always be the cross-validated predictions from each base learner, and the outcome will always be the true outcome, y.\n\nsl_fit &lt;- glm(y ~ pred_a + pred_b + pred_c, data = obs_preds)\nbroom::tidy(sl_fit) %&gt;% gt() %&gt;%\n  fmt_number(estimate:p.value, decimals=2) %&gt;%\n  tab_header(\"Metalearner regression coefficients\") \n\n\n\n\n\n\n\nMetalearner regression coefficients\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.00\n0.00\n−1.42\n0.16\n\n\npred_a\n−0.02\n0.00\n−4.75\n0.00\n\n\npred_b\n0.85\n0.01\n128.15\n0.00\n\n\npred_c\n0.17\n0.01\n30.07\n0.00\n\n\n\n\n\n\n\nThis metalearner provides us with the coefficients, or weights, to apply to each of the base learners. In other words, if we have a set of predictions from Learner A, B, and C, we can obtain our best possible predictions by starting with an intercept of -0.003, then adding -0.017 \\(\\times\\) predictions from Learner A, 0.854 \\(\\times\\) predictions from Learner B, and 0.165 \\(\\times\\) predictions from Learner C.\nFor more information on the metalearning step, check out the Appendix.\n\n\n\nStep 6: Fit base learners on entire data set\n\n\n\n\n\nAfter we fit the metalearner, we officially have our superlearner algorithm, so it’s time to input data and obtain predictions! To implement the algorithm and obtain final predictions, we first need to fit the base learners on the full data set.\n\nfit_a &lt;- glm(y ~ x2 + x4, data=obs)\nfit_b &lt;- glm(y ~ x1 + x2 + x1*x3 + sin(x4), data=obs)\nfit_c &lt;- glm(y ~ x1*x2*x3, data=obs)\n\n\n\n\nStep 7: Obtain predictions from each base learner on entire data set\n\n\n\n\n\nWe’ll use those base learner fits to get predictions from each of the base learners for the entire data set, and then we will plug those predictions into the metalearner fit. Remember, we were previously using cross-validated predictions, rather than fitting the base learners on the whole data set. This was to avoid overfitting.\n\npred_a &lt;- predict(fit_a)\npred_b &lt;- predict(fit_b)\npred_c &lt;- predict(fit_c)\nfull_data_preds &lt;- tibble(pred_a, pred_b, pred_c)\n\n\n\n\nStep 8: Use metalearner fit to weight base learners\n\n\n\n\n\nOnce we have the predictions from the full data set, we can input them to the metalearner, where the output will be a final prediction for y.\n\nsl_predictions &lt;- predict(sl_fit, newdata = full_data_preds)\ndata.frame(sl_predictions = head(sl_predictions)) %&gt;%\n  gt() %&gt;% fmt_number(sl_predictions, decimals=2) %&gt;%\ntab_header(\"Final SL predictions (manual)\") \n\n\n\n\n\n\n\nFinal SL predictions (manual)\n\n\nsl_predictions\n\n\n\n\n5.44\n\n\n−1.20\n\n\n−0.79\n\n\n−0.71\n\n\n−1.02\n\n\n−1.03\n\n\n\n\n\n\n\nAnd… that’s it! Those are our superlearner predictions for the full data set.\n\n\n\nStep 9: Obtain predictions on new data\n\n\n\n\nWe can now modify Step 7 and Step 8 to accommodate any new observation(s):\n\nTo predict on new data: 1. Use the fits from each base learner to obtain base learner predictions for the new observation(s). 2. Plug those base learner predictions into the metalearner fit.\n\nWe can generate a single new_observation to see how this would work in practice.\n\nnew_obs &lt;- tibble(x1 = .5, x2 = 0, x3 = 0, x4 = -3)\nnew_pred_a &lt;- predict(fit_a, new_obs)\nnew_pred_b &lt;- predict(fit_b, new_obs)\nnew_pred_c &lt;- predict(fit_c, new_obs)\nnew_pred_df &lt;- tibble(\"pred_a\" = new_pred_a, \"pred_b\" = new_pred_b, \"pred_c\" = new_pred_c)\npredict(sl_fit, newdata = new_pred_df)\n\n        1 \n0.1183103 \n\n\nOur superlearner model predicts that an observation with predictors \\(X_1=.5\\), \\(X_2=0\\), \\(X_3=0\\), and \\(X_4=-3\\) will have an outcome of \\(Y=0.118\\).\n\n\n\nStep 10 and beyond…\n\n\n\n\nWe could compute the MSE of the ensemble superlearner predictions.\n\nsl_mse &lt;- mean((obs$y - sl_predictions)^2)\nsl_mse\n\n[1] 0.01927392\n\n\nWe could also add more algorithms to our base learner library (we definitely should, since we only used linear regressions!), and we could write functions to tune these algorithms’ hyperparameters over various grids. For example, if we were to include random forest in our library, we may want to tune over a number of trees and maximum bucket sizes.\nWe can then cross-validate this entire process to evaluate the predictive performance of our superlearner algorithm. Alternatively, we could leave a hold-out training data set to evaluate the performance."
  },
  {
    "objectID": "blog/sl/superlearning.html#cv-risk-and-coefficient-weights",
    "href": "blog/sl/superlearning.html#cv-risk-and-coefficient-weights",
    "title": "Become a Superlearner! An Illustrated Guide to Superlearning",
    "section": "CV-Risk and Coefficient Weights",
    "text": "CV-Risk and Coefficient Weights\nWe can examine the cross-validated Risk (loss function), and the Coefficient (weight) given to each of the models.\n\nsl_fit\n\n\nCall:  \nSuperLearner(Y = obs$y, X = x_df, family = gaussian(), SL.library = c(\"SL.ranger\",  \n    \"SL.glmnet\", \"SL.earth\")) \n\n                     Risk      Coef\nSL.ranger_All 0.013672503 0.1606329\nSL.glmnet_All 0.097257031 0.0000000\nSL.earth_All  0.003181357 0.8393671\n\n\nFrom this summary we can see that the CV-risk (the default risk is MSE) in this library of base learners is smallest for SL.Earth. This translates to the largest coefficient, or weight, given to the predictions from earth.\nThe LASSO model implemented by glmnet has the largest CV-risk, and after the metalearning step, those predictions receive a coefficient, or weight, of 0. This means that the predictions from LASSO will not be incorporated into the final predictions at all."
  },
  {
    "objectID": "blog/sl/superlearning.html#obtaining-the-predictions",
    "href": "blog/sl/superlearning.html#obtaining-the-predictions",
    "title": "Become a Superlearner! An Illustrated Guide to Superlearning",
    "section": "Obtaining the predictions",
    "text": "Obtaining the predictions\nWe can extract the predictions easily via the SL.predict element of the SuperLearner fit object.\n\nhead(data.frame(sl_predictions = sl_fit$SL.predict)) %&gt;% gt() %&gt;%\n  fmt_number(everything(),decimals=2) %&gt;% tab_header(\"Final SL predictions (package)\") \n\n\n\n\n\n\n\nFinal SL predictions (package)\n\n\nsl_predictions\n\n\n\n\n5.28\n\n\n−1.19\n\n\n−0.68\n\n\n−0.87\n\n\n−0.97\n\n\n−1.08"
  },
  {
    "objectID": "blog/sl/superlearning.html#cross-validated-superlearner",
    "href": "blog/sl/superlearning.html#cross-validated-superlearner",
    "title": "Become a Superlearner! An Illustrated Guide to Superlearning",
    "section": "Cross-validated Superlearner",
    "text": "Cross-validated Superlearner\nRecall that we can cross-validate the entire model fitting process to evaluate the predictive performance of our superlearner algorithm. This is easy with the function CV.SuperLearner(). Beware, this gets computationally burdensome very quickly!\n\ncv_sl_fit &lt;- CV.SuperLearner(Y = obs$y, X = x_df, family = gaussian(),\n                     SL.library = c(\"SL.ranger\", \"SL.glmnet\", \"SL.earth\"))\n\nFor more information on the SuperLearner package, take a look at this vignette."
  },
  {
    "objectID": "blog/sl/superlearning.html#alternative-packages-to-superlearn",
    "href": "blog/sl/superlearning.html#alternative-packages-to-superlearn",
    "title": "Become a Superlearner! An Illustrated Guide to Superlearning",
    "section": "Alternative packages to superlearn",
    "text": "Alternative packages to superlearn\nOther packages freely available in R that can be used to implement the superlearner algorithm include sl3 (an update to the original Superlearner package), h2o, and caretEnsemble. I previously wrote a brief demo on using sl3 for an NYC R-Ladies demo.\nThe authors of tidymodels – a suite of packages for machine learning including recipes, parsnip, and rsample – recently came out with a new package to perform superlearning/stacking called stacks. Prior to this, Alex Hayes wrote a blog post on using tidymodels infrastructure to implement superlearning."
  },
  {
    "objectID": "talks/swimmers.html",
    "href": "talks/swimmers.html",
    "title": "Swimmer plots for longitudinal data using ggplot2",
    "section": "",
    "text": "The slide deck for my R/medicine talk is available [here].\nA recording is on Youtube."
  },
  {
    "objectID": "talks/ensemble.html",
    "href": "talks/ensemble.html",
    "title": "Introduction to Ensemble Learning and Super Learning",
    "section": "",
    "text": "Slide deck available [here]."
  },
  {
    "objectID": "art/illustrations_viz.html",
    "href": "art/illustrations_viz.html",
    "title": "Visual Guides for Causal Inference",
    "section": "",
    "text": "This visual guides for causal inference series describes the estimation procedure for a mean difference in outcomes under a binary exposure for four estimation methods: g-computation, inverse probability weighting, one-step estimation (coming soon!), and targeted maximum likelihood estimation. The latter two methods are doubly robust and can be used with machine learning regressions such as the ensemble learning method superlearning.\nThese guides are available on Github as pdfs and high resolution jpgs. They are free to use under a CC-BY license. Please provide attribution (Kat Hoffman).\n\nG-computation\n\n\n\n\n\nInverse Probability Weighting\n\n\n\n\n\nTargeted Maximum Likelihood Estimation\n\n\n\nSuperlearning",
    "crumbs": [
      "Visual Guides for Causal Inference"
    ]
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Selected slide decks are shown below. A full listing of talks is available on my CV.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nCorticosteroids in COVID-19: Optimizing Observational Research through Target Trial Emulations\n\n\n\n\n\n\n\nSep 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSwimmer plots for longitudinal data using ggplot2\n\n\n\n\n\n\n\nAug 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Ensemble Learning and Super Learning\n\n\n\n\n\n\n\nJan 19, 2021\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive/publications.html",
    "href": "archive/publications.html",
    "title": "Publications",
    "section": "",
    "text": "Last updated September 27, 2022– Pulled automatically from my Google Scholar profile. See this post for details.\n\n\n2022\n\n\n\n\nG Waldrop, SA Safavynia, ME Barra, S Agarwal, DA Berlin, AK Boehme et al. (2022) Prolonged Unconsciousness is Common in COVID‐19 and Associated with Hypoxemia, Annals of neurology,\n\n\nK Weidman, E LaFond, KL Hoffman, P Goyal, CN Parkhurst, H Derry-Vick et al. (2022) Post–Intensive Care Unit Syndrome in a Cohort of COVID-19 Survivors in New York City, Annals of the American Thoracic Society, 19 (7), 1158-1168\n\n\nDR Price, E Benedetti, KL Hoffman, L Gomez-Escobar, S Alvarez-Mulett et al. (2022) Angiopoietin 2 is associated with vascular necroptosis induction in coronavirus disease 2019 acute respiratory distress syndrome, The American journal of pathology, 192 (7), 1001-1015\n\n\nMJ Satlin, L Chen, A Gomez-Simmonds, J Marino, G Weston, T Bhowmick et al. (2022) Impact of a rapid molecular test for Klebsiella pneumoniae carbapenemase and ceftazidime-avibactam use on outcomes after bacteremia caused by carbapenem-resistant Enterobacterales, Clinical Infectious Diseases,\n\n\nM Buyukozkan, S Alvarez-Mulett, AC Racanelli, F Schmidt, R Batra et al. (2022) Integrative Metabolomic and Proteomic Signatures Define Clinical Outcomes in Severe COVID-19, medRxiv, 2021.07. 19.21260776\n\n\nX Wu, Y Jeong, SP de Frías, I Easthausen, K Hoffman, C Oromendia et al. (2022) Serum proteomic profiling of rheumatoid arthritis–interstitial lung disease with a comparison to idiopathic pulmonary fibrosis, Thorax, 77 (10), 1041-1044\n\n\nI Díaz, KL Hoffman, NS Hejazi (2022) Causal survival analysis under competing risks using longitudinal modified treatment policies, arXiv preprint arXiv:, :2202.03513\n\n\nJI Basem, AF Roth, RS White, VE Tangel, SY Jiang, JM Choi, KL Hoffman et al. (2022) Patient care in rapid-expansion intensive care units during the COVID-19 pandemic crisis, BMC anesthesiology, 22 (1), 1-11\n\n\nR Batra, W Whalen, S Alvarez-Mulett, LG Gomez-Escobar, KL Hoffman et al. (2022) Multi-omic comparative analysis of COVID-19 and bacterial sepsis-induced ARDS, PLoS pathogens, 18 (9), e1010819\n\n\nJK Krishnan, M Rajan, BR Baer, KL Hoffman, MN Alshak, KI Aronson et al. (2022) Assessing mortality differences across acute respiratory failure management strategies in Covid-19, Journal of Critical Care, 70, 154045\n\n\nE Coskun, R Kalil, A Yin, G Wehmeyer, K Hoffman, R Kopparam et al. (2022) Choices and Outcomes of the Oldest Old Admitted During the First Wave of COVID-19 in New York City, Journal of Palliative Care, 08258597221098130\n\n\nK Crane, J Choi, JK Krishnan, D Pan, L Lief, K Hoffman, P Goyal et al. (2022) Association Between 28-Day Mortality and Cumulative Hospital Caseloads During the First COVID-19 Surge, D, D21. RISK FACTORS AND PREDICTORS IN CRITICAL ILLNESS AND CHRONIC RESPIRATORY …\n\n\nWZ Zhang, W Simmons, K Kim, K Hoffman, D Redmond et al. (2022) Hyperferritinemia in COVID-19: A Mechanistic Role for Macrophage Ferritin in Acute Respiratory Distress Syndrome, D, D97. MACROPHAGES AND INFLAMMATION IN ACUTE LUNG INJURY, A5308-A5308\n\n\nJK Krishnan, KM Ancy, C Oromendia, KL Hoffman, I Easthausen et al. (2022) Characterizing COPD Symptom Variability in the Stable State Utilizing the Evaluating Respiratory Symptoms in COPD Questionnaire., Chronic Obstructive Pulmonary Diseases (Miami, Fla.),\n\n\nK Hoffman, E Schenck, MJ Satlin, W Whalen, D Pan, N Williams, I Diaz (2022) Corticosteroids in COVID-19: Optimizing Observational Research through Target Trial Emulations (preprint), ,\n\n\nR Batra, W Whalen, S Alvarez-Mulett, K Hoffman, W Simmons et al. (2022) Multi-omic comparative analysis of COVID-19 and bacterial sepsis-induced ARDS (preprint), ,\n\n\nK Weidman, E LaFond, KL Hoffman, P Goyal, CN Parkhurst, H Derry-Vick et al. (2022) Post-Intensive Care Unit Syndrome in a Cohort of COVID-19 Survivors in New York City., Ann Am Thorac Soc, 1158-1168\n\n\nKL Hoffman, EJ Schenck, M Satlin, W Whalen, D Pan, N Williams, I Díaz (2022) Corticosteroids in COVID-19: Optimizing Observational Research through Target Trial Emulations, medRxiv,\n\n\n\n\n2021\n\n\n\n\nM Reiterer, M Rajan, N Gómez-Banoy, JD Lau, LG Gomez-Escobar, L Ma et al. (2021) Hyperglycemia in acute COVID-19 is characterized by insulin resistance and adipose tissue infectivity by SARS-CoV-2, Cell metabolism, 33 (11), 2174-2188. e5\n\n\nI Díaz, N Williams, KL Hoffman, EJ Schenck (2021) Nonparametric causal effects based on longitudinal modified treatment policies, Journal of the American Statistical Association, 1-16\n\n\nC Su, Z Xu, K Hoffman, P Goyal, MM Safford, J Lee, S Alvarez-Mulett et al. (2021) Identifying organ dysfunction trajectory-based subphenotypes in critically ill patients with COVID-19, Scientific Reports, 11 (1), 1-13\n\n\nEJ Schenck, KL Hoffman, M Cusick, J Kabariti, ET Sholle, TR Campion Jr (2021) Critical carE Database for Advanced Research (CEDAR): An automated method to support intensive care units with electronic health record data, Journal of Biomedical Informatics, 118, 103789\n\n\nLG Gómez-Escobar, KL Hoffman, JJ Choi, A Borczuk, S Salvatore et al. (2021) Cytokine signatures of end organ injury in COVID-19, Scientific reports, 11 (1), 1-15\n\n\nM Reiterer, M Rajan, N Gómez-Banoy, JD Lau, LG Gomez-Escobar et al. (2021) Hyperglycemia in acute COVID-19 is characterized by adipose tissue dysfunction and insulin resistance, MedRxiv,\n\n\nU Khan, K Ho, EK Hwang, C Peña, J Brouwer, K Hoffman, D Betel et al. (2021) Impact of use of antibiotics on response to immune checkpoint inhibitors and tumor microenvironment, American Journal of Clinical Oncology, 44 (6), 247-253\n\n\nLK Torres, KL Hoffman, C Oromendia, I Diaz, JS Harrington, EJ Schenck et al. (2021) Attributable mortality of acute respiratory distress syndrome: a systematic review, meta-analysis and survival analysis using targeted minimum loss-based estimation, Thorax, 76 (12), 1176-1185\n\n\nDR Price, KL Hoffman, C Oromendia, LK Torres, EJ Schenck, ME Choi et al. (2021) Effect of neutropenic critical illness on development and prognosis of acute respiratory distress syndrome, American journal of respiratory and critical care medicine, 203 (4), 504-508\n\n\nWZ Zhang, KL Hoffman, KT Schiffer, C Oromendia, MC Rice et al. (2021) Association of plasma mitochondrial DNA with COPD severity and progression in the SPIROMICS cohort, Respiratory research, 22 (1), 1-11\n\n\nEJ Schenck, KL Hoffman, C Oromendia, E Sanchez, EJ Finkelsztein et al. (2021) A comparative analysis of the respiratory subscore of the sequential organ failure assessment scoring system, Annals of the American Thoracic Society, 18 (11), 1849-1860\n\n\nE Mauer, J Lee, J Choi, H Zhang, KL Hoffman, IJ Easthausen, M Rajan et al. (2021) A predictive model of clinical deterioration among hospitalized COVID-19 patients by harnessing hospital course trajectories, Journal of biomedical informatics, 118, 103794\n\n\nM Plataki, D Pan, P Goyal, K Hoffman, JMK Choi, H Huang, MM Safford et al. (2021) Association of body mass index with morbidity in patients hospitalised with COVID-19, BMJ open respiratory research, 8 (1), e000970\n\n\nC Su, KL Hoffman, Z Xu, E Sanchez, II Siempos, JS Harrington et al. (2021) Evaluation of Albumin Kinetics in Critically Ill Patients With Coronavirus Disease 2019 Compared to Those With Sepsis-Induced Acute Respiratory Distress Syndrome, Critical care explorations, 3 (12)\n\n\nDR Price, E Benedetti, KL Hoffman, L Gomez-Escobar, S Alvarez-Mulett et al. (2021) The maladaptive vascular response in COVID-19 acute respiratory distress syndrome and recovery, medRxiv,\n\n\nC Su, K Hoffman, X Zhenxing, E Sanchez, I Siempos, JS Harrington et al. (2021) Evaluation of albumin kinetics in mechanically ventilated patients with COVID-19 compared to those with sepsis-induced ARDS, medRxiv,\n\n\nDR Price, KL Hoffman, E Sanchez, AMK Choi, II Siempos (2021) Temporal trends of outcomes of neutropenic patients with ARDS enrolled in therapeutic clinical trials, Intensive care medicine, 47 (1), 122-123\n\n\nI Dıaz, N Williams, KL Hoffman, EJ Schenck (2021) Supplementary Materials for Non-parametric causal effects based on longitudinal modified treatment policies., ,\n\n\nJK Krishnan, MR Rajan, B Baer, C Ezeomah, SS Hill, MN Alshak et al. (2021) Assessing Mortality Difference Across COVID-19 Intubation Strategies, TP, P50. TP050 COVID: NONPULMONARY CRITICAL CARE, MECHANICAL VENTILATION …\n\n\nGM Mondellini, J Qi, L Braghieri, A Pinsino, AJ Kim, T Melie, VR Feldman et al. (2021) Effects of ACE Inhibitors, Angiotensin Receptor Blockers and Angiotensin Receptor-Neprilysin Inhibitors on Survival Free from Gastrointestinal Bleeding in HeartMate 3 Patients …, The Journal of Heart and Lung Transplantation, 40 (4), S445\n\n\nK Hoffman (2021) Covid‐19: a view from the sidelines, Significance, 18 (1), 40-43\n\n\nE Coskun, R Kalil, A Yin, G Wehmeyer, K Hoffman, R Kopparam, P Goyal et al. (2021) Outcomes of the Oldest Old Admitted with COVID-19, Journal of the American Geriatrics Society, S, S266-S266\n\n\n\n\n2020\n\n\n\n\nP Goyal, JJ Choi, LC Pinheiro, EJ Schenck, R Chen, A Jabri, MJ Satlin et al. (2020) Clinical characteristics of Covid-19 in New York city, New England Journal of Medicine, 382 (24), 2372-2374\n\n\nEJ Schenck, K Hoffman, P Goyal, J Choi, L Torres, K Rajwani, CW Tam et al. (2020) Respiratory mechanics and gas exchange in COVID-19–associated respiratory failure, Annals of the American Thoracic Society, 17 (9), 1158-1161\n\n\nP Goyal, JB Ringel, M Rajan, JJ Choi, LC Pinheiro, HA Li, GT Wehmeyer et al. (2020) Obesity and COVID-19 in New York City: a retrospective cohort study, Annals of internal medicine, 173 (10), 855-858\n\n\nA Rahman, E Schelbaum, K Hoffman, I Diaz, H Hristov, R Andrews, S Jett et al. (2020) Sex-driven modifiers of Alzheimer risk: a multimodality brain imaging study, Neurology, 95 (2), e166-e178\n\n\nWZ Zhang, MC Rice, KL Hoffman, C Oromendia, IZ Barjaktarevic et al. (2020) Association of urine mitochondrial DNA with clinical measures of COPD in the SPIROMICS cohort, JCI insight, 5 (3)\n\n\nT Chukir, Y Liu, K Hoffman, JP Bilezikian, A Farooki (2020) Calcitriol elevation is associated with a higher risk of refractory hypercalcemia of malignancy in solid tumors, The Journal of Clinical Endocrinology & Metabolism, 105 (4), e1115-e1123\n\n\nA Pinsino, GM Mondellini, EA Royzman, KL Hoffman, D D’Angelo et al. (2020) Cystatin C-versus creatinine-based assessment of renal function and prediction of early outcomes among patients with a left ventricular assist device, Circulation: Heart Failure, 13 (1), e006326\n\n\nK Andreadis, K Hoffman, D D'Angelo, L Sulica (2020) Sulcus vocalis: Results of excision without reconstruction, The Laryngoscope, 130 (9), 2208-2212\n\n\nX Wu, S Poli De Frias, S Taheri, K Hoffman, I Easthausen, AJ Esposito et al. (2020) Differential Protein Expression in Rheumatoid Arthritis Interstitial Lung Disease, D, D105. ILD EPIDEMIOLOGY II, A7796-A7796\n\n\nY Abdelghany, S ALMAraghi, R Simon, M Rice, K Hoffman, SA Kikkers et al. (2020) ASSOCIATION OF URINE AND BRONCHOALVEOLAR LAVAG LIPOCALIN-1 (NGAL) LEVELS WITH CLINICAL MEASURES OF COPD: AN ANALYSIS FROM THE SUBPOPULATIONS AND INTERMEDIATE OUTCOME MEASURES …, Chest, 158 (4), A1662-A1663\n\n\nK Ackerman, K Hoffman, I Diaz, K Ballman, R Kodiyanplakkal, AMK Choi et al. (2020) Effect of Sepsis on Death as Modified by Solid Organ Transplantation, A, A41. CRITICAL CARE: PREDICTING AND MEASURING OUTCOMES, A1631-A1631\n\n\nLK Torres, K Hoffman, C Oromendia, I Diaz, E Schenck et al. (2020) Attributable Mortality of Acute Respiratory Distress Syndrome in Critically Ill Septic Patients: Estimation Using a Novel Causal Inference Method, A, A41. CRITICAL CARE: PREDICTING AND MEASURING OUTCOMES, A1645-A1645\n\n\nHS Bhatia, J Bailey, O Unlu, K Hoffman, RJ Kim (2020) Response to “Direct oral anticoagulants in patients with atrial fibrillation and chronic kidney disease”, Pacing and Clinical Electrophysiology, 43 (2), 267-267\n\n\n\n\n2019\n\n\n\n\nCP Soneru, CA Riley, K Hoffman, A Tabaee, TH Schwartz (2019) Intra-operative MRI vs endoscopy in achieving gross total resection of pituitary adenomas: a systematic review, Acta Neurochirurgica, 161 (8), 1683-1698\n\n\nU Khan, C Peña, J Brouwer, K Hoffman, AR Choudhury, C Zhang et al. (2019) Impact of antibiotic use on response to treatment with immune checkpoint inhibitors., Journal of Clinical Oncology, 37 (4_suppl), 143-143\n\n\nHS Bhatia, J Bailey, O Unlu, K Hoffman, RJ Kim (2019) Efficacy and safety of direct oral anticoagulants in patients with atrial fibrillation and chronic kidney disease, Pacing and Clinical Electrophysiology, 42 (11), 1463-1470\n\n\nA Pinsino, A Gaudig, KL Hoffman, D D'Angelo, EA Royzman et al. (2019) Digoxin is Associated with Decreased Survival Free from Hemocompatibility-Related Adverse Events in LVAD Patients-A Propensity Score Matched Analysis, The Journal of Heart and Lung Transplantation, 38 (4), S170\n\n\nL Comisar, E Schenck, K Hoffman, C Oromendia, I Siempos, E Sanchez et al. (2019) IMPACT OF BMI ON OUTCOME OF PATIENTS WITH HEMATOLOGIC MALIGNANCIES IN THE ICU, Chest, 156 (4), A1143-A1144\n\n\nAM Zuver, KL Hoffman, D D'Angelo, D Onat, EA Royzman, JC Hupf et al. (2019) Microbiome-Derived TMAO Exhibits No Protective Effect against GIB in LVAD Patients, The Journal of Heart and Lung Transplantation, 38 (4), S195-S196\n\n\n\n\n2015\n\n\n\n\nK Hoffman (2015) A Beautiful Affliction: Tracing the Role of Mentally Ill Artists in Reducing the Negative Stigma of Mental Illness, University of Detroit Mercy,"
  },
  {
    "objectID": "art/illustrations.html",
    "href": "art/illustrations.html",
    "title": "Illustrations",
    "section": "",
    "text": "I enjoy making educational visualizations, including technical guides, summary graphics, and comics. These are all available on Github and free to use under a CC-BY license. Please provide attribution (Kat Hoffman)."
  },
  {
    "objectID": "art/illustrations.html#visual-guides-for-causal-inference",
    "href": "art/illustrations.html#visual-guides-for-causal-inference",
    "title": "Illustrations",
    "section": "Visual Guides for Causal Inference",
    "text": "Visual Guides for Causal Inference\n\nG-computation\n\n\n\n\n\nInverse Probability Weighting\n\n\n\n\n\nTargeted Maximum Likelihood Estimation\n\n\n\nSuperlearning\n\n\n\n\nIdentification vs. Estimation in Causal Inference\n\n\n\nStatic, Dynamic, and Modified Treatment Policies"
  },
  {
    "objectID": "art/illustrations.html#comics",
    "href": "art/illustrations.html#comics",
    "title": "Illustrations",
    "section": "Comics",
    "text": "Comics"
  },
  {
    "objectID": "art/illustrations_draw.html",
    "href": "art/illustrations_draw.html",
    "title": "Educational Drawings and Comics",
    "section": "",
    "text": "These educational illustrations and comics are all available on Github and free to use under a CC-BY license. Please provide attribution (Kat Hoffman).",
    "crumbs": [
      "Educational Drawings and Comics"
    ]
  },
  {
    "objectID": "art/illustrations_draw.html#comics",
    "href": "art/illustrations_draw.html#comics",
    "title": "Educational Drawings and Comics",
    "section": "Comics",
    "text": "Comics",
    "crumbs": [
      "Educational Drawings and Comics"
    ]
  },
  {
    "objectID": "talks/steroids.html",
    "href": "talks/steroids.html",
    "title": "Corticosteroids in COVID-19: Optimizing Observational Research through Target Trial Emulations",
    "section": "",
    "text": "Slide deck for a talk corresponding to the paper Comparison of a Target Trial Emulation Framework to Cox Regression to Estimate the Effect of Corticosteroids on COVID-19 Mortality (Hoffman et al. 2022) is available [here]."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Katherine (Kat) Hoffman",
    "section": "",
    "text": "Katherine (Kat) Hoffman\nI am an MS-level biostatistician and third-year Biostatistics PhD student at the University of Washington. I am advised by Dr. Marco Carone and develop statistical methodology motivated by challenges that arise in infectious disease and vaccine research. My work is supported by Dr. Helen Chu in the Department of Medicine.\nI am driven by the goal of advancing rigorous, reproducible biomedical research. My primary interests lie in causal inference, particularly for settings involving time-varying or continuous exposures and time-to-event outcomes.\nOutside of my research collaborations, I enjoy writing blog posts about statistics, programming, and career-related topics. For many of my posts I create illustrations and infographics, such as my Visual Guides for Causal Inference series."
  },
  {
    "objectID": "blog/sl/sl.html",
    "href": "blog/sl/sl.html",
    "title": "Using {sl3} for superlearning",
    "section": "",
    "text": "A short and sweet guide to using the R package {sl3} for superlearning. This is part of a tutorial created for an R Ladies NYC Talk in 2019.\nJanuary 10, 2020.\nIn September 2019, I gave an R-Ladies NYC presentation about using the package sl3 to implement the superlearner algorithm for predictions. You can download the slides for it here. This post is a modification to the original demo I gave.\nFor a more in-depth background on what the superlearner algorithm is, please see my more recent blog post."
  },
  {
    "objectID": "blog/sl/sl.html#an-aside-exploring-sl3s-many-options",
    "href": "blog/sl/sl.html#an-aside-exploring-sl3s-many-options",
    "title": "Using {sl3} for superlearning",
    "section": "An aside: Exploring sl3’s many options",
    "text": "An aside: Exploring sl3’s many options\nThere’s a ton of different aspects of model fitting sl3 has the capabilities to address. For example, we can look into algorithms for when the outcome is binomial, categorical, or continuous. There are also options for when you have clustered data, or if you need to preprocess/screen your data before implementing base learners.\n\nsl3_list_properties()\n\n [1] \"binomial\"      \"categorical\"   \"continuous\"    \"cv\"           \n [5] \"density\"       \"h2o\"           \"ids\"           \"importance\"   \n [9] \"offset\"        \"preprocessing\" \"sampling\"      \"screener\"     \n[13] \"timeseries\"    \"weights\"       \"wrapper\"      \n\n\nWe can learn more about each of these properties on this reference page."
  },
  {
    "objectID": "blog/sl/sl.html#another-aside-looking-at-available-learners",
    "href": "blog/sl/sl.html#another-aside-looking-at-available-learners",
    "title": "Using {sl3} for superlearning",
    "section": "Another aside: looking at available “learners”",
    "text": "Another aside: looking at available “learners”\nWe’ll need to pick out base learners for our stack, as well as pick a metalearner. Since we are trying to predict z-scores, a continuous variable, let’s look at our potential learners for a continuous variable.\n\nsl3_list_learners(\"continuous\") \n\n [1] \"Lrnr_arima\"                     \"Lrnr_bartMachine\"              \n [3] \"Lrnr_bayesglm\"                  \"Lrnr_bilstm\"                   \n [5] \"Lrnr_bound\"                     \"Lrnr_caret\"                    \n [7] \"Lrnr_cv_selector\"               \"Lrnr_dbarts\"                   \n [9] \"Lrnr_earth\"                     \"Lrnr_expSmooth\"                \n[11] \"Lrnr_ga\"                        \"Lrnr_gam\"                      \n[13] \"Lrnr_gbm\"                       \"Lrnr_glm\"                      \n[15] \"Lrnr_glm_fast\"                  \"Lrnr_glmnet\"                   \n[17] \"Lrnr_grf\"                       \"Lrnr_gru_keras\"                \n[19] \"Lrnr_gts\"                       \"Lrnr_h2o_glm\"                  \n[21] \"Lrnr_h2o_grid\"                  \"Lrnr_hal9001\"                  \n[23] \"Lrnr_HarmonicReg\"               \"Lrnr_hts\"                      \n[25] \"Lrnr_lightgbm\"                  \"Lrnr_lstm_keras\"               \n[27] \"Lrnr_mean\"                      \"Lrnr_multiple_ts\"              \n[29] \"Lrnr_nnet\"                      \"Lrnr_nnls\"                     \n[31] \"Lrnr_optim\"                     \"Lrnr_pkg_SuperLearner\"         \n[33] \"Lrnr_pkg_SuperLearner_method\"   \"Lrnr_pkg_SuperLearner_screener\"\n[35] \"Lrnr_polspline\"                 \"Lrnr_randomForest\"             \n[37] \"Lrnr_ranger\"                    \"Lrnr_rpart\"                    \n[39] \"Lrnr_rugarch\"                   \"Lrnr_screener_correlation\"     \n[41] \"Lrnr_solnp\"                     \"Lrnr_stratified\"               \n[43] \"Lrnr_svm\"                       \"Lrnr_tsDyn\"                    \n[45] \"Lrnr_xgboost\"                  \n\n\nYou’ll notice each learner starts with Lrnr and seems to correspond to a package in R."
  },
  {
    "objectID": "blog/sl/sl.html#examine-coefficients-and-cv-risk",
    "href": "blog/sl/sl.html#examine-coefficients-and-cv-risk",
    "title": "Using {sl3} for superlearning",
    "section": "Examine coefficients and CV-risk",
    "text": "Examine coefficients and CV-risk\nThe default risk is MSE (Mean Squared Error). The coefficients show you how the metalearner decided to weight each base model for the final ensemble.\n\nsl_fit$print() %&gt;% gt()\n\n[1] \"SuperLearner:\"\nList of 3\n $ : chr \"Lrnr_randomForest_500_TRUE_5\"\n $ : chr \"Lrnr_gbm_10000_2_0.001\"\n $ : chr \"Lrnr_glm_TRUE\"\n[1] \"Lrnr_glm_TRUE\"\n$coefficients\n                   intercept Lrnr_randomForest_500_TRUE_5 \n                -0.037630882                  0.056299184 \n      Lrnr_gbm_10000_2_0.001                Lrnr_glm_TRUE \n                 0.876353346                  0.005369642 \n\n$R\n                             intercept Lrnr_randomForest_500_TRUE_5\nintercept                    -68.52007                     39.84692\nLrnr_randomForest_500_TRUE_5   0.00000                     21.59036\nLrnr_gbm_10000_2_0.001         0.00000                      0.00000\nLrnr_glm_TRUE                  0.00000                      0.00000\n                             Lrnr_gbm_10000_2_0.001 Lrnr_glm_TRUE\nintercept                                  40.07621     40.091631\nLrnr_randomForest_500_TRUE_5               14.60067     13.860606\nLrnr_gbm_10000_2_0.001                     10.05776      9.862373\nLrnr_glm_TRUE                               0.00000     -8.642721\n\n$rank\n[1] 4\n\n$family\n\nFamily: gaussian \nLink function: identity \n\n\n$deviance\n[1] 4723.324\n\n$aic\n[1] 13362.07\n\n$null.deviance\n[1] 5000.347\n\n$iter\n[1] 2\n\n$df.residual\n[1] 4691\n\n$df.null\n[1] 4694\n\n$converged\n[1] TRUE\n\n$boundary\n[1] FALSE\n\n$linkinv_fun\nfunction (eta) \neta\n&lt;environment: namespace:stats&gt;\n\n$link_fun\nfunction (mu) \nmu\n&lt;environment: namespace:stats&gt;\n\n$training_offset\n[1] FALSE\n\n[1] \"Cross-validated risk:\"\n                        learner coefficients      MSE        se    fold_sd\n1: Lrnr_randomForest_500_TRUE_5  0.056299184 1.034798 0.0237462 0.07053924\n2:       Lrnr_gbm_10000_2_0.001  0.876353346 1.006393 0.0233679 0.07188694\n3:                Lrnr_glm_TRUE  0.005369642 1.021995 0.0238539 0.06474016\n4:                 SuperLearner           NA 1.006033 0.0233913 0.07107934\n   fold_min_MSE fold_max_MSE\n1:    0.9099614     1.165088\n2:    0.8753148     1.121065\n3:    0.8900577     1.116998\n4:    0.8772851     1.119882\n\n\n\n\n\n\n\n\nlearner\ncoefficients\nMSE\nse\nfold_sd\nfold_min_MSE\nfold_max_MSE\n\n\n\n\nLrnr_randomForest_500_TRUE_5\n0.056299184\n1.034798\n0.0237462\n0.07053924\n0.9099614\n1.165088\n\n\nLrnr_gbm_10000_2_0.001\n0.876353346\n1.006393\n0.0233679\n0.07188694\n0.8753148\n1.121065\n\n\nLrnr_glm_TRUE\n0.005369642\n1.021995\n0.0238539\n0.06474016\n0.8900577\n1.116998\n\n\nSuperLearner\nNA\n1.006033\n0.0233913\n0.07107934\n0.8772851\n1.119882"
  },
  {
    "objectID": "blog/sl/sl.html#look-at-the-predictions",
    "href": "blog/sl/sl.html#look-at-the-predictions",
    "title": "Using {sl3} for superlearning",
    "section": "Look at the predictions",
    "text": "Look at the predictions\npredict() allows you to see what the model predicts on any given task. Here we look at predictions from the same data we trained the superlearner on, so the predicted weight to height z-scores of the first six children in our data set.\n\nsl_fit$predict(washb_task) %&gt;% head()\n\n[1] -0.6641946 -0.7508882 -0.7014290 -0.7542267 -0.6456398 -0.6791542"
  },
  {
    "objectID": "blog/lmtp/lmtp.html#lets-review-causal-inference-at-a-high-level.",
    "href": "blog/lmtp/lmtp.html#lets-review-causal-inference-at-a-high-level.",
    "title": "An Illustrated Guide to Modified Treatment Policies, Part 1: Introduction and Motivation",
    "section": "Let’s review causal inference at a high level.",
    "text": "Let’s review causal inference at a high level.\n\n\n\nHTML Image as link\n\n\n\n  \n\n\nMost modern causal inference methods utilize the potential outcomes framework, which asks questions of what would happen in a hypothetical world in which we could manipulate exposures and then compare outcomes for the same observations under different exposures.\nQuestions with binary exposures typically take the form, “what would happen if everyone had received the exposure compared to if no one had received the exposure?” Similarly, questions with multi-level or continuous exposures often take the form “what would happen if everyone had received this much of the exposure, versus if everyone had received that much of the exposure?” Study designs like this which compare all observations under two (or more) specific levels of exposure are called static interventions."
  },
  {
    "objectID": "blog/lmtp/lmtp.html#static-interventions-are-a-common-approach-in-causal-inference-but-a-few-problems-frequently-arise.",
    "href": "blog/lmtp/lmtp.html#static-interventions-are-a-common-approach-in-causal-inference-but-a-few-problems-frequently-arise.",
    "title": "An Illustrated Guide to Modified Treatment Policies, Part 1: Introduction and Motivation",
    "section": "Static interventions are a common approach in causal inference, but a few problems frequently arise.",
    "text": "Static interventions are a common approach in causal inference, but a few problems frequently arise.\n\nProblem #1: Positivity Violations\n\nA key requirement for causal inference is the positivity assumption. This means if you are comparing two exposures (say, treatments A vs. B), for every covariate combination in your dataset, there must be individuals who sometimes get treatment A and sometimes get treatment B. Intuitively, this ensures there is enough “experimentation” in the dataset that effects can be estimated.\n\n\n\n\n  \n\n\nAs you can imagine, this often does not hold true. Sometimes there are theoretical positivity violations: for example, if we are studying the effects of a drug for heart disease, cardiologists might say this drug is never given to people who also have chronic kidney disease because the drug damages the kidneys. The probability of a patient receiving the drug conditional on having kidney disease will be zero in all datasets.\nOther times we see practical positivity violations, meaning that due to small sample size and/or random chance, there are certain covariate combinations in which few observations receive the treatment of interest in our dataset. Perhaps in a dataset to study the previous example’s heart disease drug, due to random chance, there are no women aged 40-60 years who received the drug in the data. It is not a theoretical positivity violation (i.e. doctors would give the drug to women in this age group), but it is a practical violation nonetheless as it yields small or zero treatment probabilities.\nPositivity issues must be addressed, preferably in the planning stage of a study.\n\nProblem #2: Unrealistic Interventions\n\nA second problem arises with static interventions: there are many situations in which it is not realistic to set everyone’s hypothetical exposure to the same value. Let’s take an example where we are studying the effect of physical activity. A static intervention approach might ask, “what would happen if everyone exercised 30 minutes a week?” Yet there is no actual intervention we could translate to the real world to make everyone exercise 30 minutes a week, i.e. there is no policy a decision-maker could enact to make this happen, even approximately. We could estimate an outcome under this policy, but this unrealistic intervention will likely have a meaningless result.\n\nNotably, you can have an unrealistic intervention without having a positivity violation. For example, we may have access to a large data set with people of all possible covariate combinations who exercise 30 minutes a week; however, it is still scientifically not meaningful or realistic to evaluate a world in which everyone exercises the exact same amount of time.\n\n\n\n\n\n  \n\nA graphical example of a static intervention on the pollution measurement of Air Quality Index (AQI). On the left we see the naturally observed exposure values for one day in the US, and on the right we see what the hypothetical exposure values would look like under a static intervention in which all study subjects had an AQI exposure of 20."
  },
  {
    "objectID": "blog/lmtp/lmtp.html#one-possible-solution-to-the-problems-posed-by-static-interventions-is-dynamic-interventions.",
    "href": "blog/lmtp/lmtp.html#one-possible-solution-to-the-problems-posed-by-static-interventions-is-dynamic-interventions.",
    "title": "An Illustrated Guide to Modified Treatment Policies, Part 1: Introduction and Motivation",
    "section": "One possible solution to the problems posed by Static Interventions is Dynamic Interventions.",
    "text": "One possible solution to the problems posed by Static Interventions is Dynamic Interventions.\nDynamic treatment policies or interventions ask what would have happened in a hypothetical world where exposure is assigned based on each observation’s covariate history.\nFor example, let’s say we want to study the effect of pollution on asthma attacks in the United States. A simple study design could be to find a representative sample of individuals living with asthma, measure their pollution exposure via the Air Quality Index (AQI) on one calendar day (along with other confounders), and record whether an asthma attack occurs within a specified time frame.\nAlthough I don’t study pollution, I know it is scientifically impossible for everyone in the US to receive the same air quality exposure, even in a world with countless environmental regulations. Natural and geographical forces such as wind, wildfires, elevation, proximity to the ocean, etc. make it unrealistic to study a single air quality exposure value for everyone.\nWe can try getting around these issues by evaluating a dynamic intervention that depends on baseline characteristics of the person or environment. For example, “if a person lives in an urban area, make their AQI exposure 40, and if a person lives in a residential area, make their AQI exposure 20.”\n\n\n\n\n  \n\nAn example of a naturally observed exposure of AQI in the US (left) and what it would look like under a hypothetical dynamic intervention in which all urban counties have an AQI of 40 and rural counties have an AQI of 20 (right).\n\n\n\n This intervention may reduce positivity violations and lead to a more realistic study, yet there are many other interesting questions about air pollution we still cannot answer with dynamic treatment policies.\n\nProblem #3: Static and dynamic interventions cannot depend on the observed exposure\n\nAnother key limitation with both static and dynamic interventions is that the hypothetical exposure value is completely determined by each observation’s baseline characteristics rather than the observed exposure. However, there are many interesting scientific questions about interventions that depend on the actual exposure received. For example, a more relevant question for our pollution example could be, “what would have happened in a hypothetical world where everyone’s AQI exposure was 10% lower than what we observed?” Or, “what if the reduction had been 20 units for individuals living in an area with a daily AQI &gt; 100?”\n\nWhat questions, or hypothetical interventions, would we propose if we did not feel limited by statistical methodology?"
  },
  {
    "objectID": "blog/lmtp/lmtp.html#longitudinal-data-analysis-complexities",
    "href": "blog/lmtp/lmtp.html#longitudinal-data-analysis-complexities",
    "title": "An Illustrated Guide to Modified Treatment Policies, Part 1: Introduction and Motivation",
    "section": "Longitudinal data analysis complexities",
    "text": "Longitudinal data analysis complexities\nLongitudinal studies pose certain crucial and often-overlooked issues, most notably time-dependent confounders. Time-dependent confounders are variables which change over time, affect the likelihood of both future exposure and the outcome, and are affected by previous exposure.\nA key time-dependent confounder for the relationship between pollution and asthma attacks could be weather. Hot, humid air causes air quality to decrease, and it also aggravates pulmonary conditions like asthma. I will explain time-dependent confounders more in later posts of this series, but for now, note that LMTP (the method and R package) properly accommodates time-dependent confounders."
  },
  {
    "objectID": "blog/lmtp/lmtp.html#lmtp-methodology-and-software-capabilities",
    "href": "blog/lmtp/lmtp.html#lmtp-methodology-and-software-capabilities",
    "title": "An Illustrated Guide to Modified Treatment Policies, Part 1: Introduction and Motivation",
    "section": "LMTP methodology and software capabilities",
    "text": "LMTP methodology and software capabilities\nThis LMTP methodology and lmtp software allows the user to handle:\n\n\n\n\n  \n\n\n\n\nCross-sectional or longitudinal exposures\n\n\nBinary, categorical, or continuous exposures\n\n\nExposures that are dependent on time, i.e. delaying the occurrence of the exposure as an intervention\n\n\nMultiple exposures\n\n\nBinary, continuous, or time-to-event outcomes\n\n\nInformative loss to follow-up/censoring\n\n\nCompeting risks\n\n\nSurvey weights\n\n\nThis could translate to longitudinal research questions such as:\n\n\nHow would asthma rates change if we reduced two pollutants (fine particulate matter and ozone) by 10% every day for a year?\n\n\nHow would drug overdose numbers change if Naloxone access laws were implemented one year later?\n\n\nHow would length of intubation for intubated COVID-19 patients change if patients who never received steroids were given a standard dose of steroids upon intubation? (Death of other causes is a competing risk and discharge is an informative loss to follow up.)"
  },
  {
    "objectID": "blog/tmle/tutorial.html",
    "href": "blog/tmle/tutorial.html",
    "title": "An Illustrated Guide to TMLE, Part I: Introduction and Motivation",
    "section": "",
    "text": "The introductory post of a three-part series to help beginners understand Targeted Maximum Likelihood Estimation (TMLE). This section contains a brief overview of the targeted learning framework and motivation for semiparametric estimation methods for inference, including causal inference.\n\n\n\nTable of Contents\nThis blog post series has three parts:\n\nPart I: Motivation\n\nTMLE in three sentences 🎯\nAn Analyst’s Motivation for Learning TMLE 👩🏼‍💻\nIs TMLE Causal Inference? 🤔\n\n\n\nPart II: Algorithm\n\nWhy the Visual Guide? 🎨\nTMLE, Step-by-Step 🚶🏽\nUsing the tmle package 📦\n\n\n\nPart III: Evaluation\n\nProperties of TMLE 📈\nWhy does TMLE work? ✨\nResources to learn more 🤓\n\n\n\n\n\nTMLE in three sentences 🎯\nTargeted Maximum Likelihood Estimation (TMLE) is a semiparametric estimation framework to estimate a statistical quantity of interest. TMLE allows the use of machine learning (ML) models which place minimal assumptions on the distribution of the data. Unlike estimates normally obtained from ML, the final TMLE estimate will still have valid standard errors for statistical inference.\n\n\nAn Analyst’s Motivation for Learning TMLE 👩🏼‍💻\nWhen I graduated with my MS in Biostatistics two years ago, I had a mental framework of statistics and data science that I think is pretty common among new graduates. It went like this:\n\nIf the goal is inference (e.g., an effect size with a confidence interval), use an interpretable, usually parametric, model and explain what the coefficients and their standard errors mean.\nIf the goal is prediction, use data-adaptive machine learning algorithms and then look at performance metrics, with the understanding that standard errors, and sometimes even coefficients, no longer exist.\n\nThis mentality changed drastically when I started learning about semiparametric estimation methods like TMLE in the context of causal inference. I quickly realized two flaws in this mental framework.\nFirst, I was thinking about inference backwards: I was choosing a model based on my outcome type (binary, continuous, time-to-event, repeated measures) and then interpreting specific coefficients as my estimates of interest. Yet it makes way more sense to first determine the statistical quantity, or estimand, that best answers a scientific question, and then use the method, or estimator, best suited for estimating it. This is the paradigm TMLE is based upon: we want to build an algorithm, or estimator, targeted to an estimand of interest.\n\n\n\n\nSecond, I thought flexible, data-adaptive models we commonly classify as statistical and/or machine learning (e.g. LASSO, random forests, gradient boosting, etc.) could only be used for prediction, since they don’t have asymptotic properties for inference (i.e. standard errors). However, certain semiparametric estimation methods like TMLE can actually use these models to obtain a final estimate that is closer to the target quantity than would be obtained using classic parametric models (e.g. linear and logistic regression). This is because machine learning models are generally designed to accommodate large numbers of covariates with complex, non-linear relationships.\n\n\nSemiparametric estimation methods like TMLE can rely on machine learning to avoid making unrealistic parametric assumptions about the underlying distribution of the data (e.g. multivariate normality).\n\nThe way we use the machine learning estimates in TMLE, surprisingly enough, yields known asymptotic properties of bias and variance – just like we see in parametric maximum likelihood estimation – for our target estimand.\nBesides allowing us to compute 95% confidence intervals and p-values for our estimates even after using flexible models, TMLE achieves other beneficial statistical properties, such as double robustness. These are discussed further in Part III.\n\n\nIs TMLE Causal Inference? 🤔\nIf you’ve heard about TMLE before, it was likely in the context of causal inference. Although TMLE was developed for causal inference due to its many attractive properties, it cannot be considered causal inference by itself. Causal inference is a two-step process that first requires causal assumptions1 before a statistical estimand can be interpreted causally.\n1 I won’t discuss causal assumptions in these posts, but this is referring to fundamental assumptions in causal inference like consistency, exchangeability, and positivity. A primary motivation for using TMLE and other semiparametric estimation methods for causal inference is that if you’ve already taken the time to carefully evaluate causal assumptions, it does not make sense to then damage an otherwise well-designed analysis by making unrealistic statistical assumptions.TMLE can be used to estimate various statistical estimands (odds ratio, risk ratio, mean outcome difference, etc.) even when causal assumptions are not met. TMLE is, as its name implies, simply a tool for estimation.\n\nIn Part II, I’ll walk step-by-step through a basic version of the TMLE algorithm: estimating the mean difference in outcomes, adjusted for confounders, for a binary outcome and binary treatment. If causal assumptions are met, this is called the Average Treatment Effect (ATE), or the mean difference in outcomes in a world in which everyone had received the treatment compared to a world in which everyone had not.\n⤴️Back to the top\n➡️Continue to Part II: The Algorithm\n\n\nReferences\nMy primary reference for all three posts is Targeted Learning by Mark van der Laan and Sherri Rose. I detail many other resources I’ve used to learn TMLE, semiparametric theory, and causal inference in Part III."
  },
  {
    "objectID": "blog/tmle/tutorial-pt2.html",
    "href": "blog/tmle/tutorial-pt2.html",
    "title": "An Illustrated Guide to TMLE, Part II: The Algorithm",
    "section": "",
    "text": "The second post of a three-part series to help beginners and/or visual learners understand Targeted Maximum Likelihood Estimation (TMLE). This section walks through the TMLE algorithm for the mean difference in outcomes for a binary treatment and binary outcome.   This post is an expansion of a printable “visual guide” available on my Github. I hope it helps analysts who feel out-of-practice reading mathematical notation follow along with the TMLE algorithm. A web-based key without explanations is also available here.\n⬅️Return to Part I: Motivation"
  },
  {
    "objectID": "blog/tmle/tutorial-pt2.html#step-1-estimate-the-outcome",
    "href": "blog/tmle/tutorial-pt2.html#step-1-estimate-the-outcome",
    "title": "An Illustrated Guide to TMLE, Part II: The Algorithm",
    "section": "Step 1: Estimate the Outcome",
    "text": "Step 1: Estimate the Outcome\nThe very first step of TMLE is to estimate the expected value of the outcome using treatment and confounders as predictors.\nThis is what that looks like in mathematical notation. There is some function \\(Q\\) which takes \\(A\\) and \\(\\mathbf{W}\\) as inputs and yields the conditional expectation of \\(Y\\):\n\\[Q(A,\\mathbf{W}) = \\mathrm{E}[Y|A,\\mathbf{W}]\\] We can use any regression to estimate this conditional expectation, but it is best to use flexible machine learning models so that we don’t have unnecessary assumptions on the underlying distribution of the data.\nWe can think about the above equation as some generic regression function in R called fit() with inputs in formula form: outcome ~ predictors:\n In real R code, we’ll use the SuperLearner() function to fit a weighted combination of multiple machine learning models (defined earlier in sl_libs). This function takes the outcome Y as a vector and a data frame X as predictors.\n\nY &lt;- dat_obs$Y\nW_A &lt;- dat_obs %&gt;% select(-Y) # remove the outcome to make a matrix of predictors (A, W1, W2, W3, W4) for SuperLearner\nQ &lt;- SuperLearner(Y = Y, # Y is the outcome vector\n                  X = W_A, # W_A is the matrix of W1, W2, W3, W4, and A\n                  family=binomial(), # specify we have a binary outcome\n                  SL.library = sl_libs) # specify our superlearner library of LASSO, RF, and MARS\n\nThen, we should estimate the outcome for every observation under three different scenarios:\n1. If every observation received the treatment they actually received.\nWe can get this expected outcome estimate by simply calling predict() on the model fit without specifying any new data.\n\\[\\hat{Q}(A,\\mathbf{W}) = \\mathrm{\\hat{E}}[Y|A,\\mathbf{W}]\\]\nWe will save that vector of estimates as a new object in R.\n\n\nQ_A &lt;- as.vector(predict(Q)$pred) # obtain predictions for everyone using the treatment they actually received\n\n2. If every observation received the treatment.\nTo do this, we’ll first need to create a data set where every observation received the treatment of interest, whether they actually did or not. Then we can call the predict() function on that data set.\n\\[\\hat{Q}(1,\\mathbf{W}) = \\mathrm{\\hat{E}}[Y|A=1,\\mathbf{W}]\\]\n\n\nW_A1 &lt;- W_A %&gt;% mutate(A = 1)  # data set where everyone received treatment\nQ_1 &lt;- as.vector(predict(Q, newdata = W_A1)$pred) # predict on that everyone-exposed data set\n\n3. If every observation received the control.\nSimilarly, we create a data set where every observation did not receive the treatment of interest, whether they actually did or not, and call the predict() function again.\n\\[\\hat{Q}(0,\\mathbf{W}) = \\mathrm{\\hat{E}}[Y|A=0,\\mathbf{W}]\\]\n\n\nW_A0 &lt;- W_A %&gt;% mutate(A = 0) # data set where no one received treatment\nQ_0 &lt;- as.vector(predict(Q, newdata = W_A0)$pred)\n\nNext, let’s create a new data frame, dat_tmle, to hold the three vectors we’ve created so far, along with the treatment status \\(A\\) and observed outcome \\(Y\\). Notice that when \\(A=1\\), the expected outcome \\(\\mathrm{\\hat{E}}[Y|A,\\mathbf{W}]\\) equals the expected outcome under treatment \\(\\mathrm{\\hat{E}}[Y|A=1,\\mathbf{W}]\\). When \\(A=0\\), the expected outcome \\(\\mathrm{\\hat{E}}[Y|A,\\mathbf{W}]\\) equals the expected outcome under no treatment \\(\\mathrm{\\hat{E}}[Y|A=0,\\mathbf{W}]\\).\n\ndat_tmle &lt;- tibble(Y = dat_obs$Y, A = dat_obs$A, Q_A, Q_0, Q_1)\ndat_tmle %&gt;%\n  head() %&gt;%\n  gt() %&gt;%\n  tab_header(\"TMLE Algorithm after Step 1\")\n\n\n\n\n  \n    \n      TMLE Algorithm after Step 1\n    \n    \n  \n  \n    \n      Y\n      A\n      Q_A\n      Q_0\n      Q_1\n    \n  \n  \n    1\n1\n0.8461853\n0.6770917\n0.8461853\n    1\n0\n0.6986440\n0.6986440\n0.8589257\n    0\n0\n0.4932538\n0.4932538\n0.7188934\n    1\n1\n0.8213403\n0.6363132\n0.8213403\n    1\n0\n0.6266258\n0.6266258\n0.8151742\n    1\n1\n0.8578239\n0.6966588\n0.8578239\n  \n  \n  \n\n\n\n\n\nWe could stop here and get our estimate of the ATE by computing the average difference between \\(\\mathrm{\\hat{E}}[Y|A=1,\\mathbf{W}]\\) and \\(\\mathrm{\\hat{E}}[Y|A=0,\\mathbf{W}]\\), which would be the mean difference in the expected outcomes, conditional on confounders. This estimation method is often called standardization, simple substitution estimation, g-formula estimation, or G-computation.\n\\[\\hat{ATE}_{G-comp}= \\hat{\\Psi}_{G-comp} = \\frac{1}{N}\\sum_{i=1}^{N}(\\mathrm{\\hat{E}}[Y|A=1,\\mathbf{W}]-\\mathrm{\\hat{E}}[Y|A=0,\\mathbf{W}])\\] \n\nate_gcomp &lt;- mean(dat_tmle$Q_1 - dat_tmle$Q_0)\nate_gcomp\n\n[1] 0.195287\n\n\nHowever, this G-computation ATE estimate does not have the appropriate bias-variance tradeoff for the ATE because it was built to have the best bias-variance tradeoff for estimating the outcome, conditional on confounders, rather than the ATE. We also cannot compute the standard error of the estimator because we don’t know the sampling distribution of the machine learning estimates.\n⤴️Back to the top"
  },
  {
    "objectID": "blog/tmle/tutorial-pt2.html#step-2-estimate-the-probability-of-treatment",
    "href": "blog/tmle/tutorial-pt2.html#step-2-estimate-the-probability-of-treatment",
    "title": "An Illustrated Guide to TMLE, Part II: The Algorithm",
    "section": "Step 2: Estimate the Probability of Treatment",
    "text": "Step 2: Estimate the Probability of Treatment\nThe next step is to estimate the probability of treatment, given confounders. This quantity is often called the propensity score, as in it gives the probability or propensity that an observation will receive a treatment of interest.\n\\[g(\\mathbf{W}) = \\mathrm{Pr}(A=1|\\mathbf{W})\\] \nWe will estimate \\(\\mathrm{Pr}(A=1|\\mathbf{W})\\) in the same way as we estimated \\(\\mathrm{E}[Y|A,\\mathbf{W}]\\): using the superlearner algorithm.\n\nA &lt;- dat_obs$A\nW &lt;- dat_obs %&gt;% select(-Y, -A) # matrix of predictors that only contains the confounders W1, W2, W3, and W4\ng &lt;- SuperLearner(Y = A, # outcome is the A (treatment) vector\n                  X = W, # W is a matrix of predictors\n                  family=binomial(), # treatment is a binomial outcome\n                  SL.library=sl_libs) # using same candidate learners; could use different learners\n\nThen we need to compute three different quantities from this model fit:\n1. The inverse probability of receiving treatment.\n\\[H(1,\\mathbf{W}) = \\frac{1}{g(\\mathbf{W})} = \\frac{1}{\\mathrm{Pr}(A=1|\\mathbf{W})}\\] We can estimate the probability of receiving treatment for every observation by using the predict() funcion without specifying any new data, and then take the inverse of that.\n\n\ng_w &lt;- as.vector(predict(g)$pred) # Pr(A=1|W)\nH_1 &lt;- 1/g_w\n\n2. The negative inverse probability of not receiving treatment.\n\\[H(0,\\mathbf{W}) = -\\frac{1}{1-g(\\mathbf{W})}= -\\frac{1}{\\mathrm{Pr}(A=0|\\mathbf{W})}\\] The probability of not receiving treatment for a binary treatment is simply 1 minus the probability of treatment.\n\n\nH_0 &lt;- -1/(1-g_w) # Pr(A=0|W) is 1-Pr(A=1|W)\n\n3. If the observation was treated, the inverse probability of receiving treatment, and if they were not treated, the negative inverse probability of not receiving treatment. I’ll discuss why later, but in the TMLE literature this is called the clever covariate.\n\\[H(A,\\mathbf{W}) = \\frac{\\mathrm{I}(A=1)}{\\mathrm{Pr}(A=1|\\mathbf{W})}-\\frac{\\mathrm{I}(A=0)}{\\mathrm{Pr}(A=0|\\mathbf{W})}\\]\n\nTo calculate the clever covariate, we’ll first add the \\(H(1,\\mathbf{W})\\) and \\(H(0,\\mathbf{W})\\) vectors to our dat_tmle data frame, and then we can use \\(A\\) to assign \\(H(A,\\mathbf{W})\\).\n\ndat_tmle &lt;- # add clever covariate data to dat_tmle\n  dat_tmle %&gt;%\n  bind_cols(\n         H_1 = H_1,\n         H_0 = H_0) %&gt;%\n  mutate(H_A = case_when(A == 1 ~ H_1, # if A is 1 (treated), assign H_1\n                       A == 0 ~ H_0))  # if A is 0 (not treated), assign H_0\n\nWe now have our initial estimates of the outcome, and the estimates of the probability of treatment:\n\ndat_tmle %&gt;% head() %&gt;% gt() %&gt;% tab_header(\"TMLE Algorithm after Step 2\") %&gt;% fmt_number(everything(), decimals = 2)\n\n\n\n\n  \n    \n      TMLE Algorithm after Step 2\n    \n    \n  \n  \n    \n      Y\n      A\n      Q_A\n      Q_0\n      Q_1\n      H_1\n      H_0\n      H_A\n    \n  \n  \n    1.00\n1.00\n0.85\n0.68\n0.85\n2.18\n−1.85\n2.18\n    1.00\n0.00\n0.70\n0.70\n0.86\n1.60\n−2.67\n−2.67\n    0.00\n0.00\n0.49\n0.49\n0.72\n3.39\n−1.42\n−1.42\n    1.00\n1.00\n0.82\n0.64\n0.82\n2.39\n−1.72\n2.39\n    1.00\n0.00\n0.63\n0.63\n0.82\n2.31\n−1.76\n−1.76\n    1.00\n1.00\n0.86\n0.70\n0.86\n2.14\n−1.88\n2.14\n  \n  \n  \n\n\n\n\n⤴️Back to the top"
  },
  {
    "objectID": "blog/tmle/tutorial-pt2.html#step-3-estimate-the-fluctuation-parameter",
    "href": "blog/tmle/tutorial-pt2.html#step-3-estimate-the-fluctuation-parameter",
    "title": "An Illustrated Guide to TMLE, Part II: The Algorithm",
    "section": "Step 3: Estimate the Fluctuation Parameter",
    "text": "Step 3: Estimate the Fluctuation Parameter\nTo reiterate, in Step 1 we estimated the expected outcome, conditional on treatment and confounders. We know those machine learning fits have an optimal bias-variance trade-off for estimating the outcome (conditional on treatment and confounders), rather than the ATE. We will now use information about the treatment mechanism (from Step 2) to optimize the bias-variance trade-off for the ATE so we can obtain valid inference.\n\n\n\nHTML Image as link\n\n\n\n  \n\n\n🚨 Warning: this step is easy to code, but difficult to understand unless you have a background in semiparametric theory. Let’s first look at what we’re doing, and then I’ll discuss at a very high-level why we’re doing that in a later section.\nThe point of this step is to solve an estimating equation for the efficient influence function (EIF) of our estimand of interest. Without diving into what an EIF or an estimating equation is, let’s accept for a moment that they will help us:\n\nUpdate our initial outcome estimates so that our estimate of the ATE is asymptotically unbiased (under certain conditions, see Part III, Statistical Properties).\nCalculate the variance, and thus the standard error, confidence interval, and p-value for our estimate of the ATE for hypothesis testing.\n\nNext, let’s take a look at a model that will help us solve an EIF estimating equation and then update our estimates:\n\\[logit(\\mathrm{E}[Y|A,\\mathbf{W}]) = logit(\\mathrm{\\hat{E}}[Y|A,\\mathbf{W}]) + \\epsilon H(A,\\mathbf{W})\\]\nTo reiterate, I haven’t explained at all why this step works; we’re just focusing on implementing it for now.\nIf we look at the left side, we can see this equation contains the true outcome \\(Y\\), just \\(logit\\) transformed. Luckily for us, there’s a well-known model that \\(logit\\) transforms the left side of an equation: logistic regression. Our estimating equation looks a lot like a simple logistic regression, actually: \\(logit(\\mathrm{E}[Y|X]) = \\beta_0 + \\beta_1 X\\).\nDo you see how our equation also has a vector on the right-hand side, \\(H(A,\\mathbf{W})\\), with a corresponding coefficient, \\(\\epsilon\\)? The only difference is that the “intercept” in our equation, \\(logit(\\mathrm{\\hat{E}}[Y|A,\\mathbf{W}])\\) is not a constant value like \\(\\beta_0\\); it is a vector of values. We can see it as an offset or a fixed intercept in a logistic regression, rather than a constant-value intercept.\nTherefore, to accomplish our goal of solving an estimating equation for the EIF we can leverage standard statistical software and fit a logistic regression with one covariate, \\(H(A,\\mathbf{W})\\), and the initial outcome estimate, \\(logit(\\mathrm{\\hat{E}}[Y|A,\\mathbf{W}])\\), as a fixed intercept. The outcome of the logistic regression is the observed outcome, \\(Y\\).\nTwo technical points for application: we use qlogis to transform the probabilities \\(\\mathrm{\\hat{E}}[Y|A,\\mathbf{W}]\\) to the \\(logit\\) scale. Also, the R code for a fixed intercept is -1 + offset(fixed_intercept).\n\n\nglm_fit &lt;- glm(Y ~ -1 + offset(qlogis(Q_A)) + H_A, data=dat_tmle, family=binomial)\n\nNote that we are only using a logistic regression because it happens to have the correct form for solving the estimating equation for the EIF for the ATE estimand. It has nothing to do with having a binary outcome, and it isn’t putting any parametric restraints on our data.\nNext we need to save the coefficient from that logistic regression, which we will call \\(\\hat{\\epsilon}\\):\n\n\neps &lt;- coef(glm_fit)\n\nIn the TMLE literature, \\(\\epsilon\\) is called the fluctuation parameter, because it provides information about how much to change, or fluctuate, our initial outcome estimates. Similarly, \\(H(A,\\mathbf{W})\\) is called the clever covariate because it “cleverly” helps us solve for the EIF and then update our estimates.\nWe will use both the fluctuation parameter and clever covariate in the next step to update our initial estimates of the expected outcome, conditional on confounders and treatment.\n⤴️Back to the top"
  },
  {
    "objectID": "blog/tmle/tutorial-pt2.html#step-4-update-the-initial-estimates-of-the-expected-outcome",
    "href": "blog/tmle/tutorial-pt2.html#step-4-update-the-initial-estimates-of-the-expected-outcome",
    "title": "An Illustrated Guide to TMLE, Part II: The Algorithm",
    "section": "Step 4: Update the Initial Estimates of the Expected Outcome",
    "text": "Step 4: Update the Initial Estimates of the Expected Outcome\nAlmost done! Let’s recap:\n\nIn Step 1, we obtained initial estimates of the expected outcome using machine learning (ML). These ML estimates are optimized to estimate \\(E[Y|A,W]\\), not the ATE.\nWe need to update those initial expected outcome estimates using information about the treatment mechanism, so we computed the expected probability of treatment, conditional on confounders, in Step 2.\nThen, in Step 3, we used quantities from Step 1 and Step 2 to solve an estimating equation for the EIF. We didn’t talk about why this works; we simply accepted for now that this is how we can target our estimand of interest (the ATE).\nNow, we will update our initial outcome estimates from Step 1 using information from Step 2 and 3 to obtain the correct bias-variance tradeoff for the ATE.\n\nTo update our expected outcome estimates, we first need to put the initial expected outcome estimates on the \\(logit\\) scale using qlogis() because that’s the scale we used to solve our estimating equation for the EIF in Step 3. Then we can calculate how much we need to fluctuate our initial estimates using the product of the clever covariate and fluctuation parameter: \\(H(A,\\mathbf{W}) \\times \\hat{\\epsilon}\\). These are our outputs of Step 2 and 3, respectively. We will add that quantity to the initial outcome estimates to create updated outcome estimates. Finally, we can put the updated estimates back on the true outcome scale using plogis().\nNote we can use \\(expit\\) to show the inverse of the \\(logit\\) function, and we will denote updates to the outcome regressions as \\(\\hat{\\mathrm{E}}^*\\) instead of \\(\\hat{\\mathrm{E}}\\).\n1. Update the expected outcomes of all observations, given the treatment they actually received and their baseline confounders.\n\\[\\hat{\\mathrm{E}}^*[Y|A,\\mathbf{W}] = expit(logit(\\mathrm{\\hat{E}}[Y|A,\\mathbf{W}]) + \\hat{\\epsilon}H(A,\\mathbf{W}))\\]\n\n\nH_A &lt;- dat_tmle$H_A # for cleaner code in Q_A_update\nQ_A_update &lt;- plogis(qlogis(Q_A) + eps*H_A)\n\n2. Update the expected outcomes, conditional on baseline confounders and everyone receiving the treatment.\n\\[\\hat{\\mathrm{E}}^*[Y|A=1,\\mathbf{W}] = expit(logit(\\mathrm{\\hat{E}}[Y|A=1,\\mathbf{W}]) + \\hat{\\epsilon}H(1,A))\\] \n\nQ_1_update &lt;- plogis(qlogis(Q_1) + eps*H_1)\n\n3. Update the expected outcomes, conditional on baseline confounders and no one receiving the treatment.\n\\[\\hat{\\mathrm{E}}^*[Y|A=0,\\mathbf{W}] = expit(logit(\\mathrm{\\hat{E}}[Y|A=0,\\mathbf{W}]) + \\hat{\\epsilon}H(0,W))\\] \n\nQ_0_update &lt;- plogis(qlogis(Q_0) + eps*H_0)\n\n⤴️Back to the top"
  },
  {
    "objectID": "blog/tmle/tutorial-pt2.html#step-5-compute-the-statistical-estimand-of-interest",
    "href": "blog/tmle/tutorial-pt2.html#step-5-compute-the-statistical-estimand-of-interest",
    "title": "An Illustrated Guide to TMLE, Part II: The Algorithm",
    "section": "Step 5: Compute the Statistical Estimand of Interest",
    "text": "Step 5: Compute the Statistical Estimand of Interest\nWe now have updated expected outcomes estimates, so we can compute the ATE as the mean difference in the updated outcome estimates under treatment and no treatment:\n\\[\\hat{ATE}_{TMLE} = \\hat{\\Psi}_{TMLE} = \\frac{1}{N}\\sum_{i=1}^{N}[\\hat{E^*}[Y|A=1,\\mathbf{W}] - \\hat{E^*}[Y|A=0,\\mathbf{W}]]\\]\n\n\ntmle_ate &lt;- mean(Q_1_update - Q_0_update)\ntmle_ate\n\n[1] 0.1911676\n\n\nWe can then say, “the average treatment effect was estimated to be 19.1%.”\nIf causal assumptions were not met, we would say, “the proportion of observations who experienced the outcome, after adjusting for baseline confounders, was estimated to be 19.1% higher for those who received treatment compared to those who did not.”\n⤴️Back to the top"
  },
  {
    "objectID": "blog/tmle/tutorial-pt2.html#step-6-calculate-the-standard-errors-for-confidence-intervals-and-p-values",
    "href": "blog/tmle/tutorial-pt2.html#step-6-calculate-the-standard-errors-for-confidence-intervals-and-p-values",
    "title": "An Illustrated Guide to TMLE, Part II: The Algorithm",
    "section": "Step 6: Calculate the Standard Errors for Confidence Intervals and P-values",
    "text": "Step 6: Calculate the Standard Errors for Confidence Intervals and P-values\nThis point estimate is great, but we usually need an estimate of variance so that we can compute confidence intervals, test statistics, p-values, etc. This is another step that contains quite a lot of theory, so I’ll give another birds-eye view for now. If you’re curious, you can read more about this in Part III.\nTo obtain the standard errors, we first need to compute the Influence Function (IF), which is the empirical version of what we used our estimating equation to figure out in Step 3. The IF tells us how much each observation influences the final estimate.\nThe equation for the IF looks like this:\n\\[\\hat{IF} = (Y-\\hat{E^*}[Y|A,\\mathbf{W}])H(A,\\mathbf{W}) + \\hat{E^*}[Y|A=1,\\mathbf{W}] - \\hat{E^*}[Y|A=0,\\mathbf{W}] - \\hat{ATE}\\]\n\ninfl_fn &lt;- (Y - Q_A_update) * H_A + Q_1_update - Q_0_update - tmle_ate\n\nOnce we have the IF, we can take the square-root of its variance divided by the number of observations to get the standard error of our estimate.\n\\[\\hat{SE} = \\sqrt{\\frac{var(\\hat{IF})}{N}} \\]\n\n\ntmle_se &lt;- sqrt(var(infl_fn)/nrow(dat_obs))\n\nOnce we have that standard error, we can easily get the 95% confidence interval and p-value of our estimate.\n\nconf_low &lt;- tmle_ate - 1.96*tmle_se\nconf_high &lt;- tmle_ate + 1.96*tmle_se\npval &lt;- 2 * (1 - pnorm(abs(tmle_ate / tmle_se)))\n\ntibble(tmle_ate, conf_low, conf_high) %&gt;% head() %&gt;% gt() %&gt;% tab_header(\"TMLE Estimate of the ATE\") %&gt;% fmt_number(everything(), decimals = 3)\n\n\n\n\n  \n    \n      TMLE Estimate of the ATE\n    \n    \n  \n  \n    \n      tmle_ate\n      conf_low\n      conf_high\n    \n  \n  \n    0.191\n0.133\n0.249\n  \n  \n  \n\n\n\n\nThen we can successfully report our ATE as 0.191 (95% CI: 0.133, 0.249).\nNote that a TMLE estimator is asymptotically normally distributed, so we could bootstrap the entire algorithm to get our standard errors instead.\n⤴️Back to the top"
  },
  {
    "objectID": "blog/nyrconf/index.html",
    "href": "blog/nyrconf/index.html",
    "title": "Tips and Tricks from the 2019 New York R Conference",
    "section": "",
    "text": "My favorite takeaways from attending the 2019 New York R Conference.\n\nJune 6, 2019.\nIn early May I attended the New York R Conference. There were 24 speakers, including my coworker at Weill Cornell Medicine, Elizabeth Sweeney! Each person did a 20-minute presentation on some way they use R for their work and/or hobbies. There was a ton of information, and even though not all of it was directly useful for my workflow as a statistical consultant in an academic setting, I really enjoyed being around so many people who love R.\nI’ve linked some videos of my favorite talks and put together some the topics/packages/functions I found most intriguing or useful in my day-to-day work as a research biostatistician. (This was originally a presentation for my biostatistics team’s computing club.)\n\nVisualizing data with naniar\nBrooke Watson, a data scientist at the American Civil Liberties Union, gave a great presentation on how she uses R to defend immigrants. She shared several data wrangling tips. One new function for me was naniar::vis_miss() to visualize your missing data quickly.\n\n#install.packages(\"tidyverse\")\n#install.packages(\"naniar\")\nlibrary(tidyverse)\nlibrary(naniar)\nvis_miss(airquality) # a base R data set\n\n\n\n\n\n\n\n\nIt returns a ggplot2 object so you can edit titles, colors, etc. if necessary. You can also add various sorting and clustering arguments to make it easier to see patterns of missingness in your data.\n\n\nChecking out data differences with daff\nBrooke also gave a demo for a neat package to check if and where differences in two data sets are occurring.\n\n#install.packages(\"daff)\nlibrary(daff)\ndat1 &lt;- data.frame(A = c(1:3), B = c(T,F,T))\ndat2 &lt;- data.frame(A = c(1:4), C = c(\"apple\",NA,NA,\"banana\"))\nmy_diff &lt;- diff_data(dat1, dat2)\nmy_diff\n\nDaff Comparison: 'dat1' vs. 'dat2' \n      +++    ---  \n@@  A C      B    \n+   1 apple  TRUE \n    2 &lt;NA&gt;   FALSE\n    3 &lt;NA&gt;   TRUE \n+++ 4 banana &lt;NA&gt; \n\n\nI thought this would be useful for when you receive new data sets and want to make sure column names, patients, etc. haven’t changed. Check out the full documentation here.\n\n\nGohelverse\nNoam Ross shared code for editable figures using David Gohel’s officer and rvg packages. I shared some example code for my team on github after I saw him present it at an R-Ladies event in the fall. Essentially you can run some pretty simple lines of code to output figures (base R, ggplot2, or otherwise) as editable figures in Powerpoint. Noam reminded us that whoever you give these figures to will now be able to edit anything, even data points, so keep that in mind before you freely give away editable figures… :)\n\n\nGoing from RMarkdown to Word, and back again with redoc\nNoam also shared his new package, redoc, which allows you to reload an Rmd-generated word file back into R as a modified Rmd file.\n\nThis is part of his goal to decrease the pain of “the valley of heartbreak.” \nInstallation command is:\n\n#remotes::install_github(\"noamross/redoc\")\n\nYou may need to update several packages to get it to run correctly, but after that the main commands are just redoc and dedoc. To see for yourself, try running my github code, making some changes to your word doc, and reloading back into Rmarkdown with the dedoc() function.\n\n\nPipelines in drake\n\nThis could definitely be an entire computing club presentation… but for long projects that you have to redo with new data often, drake is becoming really popular. Amanda Dobbyn gave an awesome presentation and you can see her slides here.\nA super informative bookdown guide by the authors can be found here. Essentially their motto is “what gets done stays done” so that you are not redoing work you’ve already done in order to update your results. Yet, you’re still redoing what needs to be done in a reproducible way!\n\n\n\nGit merge conflicts\nI went to a whole-day workshop on Git so if you’re interested in talking more about this let me know. BUT the biggest thing I learned was that if you are ever using Git and find your code has strange characters like &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; HEAD followed by ======== and a long set of letters/numbers, this means you have a merge conflict. It’s meant to be a flag so you know where to fix the differences in your two files you’re trying to version control! I spent days struggling with this problem before, so I thought I’d pass the message along in case anyone runs into it someday. :)\n\n\nTalks to check out\nSome of my favorite talks from the conference were…\n-Emily Robinson’s accessible instructions for how to make a package with usethis Jaqueline Nolis’ really funny talk on how neural nets aren’t actually hard at all -Andrew Gelman’s discussion on “solving all your statistical problems with p-values” (but he’s a Bayesian, ha ha)\n\n\n\n\n\n\n\n\n\nTakeaways\n\nYou can check out tweets from the conference by searching the hashtag #rstatsnyc on Twitter\nCheck out the R-Ladies NYC meetups and New York R meetups!"
  },
  {
    "objectID": "blog/iterated-expectations/index.html",
    "href": "blog/iterated-expectations/index.html",
    "title": "Rethinking Conditional and Iterated Expectations as Linear Regression Models",
    "section": "",
    "text": "TLDR\n\nYou can a regress an outcome on a grouping variable plus any other variable(s) and the unadjusted and adjusted group means will be identical.\nWe can see this in a simple example using the palmerpenguins data:\n\n#remotes::install_github(\"allisonhorst/palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(gt)\n\n# use complete cases for simplicity\npenguins &lt;- drop_na(penguins)\n\npenguins %&gt;%\n  # fit a linear regression for bill length given bill depth and species\n  # make a new column containing the fitted values for bill length\n  mutate(preds = predict(lm(bill_length_mm ~ bill_depth_mm + species, data = .))) %&gt;%\n  # compute unadjusted and adjusted group means\n  group_by(species) %&gt;%\n  summarise(mean_bill_length = mean(bill_length_mm),\n            mean_predicted_bill_length = mean(preds)) %&gt;%\n  gt()\n\n\n\n\n\n\n\nspecies\nmean_bill_length\nmean_predicted_bill_length\n\n\n\n\nAdelie\n38.82397\n38.82397\n\n\nChinstrap\n48.83382\n48.83382\n\n\nGentoo\n47.56807\n47.56807\n\n\n\n\n\n\n\nThis is because \\(E[E[Y|X,Z]|Z=z]=E[Y|Z=z]\\).\nWe can view a fitted value from the regression, \\(E[Y|X,Z]\\), as a random variable to help us see this.\n\nSkip to the end to see the proof.\n\n\nI’ll admit I spent many weeks of my first probability theory course struggling to understand when and why my professor was writing \\(X\\) versus \\(x\\). When I finally learned all the rules for expectations of random variables, I still had zero appreciation for their implications in my future work as an applied statistician.\nI recently found myself in a rabbit hole of expectation properties while trying to write a seemingly simple function in R. Now that I have the output of my function all sorted out, I have a newfound appreciation for how I can use regressions – a framework I’m very comfortable with – to rethink some of the properties I learned in my probability theory courses.\nIn the function, I was regressing an outcome on a few variables plus a grouping variable, and then returning the group means of the fitted values. My function kept outputting adjusted group means that were identical to the unadjusted group means.\nI soon realized that for what I needed to do, my grouping variable should not be in the regression model. However, I was still perplexed as to how the adjusted and unadjusted group means could be the same.\nI created a very basic example to test this unexpected result. I regressed a variable from the new penguins data set, bill_length_mm, on another variable called bill_depth_mm and a grouping variable species. I then looked at the mean within each category of species for both the unadjusted bill_depth_mm and fitted values from my linear regression model for bill_depth_mm.\npenguins %&gt;%\n  # fit a linear regression for bill length given bill depth and species\n  # make a new column containing the fitted values for bill length\n  mutate(preds = predict(lm(bill_length_mm ~ bill_depth_mm + species, data = .))) %&gt;%\n  # compute unadjusted and adjusted group means\n  group_by(species) %&gt;%\n  summarise(mean_bill_length = mean(bill_length_mm),\n            mean_predicted_bill_length = mean(preds)) %&gt;%\n  gt()\n\n\n\n\n\n\n\nspecies\nmean_bill_length\nmean_predicted_bill_length\n\n\n\n\nAdelie\n38.82397\n38.82397\n\n\nChinstrap\n48.83382\n48.83382\n\n\nGentoo\n47.56807\n47.56807\n\n\n\n\n\n\nI saw the same strange output, even in my simple example. I realized this must be some statistics property I’d learned about and since forgotten, so I decided to write out what I was doing in expectations.\nFirst, I wrote down the unadjusted group means in the form of an expectation. I wrote down a conditional expectation, since we are looking at the mean of bill_length_mm when species is restricted to a certain category. We can explicitly show this by taking the expectation of a random variable, \\(\\mathrm{Bill Length}\\), while setting another random variable, \\(\\mathrm{Species}\\), equal to only one category at a time.\n\\(E[\\mathrm{BillLength}|\\mathrm{Species}=Adelie]\\)\n\\(E[\\mathrm{BillLength}|\\mathrm{Species}=Chinstrap]\\)\n\\(E[\\mathrm{BillLength}|\\mathrm{Species}=Gentoo]\\)\nMore generally, we could write out the unadjusted group mean using a group indicator variable, \\(\\mathrm{Species}\\), which can take on all possible values \\(species\\).\n\\(E[\\mathrm{BillLength}|\\mathrm{Species}=species]\\)\nSo that’s our unadjusted group means. What about the adjusted group mean? We can start by writing out the linear regression model, which is the expected value of \\(\\mathrm{BillLength}\\), conditional on the random variables \\(\\mathrm{BillDepth}\\) and \\(\\mathrm{Species}\\).\n\\(E[\\mathrm{BillLength}|\\mathrm{BillDepth},\\mathrm{Species}]\\)\nWhen I used the predict function on the fit of that linear regression model, I obtained the fitted values from that expectation, before I separated the fitted values by group to get the grouped means. We can see those fitted values as random variables themselves, and write out another conditional mean using a group indicator variable, just as we did for the unadjusted group means earlier.\n\\[E[E[\\mathrm{BillLength}|\\mathrm{BillDepth},\\mathrm{Species}]|\\mathrm{Species}=species]\\]\nMy table of unadjusted and adjusted Bill Length means thus showed me that:\n\\[E[E[\\mathrm{BillLength}|\\mathrm{BillDepth},\\mathrm{Species}]|\\mathrm{Species}=species] \\\\ = E[\\mathrm{BillLength}|\\mathrm{Species}=species]\\]\nOr, in more general notation:\n\\[E[E[Y|X,Z]|Z=z] = E[Y|Z=z]\\]\nIs it true?! Spoiler alert – yes. Let’s work through the steps of the proof one by one.\n\n\n\nProof set-up\nLet’s pretend for the proof that both our \\(Y\\) (outcome), \\(X\\) (adjustment variable), and \\(Z\\) (grouping variable) are categorical (discrete) variables. This is just to make the math a bit cleaner, since the expectation of a discrete variable (a weighted summation) is a little easier to show than the expectation of a continuous variable (the integral of a probability density function times the realization of the random variable).\nA few fundamental expectation results we’ll need:\n\nConditional probability\n\\(P(A|B) = \\frac{P(A ∩ B)}{P(B)}\\)\n\n\nPartition theorem\n\\(E[A|B] = \\sum_Ba \\cdot P(A=a|B=b)\\)\n\n\nMarginal distribution from a joint distribution\n\\(\\sum_A\\sum_Ba\\cdot P(A=a,B=b) = \\sum_Aa\\sum_B\\cdot P(A=a,B=b) = \\sum_Aa\\cdot P(A=a)=E[A]\\)\n\n\n\n\nStep-by-step Proof\nClick on the superscript number after each step for more information.\n\\(E[E[Y|X,Z]|Z=z]\\)\n\\(=E[E[Y|X,Z=z]|Z=z]\\) 1\n1 Because we’re making our outer expectation conditional on \\(Z=z\\), we can also move \\(Z=z\\) into our inner expectation. This becomes obvious in the penguins example, since we only use the fitted values from one category of species to get the adjusted group mean for that category.2 We can rewrite \\(E[Y|X,Z=z]\\) as the weighted summation of all possible values \\(X\\) can take. \\(E[Y|X,Z=z]\\) will only ever be able to take values of \\(X\\) that vary over the range of \\(x\\), \\(E[Y|X=x,Z=z]\\) since our value \\(z\\) is already fixed. We can weight each of these possible \\(E[Y|X=x,Z=z]\\) values by \\(P(X=x|Z=z)\\), since that’s the probabilty \\(X\\) will take value \\(x\\) at our already-fixed \\(z\\). Thus, we can start to find \\(E[E[Y|X,Z=z]|Z=z]\\) by weighting each \\(E[Y|X=x,Z=z]\\) by \\(P(X=x|Z=z)\\) and adding them all up (see Partition Theorem).\\(=\\sum_{X}E[Y|X=x,Z=z]\\cdot P(X=x|Z=z)\\) 2\n\\(=\\sum_{X}\\sum_{Y}y P(Y=y|X=x,Z=z)\\cdot P(X=x|Z=z)\\) 3\n3 We can get the expectation of \\(Y\\) at each of those possible values of \\(X\\) by a similar process as step 2 (weighting each \\(y\\) by \\(P(Y=y|X=x, Z=z)\\).4 By the Law of Conditional Probability, we can rewrite our conditional probabilities as joint distributions.\\(=\\sum_{X}\\sum_{Y}y \\frac{P(Y=y,X=x,Z=z)}{P(X=x,Z=z)}\\cdot \\frac{P(X=x,Z=z)}{P(Z=z)}\\) 4\n\\(=\\sum_{X}\\sum_{Y}y \\frac{P(Y=y,X=x,Z=z)}{P(Z=z)}\\) 5\n5 The denominator of the first fraction cancels out with the numerator of the second fraction.6 We can switch the summations around so that \\(y\\) is outside the summation over all values of \\(X\\). This lets us get the joint distribution of only \\(Y\\) and \\(Z\\).\\(=\\sum_{Y}y\\sum_{X}\\frac{P(Y=y,X=x,Z=z)}{P(Z=z)}\\) 6\n\\(=\\sum_{Y}y\\frac{P(Y=y,Z=z)}{P(Z=z)}\\) 7\n7 This is a conditional expectation, written in the form of a joint distribution.8 By the Partition Theorem.\\(=\\sum_{Y}y P(Y=y|Z=z)\\) 8\n\\(=E[Y|Z=z]\\) 9\n9 Rewriting the previous equation as an expectation.So, we’ve proved that:\n\\(E[E[Y|X,Z]|Z=z] = E[Y|Z=z]\\)\nwhich, thankfully, means I have an answer to my function output confusion. It was a lightbulb moment for me to realize I should think of an inner expectation as a random variable, and all the rules I learned about conditional and iterated expectations can be revisited in the regressions I fit on a daily basis.\nHere’s hoping you too feel inspired to revisit probability theory from time to time, even if your work is very applied. It is, after all, a perfect activity for social distancing! 😷\n\n\nReferences\nGorman KB, Williams TD, Fraser WR (2014) Ecological Sexual Dimorphism and Environmental Variability within a Community of Antarctic Penguins (Genus Pygoscelis). PLoS ONE 9(3): e90081. https://doi.org/10.1371/journal.pone.0090081\nA Conditional Expectation - Arizona Math"
  },
  {
    "objectID": "blog/forest-plots/index.html",
    "href": "blog/forest-plots/index.html",
    "title": "Annotated Forest Plots using ggplot2",
    "section": "",
    "text": "This post contains a short R code walkthrough to make annotated forest plots like the one shown above. There are packages to make plots like these such as forester, forestplot, and ggforestplot, but sometimes I still prefer to make my own.\nThe big picture of this is that we’ll be making three separate ggplot2 objects and putting them together with patchwork. You could also use packages like cowplot, gridarrange or ggarrange to put the intermediate plot objects together. You can skip to the end to see the full code.\n\n\nStep 0: Load libraries and data\nFirst we will load the necessary libraries and the data set. The data we’ll use for this plot are the effect estimates for 10 Cox regression models. The models, titled A-J, are stored in data sets called res and res_log, stored as csvs on my Github. We’ll combine these data and then begin plotting.\nlibrary(tidyverse)\nlibrary(gt)\n\nres_log &lt;- read_csv(\"https://raw.githubusercontent.com/kathoffman/steroids-trial-emulation/main/output/res_log.csv\")\nres &lt;- read_csv(\"https://raw.githubusercontent.com/kathoffman/steroids-trial-emulation/main/output/res.csv\")\n\nres &lt;- res_log |&gt;\n      rename_with(~str_c(\"log.\", .), estimate:conf.high) |&gt;\n  select(-p.value) |&gt;\n  full_join(res)\nThe results object contains:\n\nmodel: the model label A-J\nlog.estimate: log hazard ratio, since these were Cox regressions\nlog.conf.low and log.conf.high: log hazard ratio 95% confidence intervals\nestimate: hazard ratio\nconf.low and conf.high: hazard ratio 95% confidence intervals\np.value: corresponding p-value\n\n\nglimpse(res)\n\nRows: 10\nColumns: 8\n$ model         &lt;chr&gt; \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"\n$ log.estimate  &lt;dbl&gt; -0.68468788, -0.05255784, -0.08640963, -0.12147024, -0.4…\n$ log.conf.low  &lt;dbl&gt; -0.8947661, -0.4201151, -0.4567781, -0.5869119, -0.88192…\n$ log.conf.high &lt;dbl&gt; -0.474609681, 0.314999396, 0.283958850, 0.343971392, 0.0…\n$ estimate      &lt;dbl&gt; 0.5042476, 0.9487994, 0.9172184, 0.8856174, 0.6559699, 0…\n$ conf.low      &lt;dbl&gt; 0.4087032, 0.6569712, 0.6333208, 0.5560418, 0.4139865, 0…\n$ conf.high     &lt;dbl&gt; 0.6221278, 1.3702585, 1.3283783, 1.4105383, 1.0393974, 0…\n$ p.value       &lt;dbl&gt; 1.681527e-10, 7.792783e-01, 6.474743e-01, 6.089951e-01, …\n\n\n\n\nStep 1: Make point and line range section of the plot\nWe will first work on making the standard “forest plot”, or the middle section of the figure. This section uses points and lines to indicate the estimate and 95% confidence interval around the estimate.\nIn my experience, journal editors sometimes ask for these estimates to look a certain way during the revision process. For this graph, the journal editor told me that it was “journal standard to plot relative measures (ratio data), such as ORs, on log scales to preserve the correct spatial relationship between values.” So, I’m going to be visually showing the log hazard ratio, and annotating later with the hazard ratio.\nLet’s look at how we can plot the log hazard ratio estimates. We first want the models to show in alphabetical order from the top to the bottom of the graph.\np &lt;- \n  res |&gt;\n  ggplot(aes(y = fct_rev(model))) + \n  theme_classic()\np\n\n\n\n\n\n\n\nThen we show all of our information (point estimate and 95% confidence interval) on the graph.\np &lt;- p +\n  geom_point(aes(x=log.estimate), shape=15, size=3) +\n  geom_linerange(aes(xmin=log.conf.low, xmax=log.conf.high)) \np\n\n\n\n\n\n\n\nWe can now add a vertical line at 0 with geom_vline and rename the x axis.\np &lt;- p +\n  geom_vline(xintercept = 0, linetype=\"dashed\") +\n  labs(x=\"Log Hazard Ratio\", y=\"\")\np\n\n\n\n\n\n\n\nNext we’ll use coord_cartesian() which will allow us to zoom to the exact height and width we want. I want to zoom out a bit to leave myself room for the text “Corticosteroids protective” vs. “Corticosteroids harmful” so I’m going to set my limits to y=c(1,11). Each of the models (10 in total) is one unit, so this will give me one extra unit of space at the top of the plot. The x-limit I played around with a bit based upon the range of my log hazard ratios, and I ultimately arrived at xlim=c(-1, .5).\np &lt;- p +\n  coord_cartesian(ylim=c(1,11), xlim=c(-1, .5))\np\n\n\n\n\n\n\n\nNow we have space to add our text about protective vs. harmful using the annotate layer.\np &lt;- p +\n  annotate(\"text\", x = -.32, y = 11, label = \"Corticosteroids protective\") +\n  annotate(\"text\", x = .3, y = 11, label = \"Corticosteroids harmful\")\np\n\n\n\n\n\n\n\nFinally, we will remove everything on the y axis, because this plot is going to align with the next plot we make, showing the hazard ratios.\np_mid &lt;- p + \n  theme(axis.line.y = element_blank(),\n        axis.ticks.y= element_blank(),\n        axis.text.y= element_blank(),\n        axis.title.y= element_blank())\np_mid\n\n\n\n\n\n\n\nWe’ll save this ggplot object as p_mid and move on to the next section of the figure.\n\n\nStep 2: Create estimate annotations plot\nTo plot the hazard ratio estimates, we first need to modify the data set a bit. We’ll start by rounding our estimates to the significant figures the journal requires. For this figure, I need two decimal places. I’ll round the numbers to two decimal places (except for p-values that are &lt; 0.01), and then “pad” the numbers as strings so that they take up 4 characters. In other words, we will turn a p-value that reads 0.2 into 0.20, which has four characters (three numbers and a decimal point).\nI’ll also paste the low and high confidence intervals for my hazard ratio estimates together with a hyphen, and call this variable estimate_lab.\nFinally, I’ll bind a row of data that shows what I want the column names of my annotations to read as (e.g. Model, Hazard Ratio). The real column names of this data frame won’t appear on my plot, but the data in the rows themselves will. This should become clearer as we create the annotation sections of the plots.\n\n# wrangle results into pre-plotting table form\nres_plot &lt;- res |&gt;\n  # round estimates and 95% CIs to 2 decimal places for journal specifications\n  mutate(across(\n    c(estimate, conf.low, conf.high),\n    ~ str_pad(\n      round(.x, 2),\n      width = 4,\n      pad = \"0\",\n      side = \"right\"\n    )\n  ),\n  # add an \"-\" between HR estimate confidence intervals\n  estimate_lab = paste0(estimate, \" (\", conf.low, \"-\", conf.high, \")\")) |&gt;\n  # round p-values to two decimal places, except in cases where p &lt; .001\n  mutate(p.value = case_when(\n    p.value &lt; .001 ~ \"&lt;0.001\",\n    round(p.value, 2) == .05 ~ as.character(round(p.value,3)),\n    p.value &lt; .01 ~ str_pad( # if less than .01, go one more decimal place\n      as.character(round(p.value, 3)),\n      width = 4,\n      pad = \"0\",\n      side = \"right\"\n    ),\n    TRUE ~ str_pad( # otherwise just round to 2 decimal places and pad string so that .2 reads as 0.20\n      as.character(round(p.value, 2)),\n      width = 4,\n      pad = \"0\",\n      side = \"right\"\n    )\n  )) |&gt;\n  # add a row of data that are actually column names which will be shown on the plot in the next step\n  bind_rows(\n    data.frame(\n      model = \"Model\",\n      estimate_lab = \"Hazard Ratio (95% CI)\",\n      conf.low = \"\",\n      conf.high = \"\",\n      p.value = \"p-value\"\n    )\n  ) |&gt;\n  mutate(model = fct_rev(fct_relevel(model, \"Model\")))\n\nglimpse(res_plot)\n\nRows: 11\nColumns: 9\n$ model         &lt;fct&gt; A, B, C, D, E, F, G, H, I, J, Model\n$ log.estimate  &lt;dbl&gt; -0.68468788, -0.05255784, -0.08640963, -0.12147024, -0.4…\n$ log.conf.low  &lt;dbl&gt; -0.8947661, -0.4201151, -0.4567781, -0.5869119, -0.88192…\n$ log.conf.high &lt;dbl&gt; -0.474609681, 0.314999396, 0.283958850, 0.343971392, 0.0…\n$ estimate      &lt;chr&gt; \"0.50\", \"0.95\", \"0.92\", \"0.89\", \"0.66\", \"0.77\", \"1.05\", …\n$ conf.low      &lt;chr&gt; \"0.41\", \"0.66\", \"0.63\", \"0.56\", \"0.41\", \"0.60\", \"0.77\", …\n$ conf.high     &lt;chr&gt; \"0.62\", \"1.37\", \"1.33\", \"1.41\", \"1.04\", \"0.99\", \"1.45\", …\n$ p.value       &lt;chr&gt; \"&lt;0.001\", \"0.78\", \"0.65\", \"0.61\", \"0.07\", \"0.045\", \"0.75…\n$ estimate_lab  &lt;chr&gt; \"0.50 (0.41-0.62)\", \"0.95 (0.66-1.37)\", \"0.92 (0.63-1.33…\n\n\nTo create the hazard ratio annotations, we’ll first organize the model order on the y axis.\np_left &lt;-\n  res_plot  |&gt;\n  ggplot(aes(y = model))\np_left\n\n\n\n\n\n\n\nNext, we will add the model as text (instead of as a label on the y axis) using geom_text. We can specify where we want the text to show up by specifying the x axis should be 0. We will also set the hjust (horizontal justification) to 0 and fontface to bold type.\np_left &lt;- \n  p_left +\n  geom_text(aes(x = 0, label = model), hjust = 0, fontface = \"bold\")\np_left\n\n\n\n\n\n\n\nWe can use the same idea to add the hazard ratios and their confidence intervals, however, we want to specify that the hazard ratio data (estimate_lab variable) should only be bold if the value is equal to the title. We will do this using an ifelse statement within the fontface argument.\np_left &lt;- \n  p_left +\n  geom_text(\n    aes(x = 1, label = estimate_lab),\n    hjust = 0,\n    fontface = ifelse(res_plot$estimate_lab == \"Hazard Ratio (95% CI)\", \"bold\", \"plain\")\n  )\n\np_left\n\n\n\n\n\n\n\nFinally, we can remove the background and edit the sizing so that this left size of the plot will match up neatly with the middle and right sides of the plot. Remember, coord_cartesian allows the user to zoom in and out of the plot without removing data.\np_left &lt;-\n  p_left +\n  theme_void() +\n  coord_cartesian(xlim = c(0, 4))\n\np_left\n\n\n\n\n\n\n\nThat’s it for the hazard ratio annotation section of the plot! Next we can create the p-value annotation of the plot.\n\n\nStep 3: Create p-value annotations\nWe will follow the same steps as in Step 2 to create the p-value annotations. Again, we use an ifelse statement to specify that only the word p-value should be bolded, rather than all the p-values.\n# right side of plot - pvalues\np_right &lt;-\n  res_plot  |&gt;\n  ggplot() +\n  geom_text(\n    aes(x = 0, y = model, label = p.value),\n    hjust = 0,\n    fontface = ifelse(res_plot$p.value == \"p-value\", \"bold\", \"plain\")\n  ) +\n  theme_void() \n\np_right\n\n\n\n\n\n\n\n\n\nStep 4: Put the three plots together with patchwork\nFinally, we will use the package patchwork to merge all three plots together. You could equivalently use gridarrange, cowplot, or some other ggplot2 figure-merging package of your choice.\nFor this specific use of patchwork you need to specify the top, left, right, and bottom indices of each plot in an areas function, and then put these together in a vector, which I call layout. Then, you can specify that layout in the design argument of plot_layout.\nTo learn about the syntax of patchwork and its plot_layout function check out this article.\nlayout &lt;- c(\n  area(t = 0, l = 0, b = 30, r = 3), # left plot, starts at the top of the page (0) and goes 30 units down and 3 units to the right\n  area(t = 1, l = 4, b = 30, r = 9), # middle plot starts a little lower (t=1) because there's no title. starts 1 unit right of the left plot (l=4, whereas left plot is r=3), goes to the bottom of the page (30 units), and 6 units further over from the left plot (r=9 whereas left plot is r=3)\n  area(t = 0, l = 9, b = 30, r = 11) # right most plot starts at top of page, begins where middle plot ends (l=9, and middle plot is r=9), goes to bottom of page (b=30), and extends two units wide (r=11)\n)\n# final plot arrangement\np_left + p_mid + p_right + plot_layout(design = layout)\n\n\n\n\n\n\n\n\n\nStep 5: Export your plot!\nI typically export as an encapsulated postscript (eps) file using ggsave for journals, but you could instead export as a jpg, png, etc.\nggsave(\"forest-plot.eps\", width=9, height=4)\nHopefully that’s not too hard to follow! Please feel free to email me if you find any errors or confusing parts. Here’s the full code:\n\n\nJust the code\n## load up the packages we will need: \nlibrary(tidyverse)\nlibrary(gt)\nlibrary(patchwork)\n## ---------------------------\n## load data\n# load in results generated from Cox PH hazards models\nres_log &lt;- read_csv(\"model-first-results-log.csv\")\nres &lt;- read_csv(\"model-first-results.csv\")\nres &lt;- res_log |&gt;\n      rename_with(~str_c(\"log.\", .), estimate:conf.high) |&gt;\n  select(-p.value) |&gt;\n  full_join(res)\n\n## plotting\n## ---------------------------\n# create forest plot on log scale (middle section of figure)\np_mid &lt;-\n  res |&gt;\n  ggplot(aes(y = fct_rev(model))) +\n  theme_classic() +\n  geom_point(aes(x=log.estimate), shape=15, size=3) +\n  geom_linerange(aes(xmin=log.conf.low, xmax=log.conf.high)) +\n  labs(x=\"Log Hazard Ratio\") +\n  coord_cartesian(ylim=c(1,11), xlim=c(-1, .5))+\n  geom_vline(xintercept = 0, linetype=\"dashed\") +\n  annotate(\"text\", x = -.32, y = 11, label = \"Corticosteroids protective\") +\n  annotate(\"text\", x = .3, y = 11, label = \"Corticosteroids harmful\") +\n  theme(axis.line.y = element_blank(),\n        axis.ticks.y= element_blank(),\n        axis.text.y= element_blank(),\n        axis.title.y= element_blank())\n# wrangle results into pre-plotting table form\nres_plot &lt;- res |&gt;\n  mutate(across(c(estimate, conf.low, conf.high), ~str_pad(round(.x, 2), width=4, pad=\"0\", side=\"right\")),\n         estimate_lab = paste0(estimate, \" (\", conf.low, \"-\", conf.high,\")\"),\n         color = rep(c(\"gray\",\"white\"),5)) |&gt;\n  mutate(p.value = case_when(p.value &lt; .01 ~ \"&lt;0.01\", TRUE ~ str_pad(as.character(round(p.value, 2)),width=4,pad=\"0\",side=\"right\"))) |&gt;\n  bind_rows(data.frame(model = \"Model\", estimate_lab = \"Hazard Ratio (95% CI)\", conf.low = \"\", conf.high=\"\",p.value=\"p-value\")) |&gt;\n  mutate(model = fct_rev(fct_relevel(model, \"Model\")))\n# left side of plot - hazard ratios\np_left &lt;-\n  res_plot  |&gt;\n  ggplot(aes(y = model)) + \n  geom_text(aes(x=0, label=model), hjust=0, fontface = \"bold\") +\n  geom_text(aes(x=1, label=estimate_lab), hjust=0, fontface = ifelse(res_plot$estimate_lab == \"Hazard Ratio (95% CI)\", \"bold\", \"plain\")) +\n  theme_void() +\n  coord_cartesian(xlim=c(0,4))\n# right side of plot - pvalues\np_right &lt;-\n  res_plot  |&gt;\n  ggplot() +\n  geom_text(aes(x=0, y=model, label=p.value), hjust=0, fontface = ifelse(res_plot$p.value == \"p-value\", \"bold\", \"plain\")) +\n  theme_void() \n# layout design (top, left, bottom, right)\nlayout &lt;- c(\n  area(t = 0, l = 0, b = 30, r = 3),\n  area(t = 1, l = 4, b = 30, r = 9),\n  area(t = 0, l = 9, b = 30, r = 11)\n)\n# final plot arrangement\np_left + p_mid + p_right + plot_layout(design = layout)\n## save final figure\n#ggsave(\"forest-plot.eps\", width=9, height=4)\n\n\nSession Info\n\nsessionInfo()\n\nR version 4.1.2 (2021-11-01)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] patchwork_1.1.2 gt_0.7.0        forcats_0.5.1   stringr_1.4.0  \n [5] dplyr_1.0.10    purrr_0.3.5     readr_2.1.2     tidyr_1.2.1    \n [9] tibble_3.1.8    ggplot2_3.4.0   tidyverse_1.3.2\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9          lubridate_1.8.0     assertthat_0.2.1   \n [4] digest_0.6.30       utf8_1.2.2          R6_2.5.1           \n [7] cellranger_1.1.0    backports_1.4.1     reprex_2.0.1       \n[10] evaluate_0.14       httr_1.4.2          pillar_1.8.1       \n[13] rlang_1.0.6         curl_4.3.2          googlesheets4_1.0.0\n[16] readxl_1.3.1        rstudioapi_0.13     rmarkdown_2.11     \n[19] labeling_0.4.2      googledrive_2.0.0   htmlwidgets_1.5.4  \n[22] bit_4.0.4           munsell_0.5.0       broom_1.0.1        \n[25] compiler_4.1.2      modelr_0.1.8        xfun_0.29          \n[28] pkgconfig_2.0.3     htmltools_0.5.2     tidyselect_1.2.0   \n[31] fansi_1.0.3         crayon_1.5.0        tzdb_0.2.0         \n[34] dbplyr_2.1.1        withr_2.5.0         grid_4.1.2         \n[37] jsonlite_1.7.3      gtable_0.3.1        lifecycle_1.0.3    \n[40] DBI_1.1.2           magrittr_2.0.3      scales_1.2.1       \n[43] cli_3.4.1           stringi_1.7.6       vroom_1.5.7        \n[46] farver_2.1.1        fs_1.5.2            xml2_1.3.3         \n[49] ellipsis_0.3.2      generics_0.1.3      vctrs_0.5.0        \n[52] tools_4.1.2         bit64_4.0.5         glue_1.6.2         \n[55] hms_1.1.1           parallel_4.1.2      fastmap_1.1.0      \n[58] yaml_2.2.2          colorspace_2.0-3    gargle_1.2.0       \n[61] rvest_1.0.2         knitr_1.37          haven_2.4.3"
  },
  {
    "objectID": "blog/ditl/index.html",
    "href": "blog/ditl/index.html",
    "title": "A Day in the Life of a Biostatistician",
    "section": "",
    "text": "It seems fitting that my first blog post is on a topic that I tried and failed to find via Google search a few years ago.\nI’ll back up for a second. A few years ago I was a recent college graduate, and trying hard to “figure out my life.” My major was biochemistry, which is one of those degrees where 99%* of people just keep on going to school.\n\nI was working full-time night-shift at a hospital as a nurse technician. The key word in that sentence is “night-shift” which meant that even on my days off, I didn’t sleep at night, but all my friends and family did. So, I was often alone and awake, with a lot of time to think about my future… and surf the web for potential careers.\nI knew I wanted a job in healthcare, but I was confused as to where in medicine would be a good fit for me. I could visualize very clearly what my days at work would look like if I were to become a physician, or a physician assistant, or a registered nurse (all careers I developed lengthy pros and cons lists for).\nHowever, there was another career involving medicine that I was drawn to but didn’t know enough about. “Biostatistics” was a class biochemistry majors took at my university, but I had been exempted because I took AP Statistics in high school.\nTo me, biostatistics seemed to be the application of some high school-level math to biological problems. I had no concept of what a degree in biostatistics, and much less a career as a biostatistician, could entail. Endless Google searches with some variant of “what does a biostatistician do” and even “day in the life of a biostatistician” had not given me a very good picture of what I would actually be doing as a biostatistician.\nI ultimately lucked out during a conversation with a professor during my senior year of college. I was rambling about my many life plans and he mentioned his cousin was the chair of a well-known Biostatistics program. He encouraged me to email her my questions about biostatistics, and I am so grateful she took time to respond with detailed answers. Her description of life as a biostatistician was enough for me to choose going to graduate school for an MS in biostatistics over medical school/physician assistant school/second-degree nursing.\nI feel wholeheartedly that although I would have enjoyed my life as an MD/PA/RN, biostatistics is the right career for me. So, in honor of my confused younger self, and as a way of paying it forward, I’ve dedicated this topic for my very first blog post!\n\nHanging out with my mom (an RN of 41 years and counting!) after a particularly exhausting night shift as a nurse technician.\n\nA disclaimer: what follows is a day in the life of one masters-level, academic research-focused biostatistician and I cannot make claims about the careers of statisticians in industry or pharmaceuticals or hospitals or government. In addition, here’s a bit more background before I get into the granular details of my work:\n\nMy official job description is to assist investigators (i.e. physicians or PhD-level researchers with a scientific question) throughout all stages of the scientific research process. This means helping with study design, data collection, data cleaning (also known as getting the data in the right form for analysis and making sure nothing is obviously incorrect), data visualization, statistical analysis, reporting and explaining my results, and writing methods and results sections for scientific papers. You will soon see that on any given day I am working on multiple projects at various stages of this process.\nI will mention R/Rstudio a bit. For those who are not familiar with it, R is an open-source (which means anyone can help contribute) programming language that is well-equipped for statistical analysis. It’s arguably very similar to Python, which is a more widely used language, but statisticians tend to use R more because its statistical packages are very well-developed. During grad school I used Python because I worked in a computational biology lab, and I learned SAS (another statistical programming language) in some of my classes, but R is what I prefer these days. Rstudio is a platform that makes it more user friendly to use R.\n\nSo, without further ado! An average day**:\n\n9:30AM - I arrive at my office and spend a few minutes chatting with my coworkers. To set the scene for you, I have a fairly spacious cubicle within a group of five other cubicles. I sit next to another research biostatistician, two health informatics professionals, and two clinical trial grant specialists. I’m actually not completely sure what that last pair’s title is, but I know their primary task is to make sure several multi-million dollar clinical trial grants stay funded (woah). Everyone I sit by is young and goofy, but very driven, making for a fun office environment.\n9:45AM - I check and answer new emails from researchers I collaborate on projects with. I send my availability for a meeting to a group of doctors who want to go over the results of a recent analysis I did on Body Mass Index and death rates in the Intensive Care Unit. In a different thread of emails, I thank several researchers from another university for clarifying their methods and sending me code for an analysis similar to one I will soon work on.\n10:00AM - I open a manuscript draft for a paper I received yesterday. It’s from a group of residents and medical students I worked with a few months ago. Their study looks at the association between blood levels of a certain biomarker and the time to death in cancer patients. My role in the analysis was to examine the associations between several biomarkers such as phosphorus, phosphate, and calcitriol. I then fit a regression model, just like y=mx+b, but with way more math. For this analysis I used a model for when your y is a time to an event (death, in this case), fittingly called a survival model. After adjusting for confounding factors like age, which affects both tumor progression and biomarker levels, there was a significant association between the biomarker and time to death in cancer patients.\n\n\n\n\n\n\n\n\n\nAn example of a figure for a manuscript. This particular plot suggests that cancer patients with an elevated biomarker died at a faster rate than patients who did not have an elevated biomarker.\nThe researchers have asked for my assistance in writing the methods section. The methods section of a scientific article is the steps the scientists took to analyze data explicitly written out for anyone looking to review or learn about their work. I read through their current draft of the paper carefully, make some edits, and send it back. They are hoping to submit this paper to a peer-reviewed journal within a few weeks.\n10:45AM - I have a weekly meeting a few blocks from my office with a neurologist I spend a large portion of my time working with. She is a leader in the field of Alzheimer’s research, and I find it very rewarding to work on her data and be a small part of a growing body of research in the field. I am “contracted” out to her research as I am to all of the researchers I work with—it’s how my institution budgets funding for grants. One of the faculty-level biostatisticians in my department—which means he has a PhD and specializes in certain statistical methods—is also part of this contract, and some days, like today, he joins me at these meetings.\n11:00AM - This week’s meeting is pretty straightforward. We discuss how we can improve one of the neurologist’s National Institute of Health grants from a statistics standpoint. The statistical methods for this project can get complicated, in part because we are looking at the brain scans of women in different stages of menopause over time, and we have to consider age as a confounding factor. We want to convey to the reviewers of our grant how we plan to do this. Since this is a methods-focused meeting, I mostly listen and take notes along with two neurology research coordinators that also attend these weekly meetings. When the meeting concludes I have for less work than usual - I only need to make a few graphs representing our study design and past results for inclusion to the grant.\n\n12:00PM - I head to lunch with a group of coworkers. They have gotten food from a nearby salad place, and we sit in one of our favorite buildings on campus and eat together. Our jokes oscillate from incredibly nerdy to pretty stupid. One of my coworkers points out that our hair is styled the same way for the third day in a row.\n12:45PM - I get back to my desk and type up my handwritten notes from the neurology meeting and put them in that project’s “Notes” folder. It’s important to me that I keep track of all my meetings electronically - I fear losing my notebooks or someone else having to decipher my cursive should I ever have to pass off a project.\n\n1:00PM - I start to make a plan for a different analysis I’m working on. This project is something new for me - it involves a protein assay and data for 1000+ different protein expression levels. The researcher I’m working with wants to know which proteins are over- and under-expressed in people with a specific autoimmune disorder and a certain type of lung disease. I’ve recently spoken to some bioinformaticians and have a clearer idea of the analysis I need to do. I draw out a little map of the code organization I think would be the most efficient for this analysis and open up Rstudio.\n1:30PM - I get to work writing up functions, which is just a fancy programming way of saying your code can do the same thing to multiple data sets (or subsets of patients, as is the case of this protein expression study I’m doing). Sometimes it takes a bit longer to write my functions than it would if I were to just copy and paste my code several different times, but the final code is much more readable and less prone to errors. By the time I’m done working, I have some interactive plots showing the significant and non-significant results. When you hover over them, they show what protein corresponds to which point on the graph. They look like this, except this is not the real data we used in her study.\n\nI use Rstudio and its amazing Rmarkdown tool to craft a draft report to the researcher, and I save it with today’s date in my “Reports” folder for that project. The report so far includes unadjusted and adjusted models of all the protein expressions using very small p-values to account for the 1000+ statistical comparisons we’re making. I show the results in various plots such as the one above (called a “volcano plot” for its shape).\nI have also started writing code for models to determine which proteins are most different, or uniquely expressed, between subgroups of patients. Tomorrow, I will use a technique common in machine learning, called clustering, to see if these protein expressions can correctly classify subgroups of patients. The goal is to find a minimum group of proteins to identify patients of interest who have both the autoimmune disorder and the lung disease my collaborator is interested in. One way this research could be impactful is that it may help determine which proteins pharmaceuticals should develop drugs to target.\nI close the report; I will continue working on this analysis tomorrow.\n4:45PM - It’s time for my last meeting of the day. I head to another floor of my office building and get my laptop set up in a conference room. I await the arrival of several doctors. It will be my first time meeting most of them, and our task today is to discuss the data collection process for a future study involving both genetic mutation data from tumor biopsies and clinical data from electronic health records on thousands of patients with lung cancer.\n5:00PM - The doctors arrive and, after introductions, we talk about the current stage of the project and what the goals are. We discuss the timing of starting chart reviews of patients, how we will upload the information to a database efficiently, and what might be the best way to condense the highly detailed genetics data into useful information for an analysis. Our solution will likely involve a series of iterative searches through the columns containing genetic information.\n\nThe start of a database I am helping researchers build to collect data for their study. This is not my favorite part of my job, but it is a crucial step for good scientific research.\n6:00PM - The meeting ends, and I go back to my desk to record more handwritten notes. I log the hours I spent on each project that day into Toggl, which is the time-tracking application our team uses. This is so we know how much time we’re spending on each project, and is as much for our own sake as it is anyone else’s. I update my to-do list, which is a giant color-coded excel spreadsheet, and eat a few chocolate covered raisins as my reward for a productive day.\n6:30PM - I leave work! I typically have some kind of activity, like happy hour (see my cute cubicle buddies below), a sports game, or Spanish class that I’m heading off to. Some days I attend coding workshops hosted by groups such as R-ladies. On nights when I’m feeling especially nerdy, I’ll go home and read a statistics paper or sift through the #rstats tips on twitter.\n\n…So, there you have it. One average daily experience as an early career masters-level statistician. All in all, I have an excellent work-life balance and overall work environment. Every day I get to learn more about science, medicine, statistics, and the intersection of these wonderful ideas. Although it varies quite a bit, approximately 10% of each day involves writing, 20% interacting with other researchers, and the rest of it is spent thinking critically and finding answers to problems I am passionate about.\nI hope if there are any 22, 42, or 14 year-olds out there considering a career in biostatistics and struggling to figure out what on earth we actually do, that you find this post and it lessens your confusion!\nAll the best,\nKat\n\nIf you found this post useful, you might be interested in a follow up post I wrote answering common email questions I’ve received. I also interviewed my friend from grad school who is a Biostatistician for a pharmaceutical CRO in a separate blog post.\n\n*Not a real statistic.\n**Exact details and diseases of the studies I am currently working on have been generalized or altered to protect the research interests of my collaborators."
  },
  {
    "objectID": "blog/otr/otr.html",
    "href": "blog/otr/otr.html",
    "title": "Building Statistical Intuition for Optimal Treatment Rules",
    "section": "",
    "text": "Developing and optimizing optimal treatment rules (OTRs) is a fast-growing topic in the medical research community. A treatment rule is a decision for treatment based upon a person’s characteristics. The intuition behind this is that not all individuals will respond to a treatment in the same way. We can exploit these heterogeneous effects and develop personalized rules which provide benefit a greater number of people.\nThe methods of OTRs are rooted in principles of causal inference, or using data to inform us about what would have happened in a hypothetical world in which different interventions had occurred. This post walks through the basic statistical intuition for OTRs. Each explanation is accompanied by mathematical notation and a small graphic to convey equivalent meaning."
  },
  {
    "objectID": "blog/otr/otr.html#acknowledgments",
    "href": "blog/otr/otr.html#acknowledgments",
    "title": "Building Statistical Intuition for Optimal Treatment Rules",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThanks to my colleague Iván Díaz for explaining OTRs to me in this way, and for reviewing this post."
  },
  {
    "objectID": "blog/covid/index.html",
    "href": "blog/covid/index.html",
    "title": "On the Sidelines: NYC’s COVID-19 Outbreak from the Eyes of a Pulmonary and Critical Care Team’s Biostatistician",
    "section": "",
    "text": "This article is dedicated to the healthcare workers who risked their lives to care for patients with COVID-19. It was originally published on October 3, 2020 and has been republished by Kevin MD, The Pursuit, The Monroe Evening News, and Significance.\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nDecember 15, 2018. My coworker is moving to California. She’s a statistician for a group of pulmonary and critical care physicians at our New York City hospital, and I’m a statistician who’s trying not to do too many things wrong, only three months into my first job out of school. “I think you’d be good with this research team,” she tells me. “There’s some really interesting studies on lung diseases.” I nod, because that’s what you do when you’ve been at your job for three months.\nI take over her projects and start learning organ failure scoring systems, criteria for acute respiratory distress syndrome, and the differences between invasive and non-invasive mechanical ventilation. My close friend does cutting-edge cancer statistics, and I feel a bit resentful. Nobody ever wants to hear about the controversial definitions of sepsis at family parties.\nAs the months pass by, I slowly build my mental encyclopedia and begin to embrace my role as a pulmonary and critical care biostatistician.\nMarch 5th, 2020. A full year and three months later, I wake up very sick. It is the kind of sick where you can’t do anything but curl up on your bathroom floor and let being sick consume you. Too sick to read, too sick to sleep. I spike a fever and can hardly move for two days before I hobble to the doctor’s office and nearly faint mid-exam. The doctor insists I stay until I drink an entire bottle of water. “Is there anyone to check on you at home?” he asks, concerned. No, no, I’ll be fine.\nBy the end of the week my fever breaks and I’m back to work. It’s early March, so “Coronavirus?!” is everyone’s first question. They’re all joking, except the pulmonologists I work with. Nothing respiratory, I assure them. One isn’t convinced. “Some young people are getting ‘silent hypoxia.’ It is possible to have COVID-19 with no respiratory symptoms at all,” she tells me. Months later, I’ll read that as the headline of various news articles, but the time, no testing is available to me.\nMarch 17, 2020. Barely two weeks pass before the number of confirmed COVID-19 cases explode in New York City. Restaurants are instructed to close the day before Saint Patrick’s Day, my birthday. I can’t meet up with my friends anymore, so I cook macaroni and cheese and run to Central Park to watch the sun set behind skyscrapers. My grandparents call me and they make Happy Birthday sound like a hymn from a Catholic mass and I laugh, and it is the only part of my day that feels like every other birthday.\nWhile I’m leaving the park, my mom texts me that she hopes I had a good day. Any other year it would be strange for her to nearly miss my birthday, but this year she is working long hours. She’s a nursing director back in Michigan and her hospital is already preparing for their own impending COVID-19 outbreak. The preparations will not be in vain.\nAs I jog home, I pass a sign asking former healthcare workers to volunteer to take care of NYC COVID-19 patients. Before I began my career in biostatistics, I worked at a hospital caring for acutely ill patients, so I sign up without hesitation. My misguided logic is that the exploding numbers of COVID-19 cases will make my critical care collaborators too busy to pursue their research, and this seems like the best way for me to help as the world descends into chaos. While I fill out the online contact form, I wonder what it will feel like to take care of patients again. I look up YouTube videos to refresh myself on drawing blood and inserting IVs.\nHow absolutely crazy that I thought my biostatistics training wouldn’t be useful, in retrospect.\nMarch 22, 2020. I’m a pulmonary and critical care team’s statistician so naturally I am one of the first analysts at my hospital pulled into COVID-19 work. It starts with a text on a Sunday – the first of many – from a pulmonologist. “Do we have a data dictionary for our ICU database, Kat?” Our informatics team is using the structure of the ICU database I work with as part of a COVID-19 tracking repository for our entire hospital. Within days, I am told to drop all of my other research projects for COVID-19 work.\nThe first request for me is straightforward: summarize the laboratory results from our first 300 COVID-19 patients. 300 patients at our hospital! That’s insane, I think to myself. It seems only a week ago the news reports said there were 300 people in the entire city with COVID-19. I begin working through issues linking the databases, identifying missing information, and explaining critical care jargon to other analysts. Each morning I pull new data and watch the files grow exponentially larger.\nThere are countless questions flooding in from all over the hospital. Most of them revolve around “who will get intubated, and when?” My hospital, like so many other hospitals in NYC, is on track to run out of ventilators soon. My attendance becomes mandatory at multiple “risk prediction” meetings each week. I find myself in charge of extensive data cleaning and then writing code for models to answer vague and terrifying questions: we need to figure out which patients will “crash,” who can be transferred, and, if we run out of ventilators, who has the best chance to survive.\nI am a junior researcher, previously unconcerned with hospital operations, suddenly confronted with the task of providing rapid answers for potentially immediate decision making. I accept my new role with the utmost seriousness. My days, normally spent coding with double monitors at a proper desk, suddenly fill with Zoom meetings from 8:30-5:00 from a laptop at my kitchen table. Each night after the meetings end, I take advantage of the relative quiet to code into the early hours of the morning.\nFor several weeks I use the long, uninterrupted hours of weekends to work, waking up with the sun and continuing on until at least 11pm, with few breaks in between. On some nights I send my mom “good morning” texts at 5am. “Are you waking up early or have you not slept yet?” is always her first question. The next is, “No fever? No cough?” She is worried about me, living in the international epicenter, but I’m just as worried about her, working at a hospital every day. She informs me that my dad is sleeping in my old bedroom in case she brings the virus home.\nHospitals around the city begin to call me, wanting to know if I can still help care for COVID-19 patients as a former healthcare worker. “I want to but I can’t, I’m so sorry, I’m helping with COVID data now.” It sounds and feels inconsequential.\nApril 4, 2020. My best friend and her sister are also nurses in Michigan. I FaceTime her to check in. She and her sister’s units have become “hot floors”: every room is filled with a COVID-19 patient. They were living with their parents, another sister, uncle, and cousins, but both have moved to an AirBnb for the foreseeable future. “It’s so crazy here, Kitty,” she tells me in a defeated voice. At the time, Michigan’s case trajectory is second only to New York’s.\nOne of her nursing friends has been hospitalized with COVID-19 and is on 6 Liters of oxygen. I can’t help but think about the prediction models I’ve been working on. I mentally run his characteristics through them. I know what my models would estimate his probability of intubation to be.\nI listen to her talk about the N-95 masks they’ve been given. “Remember how they used to say those were one-time use?” she asks me. I do. “They started telling us they were good for the whole day, and then they said they’d be good for the whole week, and now they’re saying we might have to start sharing.” I wonder what data analyst, perhaps just like me, is crunching those numbers and feeding the information to hospital administration. “The virus is so terrible. I’ve never done so much post-mortem care, zipped so many body bags…” Her voice drifts off.\nI feel guilty, on the sidelines. I see the raccoon eyes – the only part of their faces visible between hair caps and procedure masks – of the physicians I spend all day hopping on and off meetings with, and I desperately want to help. I cannot hold the hand of a COVID-19 patient but I have all their data at my keystrokes: millions of lab results, vital signs, and procedure codes. I see their inflammatory cytokines spike, I watch their oxygen levels plummet, I can tell you which organs are failing, who’s on which experimental drug, and who’s just been made Do Not Intubate and Do Not Resuscitate. I follow in horror, almost in real-time, the time-stamps of admission, intubation, death. I cannot compare this experience to physically caring for COVID-19 patients, but I feel haunted by it all the same.\nI hole up in my tiny studio in Manhattan for days at a time, listening to the wails of ambulances and pings of messages from my computer. I see only one friend with any frequency; we both live alone, 18 blocks from each other. She texts me often, asking to meet in Central Park. She suspects I am not doing well, and she is right. I walk with her all over the Upper East Side a few times a week, each of us donned in our black cotton masks. We try not to talk about COVID-19, but it’s hard to avoid when our walks take us past the pop-up ICU tents and refrigerated trucks that stretch entire blocks – the overflow morgues for NYC’s dead.\nWe try to time our walks so that we’re outside at 7pm, when the city unites to cheer for healthcare workers. If I’m not out walking with her, I climb religiously onto my fire escape every night to clap. Sometimes a man in the apartment across the street sings Sinatra. I want to wake up in the city that never sleeps… New York, New York! I’ve only lived here two years, but I miss “the city that never sleeps” so badly that it hurts.\nLife continues in this way for me, with no real sense of time or distinguishing events, from mid March until early May.\nMay 10, 2020. It is Mother’s Day, and my 50th straight day of working with COVID-19 data. At 11pm, my cell phone goes off. It is an ominous vibration against my kitchen table, where I am perpetually sitting with my laptop whirring. “Hi Honey… I just wanted to let you know that, mmm…” it’s my mom, and her voice is cracking. I finish the sentence for her, “Aunt Peggy died?” I ask, sadly. “Yes.” “Okay. Thanks for letting me know.” I stare into the white brick wall in front of my kitchen table for so long that I start seeing multicolored spots.\nMy grandfather’s eldest sister, my Aunt Peggy, had begun showing telltale symptoms of COVID-19 and tested positive only a few days previously. She’d been without any visitors in her assisted living home for months due to isolation restrictions. She was royalty in our family; the red-lipsticked, always fashionably late, prized guest at every family party. She had an unforgettable, incredibly sweet voice, and I can still hear her words to me last Christmas. “How’s New York, Katherine? I’m so proud of you.” She was the first nurse in my family, and she influenced my mom to become a nurse, who influenced me to pursue medical research. The matriarch of our family left us on Mother’s Day.\nI spend the night trying to find a rental car company that will allow me to drive one-way from New York to Michigan. It can’t be done; I am several weeks too late in my exodus from the city. I book a flight instead and leave a few days later on a near-empty plane to spend time with my family. I plan to stay in Michigan for two weeks, but I don’t leave for two months.\nSeptember 20, 2020. The leaves I watched bud in Central Park during my walks this spring are changing to red and gold. As I write this, I think of countless other ways I could attempt to explain what my tiny corner of the world was like during NYC’s outbreak. Most are too personal to ever record. At the same time, it is difficult to share even the memories I have, partially because I know they are incomparable to the frontline workers’ who risked their lives everyday.\nMy experiences living and working in Manhattan during March, April, and May will stick with me forever. I hope there comes a day that I can meet in real life – mask free – all the analysts, hospital administrators, physicians, residents, fellows, medical students, and data engineers I conversed with so frequently during the height of the outbreak. At the same time, I hope we never have to work together again. It is a wish that I fear will not come true.\nJust this past week I attended a meeting with our Informatics team. “It’s good to ‘see’ everyone,” someone said. It’s only half true; the circumstances that bring us to meetings together are never good. We discussed data structures for a possible second wave of COVID-19 in NYC as schools and indoor dining reopen. After the call, I felt an immense sadness, despite being in a much better place than when I left the city in May.\nAt the bottom of my heart, I don’t know if I can handle another round of it all. Can you?"
  },
  {
    "objectID": "blog/trt-timelines/trt-timelines.html",
    "href": "blog/trt-timelines/trt-timelines.html",
    "title": "Patient Treatment Timelines for Longitudinal Survival Data",
    "section": "",
    "text": "A ggplot code walkthrough for making treatment timelines or “swimmer plots” for longitudinal time-to-event data on 1) a categorical covariate and 2) a continuous covariate organized by missingness frequency.\n\nNovember 3, 2019.\nI am a biostatistician at a research university, and I often find myself working with longitudinal survival data. As with any data analysis, I need to examine the quality of my data before deciding which statistical methods to implement.\nThis post contains reproducible examples for how I prefer to visually explore survival data containing longitudinal exposures or covariates. I create a “treatment timeline” for each patient, and the end product looks something like this:\n\n\n\n\n\n\n\nEach line represents one patient, and each square represents a measurement at a specified point. I find these graphs useful to look for patterns or discrepancies in follow up times, treatment, and missingness. They also allow me to verify that my coding is correct as I move through different data manipulations for my analyses.\nFor the following examples I generate longitudinal survival data sets. I skip over how I made this data because it’s not the focus of this post, but if you have questions about the data generation please let me know. In general, I am trying to work from the point in a survival data analysis where you’ve already calculated each individual’s time to event.\n\nExample 1: Binary Treatment\nTo make these charts, my first step is to load the package tidyverse, since I use functions from dplyr, tidyr, forcats, and ggplot2.\nlibrary(tidyverse)\nHere is the data generation code. If you’re interested in how I made it, you can check the comments, but if not, I recommend just copying and pasting into your R console to try the plotting code.\nset.seed(7)\nn &lt;- 50 # The data sets I make these visualizations for are typically 100-500 patients in size, but for space purposes I'll set my n to only 50.\ndat &lt;- \n  tibble(.rows = n) %&gt;%  # empty data frame / tibble with n rows\n  mutate(pt_id = factor(row_number()),  # patient ids are 1-n\n         pt_trt_prob = runif(n,0,1), # randomly generate a treatment probability for each patient so we can see patterns\n         months_followup = round(runif(n, 0, 20)), # randomly generate length of time in the study\n         death = rbinom(n, 1, .5)) %&gt;% # death randomly occurs at any time point. Obviously an unrealistic assumption. :)\n  group_by(pt_id) %&gt;%   # group by patient so we can do more data manipulation\n  complete(months_followup = full_seq(0:max(months_followup), 1)) %&gt;% # add i^n all the months patients are in the study\n  fill(pt_trt_prob, .direction = \"up\") %&gt;% # fill in the treatment probability I made earlier so I can use this to add treatment for every time point\n  ungroup() %&gt;% # no longer need patients grouped\n  mutate(trt = factor(rbinom(row_number(), 1, pt_trt_prob^2)),  # fill in treatment for everyone based on their treatment probability\n         death = replace_na(death, 0)) %&gt;%  # also fill in death\n  select(pt_id, months_followup, trt, death) # remove leftover columns from data generation\nLet’s look at the data. It’s in “long” format with patient ID’s repeating for each month_followup they were in my (fake) study. At every month, we know whether or not they were on the treatment, and whether they died at that time point. This first example does not contain any missingness.\nknitr::kable(head(dat))\n\n\n\n\npt_id\nmonths_followup\ntrt\ndeath\n\n\n\n\n1\n0\n1\n0\n\n\n1\n1\n1\n0\n\n\n1\n2\n1\n0\n\n\n1\n3\n1\n0\n\n\n1\n4\n1\n0\n\n\n1\n5\n1\n0\n\n\n\n\nWe can now plot our data in a very basic way. We really only need to specify in the aesthetics that the x-axis is time, the y-axis is subject IDs, the color should correspond to treatment, and our lines should be grouped together by subject. Don’t forget that last one! Then we can say we want geom_line to make a base timeline for each subject, and the points on the timeline should be squares (shape = 15 in the geom_point mapping function).\nEt voilà! A cute patient treatment timeline.\ndat %&gt;%\n  ggplot(aes(x = months_followup, y = pt_id, group = pt_id, col = trt)) +\n  geom_line()  + \n  geom_point(shape = 15)\n\n\n\n\n\n\n\nOkay, it’s not that cute. But that little bit of code is really the core of the plot! If you want to add markers of death (or any other event) and rearrange by length of follow up time it just takes a little bit of extra data manipulation and one additional line of ggplot2 code.\ndat %&gt;%\n  group_by(pt_id) %&gt;%\n  # make a new column with all patients last follow up visit\n  mutate(last_month_followup = max(months_followup), \n         # new variable for month that patients died, if they died\n         month_death = case_when(death == 1 ~ last_month_followup, TRUE ~ NA_real_)) %&gt;%\n         # reorder pt id by last month of follow up (highest to lowest)\n         # without fct_rev, chart is arranged in opposite direction (lowest to highest)\n  ungroup() %&gt;%\n  mutate(pt_id = fct_rev(fct_reorder(pt_id, last_month_followup))) %&gt;%\n  ggplot(aes(x = months_followup, y = pt_id, group = pt_id, col = trt)) +\n  geom_line()  + \n  geom_point(shape = 15) + \n  # add in a new mapping layer of points that correspond to death\n  geom_point(aes(x = month_death, y = pt_id), col = \"black\", shape = 4) \nWarning: Removed 515 rows containing missing values (geom_point).\n\n\n\n\n\n\n\nDon’t worry about the warning for removing missing values – it’s because we have NAs at most months for our month_death, so geom_point doesn’t know where to put an ‘x’ marker.\nFinally, let’s work on making the plot a bit nicer to look at:\ndat %&gt;%\n  group_by(pt_id) %&gt;%\n  mutate(last_month_followup = max(months_followup), \n         month_death = case_when(death == 1 ~ last_month_followup, TRUE ~ NA_real_)) %&gt;%\n  ungroup() %&gt;%\n  mutate(pt_id = fct_rev(fct_reorder(pt_id, last_month_followup)),\n         # make the treatment variable labels nicer\n         trt = factor(trt, levels=0:1, labels=c(\"No\",\"Yes\"))) %&gt;%\n  ggplot(aes(x = months_followup, y = pt_id, group = pt_id, col = trt)) +\n  geom_line()  + \n  geom_point(shape = 15) + \n  geom_point(aes(x = month_death, y = pt_id), col = \"black\", shape = 4) +\n  theme_bw() +\n  labs(x = \"Months of Follow-Up\", y = \"Patient ID\", col = \"Treatment\",\n       title = \"Patient Treatment Timeline\",\n       subtitle = \"x indicates month of patient death\") +\n  # edit legend box and make patient ids small\n  theme(axis.text.y = element_text(size=6),\n        legend.position = c(.6, .9), legend.direction = \"horizontal\",\n        legend.background = element_rect(linetype=\"solid\", colour =\"black\")) +\n  # remove extra space around timeline\n  scale_x_continuous(expand=c(0.01,0.01)) +\n  # set the color of the lines and points\n  scale_color_manual(values=c(\"dodgerblue\",\"firebrick1\"))\nWarning: Removed 515 rows containing missing values (geom_point).\n\n\n\n\n\n\n\nI have never made one of these plots for publication, so I don’t mind that the subtitle contains legend information. If you wanted to have the month_death markers in a legend, you could change:\ngeom_point(aes(x = month_death, y = pt_id), col = \"black\", shape = 4)\nto:\ngeom_point(aes(x = month_death, y = pt_id, shape = month_death), col = \"black\")\n\n\nExample 2: Continuous Covariate with Missingness\nWe can follow the very same steps for making a timeline for a continuous variable.\nAgain, here is the data generation code so you can make these plots yourself:\ndat_cc &lt;- \n  tibble(.rows = n) %&gt;%\n  mutate(pt_id = row_number(),\n         months_followup = round(runif(n, 0, 12)), # random months of follow up\n         pt_cov_mean = runif(n, 80, 150), # random mean of a patient's covariate measures\n         death = rbinom(n, 1, 0.5)) %&gt;% # random death\n  group_by(pt_id) %&gt;%\n  complete(months_followup = full_seq(0:max(months_followup), 1)) %&gt;%\n  fill(pt_cov_mean, death, .direction = \"up\") %&gt;%\n  mutate(last_month_followup = max(months_followup),\n         death_date = case_when(death == 1 ~ last_month_followup, TRUE ~ NA_real_)) %&gt;%\n  ungroup() %&gt;%\n  mutate(cov = rnorm(row_number(), pt_cov_mean, 10))  # everyone's covariates are close to their original mean (use to see patterns later)\nThis time I am incorporating missingness at a patient-level on my simulated covariate measurements to show how we can use the plots to look at potential patterns in missingness.\ndat_cc_samp &lt;- dat_cc %&gt;%\n  mutate(idx = row_number()) %&gt;%\n  sample_frac(.4, weight = pt_cov_mean^3)  %&gt;% # sample 40% of data, with weights for the sample determined by the patient's mean covariate. This would mean patients with a higher mean covariate measure are more likely to have missing data.\n  pull(idx)\n\ndat_cc_miss  &lt;- dat_cc %&gt;%\n  mutate(cov = case_when(row_number() %in% dat_cc_samp ~ NA_real_,\n                         TRUE ~ cov)) %&gt;%\n  select(pt_id, months_followup, cov, death)\nLooking again at the data we’ll be using:\nknitr::kable(head(dat_cc_miss))\n\n\n\n\npt_id\nmonths_followup\ncov\ndeath\n\n\n\n\n1\n0\n91.69544\n0\n\n\n2\n0\nNA\n1\n\n\n2\n1\nNA\n1\n\n\n2\n2\n156.57317\n1\n\n\n2\n3\nNA\n1\n\n\n2\n4\n172.23156\n1\n\n\n\n\nWe can plot the data and see this missingness with gray timeline points and lines:\ndat_cc_miss %&gt;%\n  group_by(pt_id) %&gt;%\n  mutate(last_month_followup = max(months_followup), \n         month_death = case_when(death == 1 ~ last_month_followup, TRUE ~ NA_real_)) %&gt;%\n  ungroup() %&gt;%\n  mutate(pt_id = fct_rev(fct_reorder(factor(pt_id), last_month_followup))) %&gt;%\n  ggplot(aes(x = months_followup, y = pt_id,\n             group = pt_id, col = cov)) +\n  geom_line()  + \n  geom_point(shape = 15) + \n  geom_point(aes(x = month_death, y = pt_id), shape=4, col=\"black\") +\n  theme_bw() +\n  labs(x = \"Months of Follow-Up\", y = \"Patient ID\", col = \"Covariate\",\n       title = \"Patient Timelines: Continuous Covariate\",\n       subtitle = \"x indicates month of patient death, gray indicates missing covariate\") +\n  theme(axis.text.y = element_text(size=6),\n        legend.position = c(.7, .9),\n        legend.direction = \"horizontal\",\n        legend.background = element_rect(linetype=\"solid\", colour =\"black\")) +\n  scale_x_continuous(expand=c(0.01,0.01)) +\n  scale_color_gradient(low=\"dodgerblue\", high=\"firebrick1\", na.value = \"lightgray\",\n                       breaks=c(90, 120, 150, 180)) \nWarning: Removed 143 rows containing missing values (geom_point).\n\n\n\n\n\n\n\nAlternatively, we could rearrange our timelines by the number of measures we have for each patient’s covariate of interest by making a new column containing the sum(!is.na(cov)) and rearranging our pt_id by that column.\ndat_cc_miss %&gt;%\n  group_by(pt_id) %&gt;%\n  mutate(last_month_followup = max(months_followup),\n         # a column containing how many measures we have for each patient\n         n_measures = sum(!is.na(cov))) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    # reorder IDs by number of measures we have for each patient\n    pt_id = fct_rev(fct_reorder(factor(pt_id), n_measures)),\n  month_death = case_when(death == 1 ~ last_month_followup, TRUE ~ NA_real_)) %&gt;%\n  ggplot(aes(x = months_followup, y = pt_id,\n             group = pt_id, col = cov)) +\n  geom_line()  + \n  geom_point(shape = 15) + \n  geom_point(aes(x = month_death, y = pt_id), shape=4, col=\"black\") +\n  theme_bw() +\n  labs(x = \"Months of Follow-Up\", y = \"Patient ID\", col = \"Covariate\",\n       title = \"Patient Timelines: Continuous Covariate\",\n       subtitle = \"x indicates month of patient death, gray indicates missing covariate\") +\n  theme(axis.text.y = element_text(size=6),\n        legend.position = c(.7, .9),\n        legend.direction = \"horizontal\",\n        legend.background = element_rect(linetype=\"solid\", colour =\"black\")) +\n  scale_x_continuous(expand=c(0.01,0.01)) +\n  scale_color_gradient(low=\"dodgerblue\",high=\"firebrick1\",na.value = \"lightgray\",\n                       breaks=c(90, 120, 150, 180)) \nWarning: Removed 143 rows containing missing values (geom_point).\n\n\n\n\n\n\n\nFrom this plot we can start to see that patients with more missing measurements have higher covariate measurements. This becomes clearer if we rearrange patients by their proportion of non-missing measurements.\ndat_cc_miss %&gt;%\n  group_by(pt_id) %&gt;%\n  mutate(last_month_followup = max(months_followup),\n         # the proportion is the total number of measures divided by the total months of followup\n         prop_measures = sum(!is.na(cov))/last_month_followup) %&gt;%\n  ungroup() %&gt;%\n  # reorder IDs by proportion of measures we have for each patient\n  mutate(pt_id = fct_rev(fct_reorder(factor(pt_id), prop_measures)),\n  month_death = case_when(death == 1 ~ last_month_followup, TRUE ~ NA_real_)) %&gt;%\n  ggplot(aes(x = months_followup, y = pt_id,\n             group = pt_id, col = cov)) +\n  geom_line()  + \n  geom_point(shape = 15) + \n  geom_point(aes(x = month_death, y = pt_id), shape=4, col=\"black\") +\n  theme_bw() +\n  labs(x = \"Months of Follow-Up\", y = \"Patient ID\", col = \"Covariate\",\n       title = \"Patient Timelines: Continuous Covariate\",\n       subtitle = \"x indicates month of patient death, gray indicates missing covariate\") +\n  theme(axis.text.y = element_text(size=6), # move legend to the side by removing legend location\n        legend.background = element_rect(linetype=\"solid\", colour =\"black\")) +\n  scale_x_continuous(expand=c(0.01,0.01)) +\n  scale_color_gradient(low=\"dodgerblue\",high=\"firebrick1\",na.value = \"lightgray\",\n                       breaks=c(90, 120, 150, 180)) \nWarning: Removed 143 rows containing missing values (geom_point).\n\n\n\n\n\n\n\nIf this were my real data, I would follow up with my research collaborators for more information on the missingness mechanism.\nPlease let me know if you have questions or a suggestion for a data set. Happy treatment timeline plotting!\n\n\nSession info\nsessionInfo()\nR version 4.1.3 (2022-03-10) Platform: x86_64-apple-darwin17.0 (64-bit) Running under: macOS Catalina 10.15.7\nMatrix products: default BLAS: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\nlocale: [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\nattached base packages: [1] stats graphics grDevices utils datasets methods base\nother attached packages: [1] forcats_0.5.1 stringr_1.4.1 dplyr_1.0.9 purrr_0.3.4\n[5] readr_2.1.2 tidyr_1.2.0 tibble_3.1.8 ggplot2_3.3.6\n[9] tidyverse_1.3.1\nloaded via a namespace (and not attached): [1] tidyselect_1.1.2 xfun_0.32 haven_2.5.0 colorspace_2.0-3 [5] vctrs_0.4.1 generics_0.1.3 htmltools_0.5.2 yaml_2.3.5\n[9] utf8_1.2.2 rlang_1.0.4 pillar_1.8.1 glue_1.6.2\n[13] withr_2.5.0 DBI_1.1.2 dbplyr_2.1.1 modelr_0.1.8\n[17] readxl_1.4.0 lifecycle_1.0.1 munsell_0.5.0 gtable_0.3.0\n[21] cellranger_1.1.0 rvest_1.0.2 htmlwidgets_1.5.4 evaluate_0.15\n[25] labeling_0.4.2 knitr_1.38 tzdb_0.3.0 fastmap_1.1.0\n[29] cabinets_0.6.0 fansi_1.0.3 highr_0.9 broom_0.8.0\n[33] backports_1.4.1 scales_1.2.1 jsonlite_1.8.0 farver_2.1.1\n[37] fs_1.5.2 gridExtra_2.3 hms_1.1.1 digest_0.6.29\n[41] stringi_1.7.8 grid_4.1.3 cli_3.3.0 tools_4.1.3\n[45] magrittr_2.0.3 crayon_1.5.1 pkgconfig_2.0.3 ellipsis_0.3.2\n[49] xml2_1.3.3 reprex_2.0.1 lubridate_1.8.0 assertthat_0.2.1 [53] rmarkdown_2.13 httr_1.4.2 rstudioapi_0.13 R6_2.5.1\n[57] compiler_4.1.3"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Prior to studying biostatistics, I studied biochemistry and took pre-medicine courses at the University of Detroit Mercy. During the summers, I cared for hospitalized patients as a nurse technician in Michigan and worked at a children’s health clinic in rural Nicaragua. These experiences, combined with a love for mathematics, have made me passionate about improving the state of medical and public health research through education in statistics.\nI received my MS in Biostatistics from the University of Michigan in 2018, and I worked as a biostatistician in academic medical research in New York City from 2018-2023. In Fall 2023, I began the PhD program in Biostatistics at the University of Washington. For the first two years, I took coursework in statistical theory, and I am currently beginning dissertation work with a focus on causal inference methods with infectious disease applications.\n\nI created this website in 2019 to store educational materials I’ve created on career, programming, and statistics topics. I am always open to hearing feedback on blog posts and illustrations, and I answer questions via email whenever I find time."
  },
  {
    "objectID": "pubs/pubs_applications.html",
    "href": "pubs/pubs_applications.html",
    "title": "Clinical Applications",
    "section": "",
    "text": "The majority of my research collaborations to date are with researchers on the Pulmonary and Critical Care Medicine team at Weill Cornell Medicine (WCM). Other long-term research experiences includes applications within neurology (cognitive decline/brain volumes), neuroepigenetics (effects of sugar consumption), canine genomics, and associations with air pollution and kidney disease.\nThis is a selected list of clinical application manuscripts I’ve co-authored. A full list of published manuscripts and papers-under-review is available on my CV.\n*Indicates authors contributed equally to the work.\n\n\n\n\n\n\n\nSelected Clinical Application Publications\n\n1. Katherine L. Hoffman, Floriana Milazzo, Nicholas T. Williams, Hillary Samples, Mark Olfson, Ivan Diaz, Lisa Doan, Magdalena Cerda, Stephen Crystal, Kara E. Rudolph. Independent and joint contributions of physical disability and chronic pain to incident opioid use disorder and opioid overdose among medicaid patients. Psychological Medicine. Published online 2023:1-12. doi:10.1017/S003329172300332X\n\n\n2. Edward J. *Schenck, Katherine L. *Hoffman, Clara Oromendia, Elizabeth Sanchez, Eli J. Finkelsztein, Kyung Sook Hong, Joseph Kabariti, Lisa K. Torres, John S. Harrington, Ilias I. Siempos, Augustine M. K. Choi, Thomas R. Campion. A Comparative Analysis of the Respiratory Subscore of the Sequential Organ Failure Assessment Scoring System. Annals of the American Thoracic Society. 2021;18(11):1849-1860. doi:10.1513/AnnalsATS.202004-399OC\n\n\n3. Luis G. *Gómez-Escobar, Katherine L. *Hoffman, Justin J. Choi, Alain Borczuk, Steven Salvatore, Sergio L. Alvarez-Mulett, Manuel D. Galvan, Zhen Zhao, Sabrina E. Racine-Brzostek, He S. Yang, Heather W. Stout-Delgado, Mary E. Choi, Augustine M. K. Choi, Soo Jung Cho, Edward J. Schenck. Cytokine signatures of end organ injury in COVID-19. Scientific Reports. 2021;11(1):12606. doi:10.1038/s41598-021-91859-z\n\n\n4. Edward J. Schenck, Katherine Hoffman, Parag Goyal, Justin Choi, Lisa Torres, Kapil Rajwani, Christopher W. Tam, Natalia Ivascu, Fernando J. Martinez, David A. Berlin. Respiratory Mechanics and Gas Exchange in COVID-19–associated Respiratory Failure. Annals of the American Thoracic Society. 2020;17(9):1158-1161. doi:10.1513/AnnalsATS.202005-427RL\n\n\n5. Lisa K. Torres, Katherine L. Hoffman, Clara Oromendia, Ivan Diaz, John S. Harrington, Edward J. Schenck, David R. Price, Luis Gomez-Escobar, Angelica Higuera, Mayra Pinilla Vera, Rebecca M. Baron, Laura E. Fredenburgh, Jin-Won Huh, Augustine M. K. Choi, Ilias I. Siempos. Attributable mortality of acute respiratory distress syndrome: A systematic review, meta-analysis and survival analysis using targeted minimum loss-based estimation. Thorax. 2021;76(12):1176-1185. doi:10.1136/thoraxjnl-2020-215950\n\n\n6. David R. Price, Katherine L. Hoffman, Clara Oromendia, Lisa K. Torres, Edward J. Schenck, Mary E. Choi, Augustine M. K. Choi, Rebecca M. Baron, Jin-Won Huh, Ilias I. Siempos. Effect of Neutropenic Critical Illness on Development and Prognosis of Acute Respiratory Distress Syndrome. American Journal of Respiratory and Critical Care Medicine. 2021;203(4):504-508. doi:10.1164/rccm.202003-0753LE\n\n\n7. David R. Price, Katherine L. Hoffman, Elizabeth Sanchez, Augustine M. K. Choi, Ilias I. Siempos. Temporal trends of outcomes of neutropenic patients with ARDS enrolled in therapeutic clinical trials. Intensive Care Medicine. 2021;47(1):122-123. doi:10.1007/s00134-020-06263-4\n\n\n8. William Z. Zhang, Katherine L. Hoffman, Kristen T. Schiffer, Clara Oromendia, Michelle C. Rice, Igor Barjaktarevic, Stephen P. Peters, Nirupama Putcha, Russell P. Bowler, J. Michael Wells, David J. Couper, Wassim W. Labaki, Jeffrey L. Curtis, Meilan K. Han, Robert Paine, Prescott G. Woodruff, Gerard J. Criner, Nadia N. Hansel, Ivan Diaz, Karla V. Ballman, Kiichi Nakahira, Mary E. Choi, Fernando J. Martinez, Augustine M. K. Choi, Suzanne M. Cloonan. Association of plasma mitochondrial DNA with COPD severity and progression in the SPIROMICS cohort. Respiratory Research. 2021;22(1):126. doi:10.1186/s12931-021-01707-x\n\n\n9. Edward J. Schenck, Katherine L. Hoffman, Marika Cusick, Joseph Kabariti, Evan T. Sholle, Thomas R. Campion. Critical carE Database for Advanced Research (CEDAR): An automated method to support intensive care units with electronic health record data. Journal of Biomedical Informatics. 2021;118:103789. doi:10.1016/j.jbi.2021.103789\n\n\n10. Chang Su, Katherine L. Hoffman, Zhenxing Xu, Elizabeth Sanchez, Ilias I. Siempos, John S. Harrington, Alexandra C. Racanelli, Maria Plataki, Fei Wang, Edward J. Schenck. Evaluation of Albumin Kinetics in Critically Ill Patients With Coronavirus Disease 2019 Compared to Those With Sepsis-Induced Acute Respiratory Distress Syndrome. Critical Care Explorations. 2021;3(12):e0589. doi:10.1097/CCE.0000000000000589\n\n\n11. William Z. Zhang, Michelle C. Rice, Katherine L. Hoffman, Clara Oromendia, Igor Z. Barjaktarevic, J. Michael Wells, Annette T. Hastie, Wassim W. Labaki, Christopher B. Cooper, Alejandro P. Comellas, Gerard J. Criner, Jerry A. Krishnan, Robert Paine, Nadia N. Hansel, Russell P. Bowler, R. Graham Barr, Stephen P. Peters, Prescott G. Woodruff, Jeffrey L. Curtis, Meilan K. Han, Karla V. Ballman, Fernando J. Martinez, Augustine Mk Choi, Kiichi Nakahira, Suzanne M. Cloonan, Mary E. Choi, SPIROMICS Investigators. Association of urine mitochondrial DNA with clinical measures of COPD in the SPIROMICS cohort. JCI insight. 2020;5(3):133984. doi:10.1172/jci.insight.133984\n\n\n12. Aneela Rahman, Eva Schelbaum, Katherine Hoffman, Ivan Diaz, Hollie Hristov, Randolph Andrews, Steven Jett, Hande Jackson, Andrea Lee, Harini Sarva, Silky Pahlajani, Dawn Matthews, Jonathan Dyke, Mony J. de Leon, Richard S. Isaacson, Roberta D. Brinton, Lisa Mosconi. Sex-driven modifiers of Alzheimer risk: A multimodality brain imaging study. Neurology. 2020;95(2):e166-e178. doi:10.1212/WNL.0000000000009781\n\n\n13. David R. *Price, Elisa *Benedetti, Katherine L. Hoffman, Luis Gomez-Escobar, Sergio Alvarez-Mulett, Allyson Capili, Hina Sarwath, Christopher N. Parkhurst, Elyse Lafond, Karissa Weidman, Arjun Ravishankar, Jin Gyu Cheong, Richa Batra, Mustafa Büyüközkan, Kelsey Chetnik, Imaani Easthausen, Edward J. Schenck, Alexandra C. Racanelli, Hasina Outtz Reed, Jeffrey Laurence, Steven Z. Josefowicz, Lindsay Lief, Mary E. Choi, Frank Schmidt, Alain C. Borczuk, Augustine M. K. Choi, Jan Krumsiek, Shahin Rafii. Angiopoietin 2 Is Associated with Vascular Necroptosis Induction in Coronavirus Disease 2019 Acute Respiratory Distress Syndrome. The American Journal of Pathology. 2022;192(7):1001-1015. doi:10.1016/j.ajpath.2022.04.002",
    "crumbs": [
      "Clinical Applications"
    ]
  }
]