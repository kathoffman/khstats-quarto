<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Katherine Hoffman">
<meta name="dcterms.date" content="2020-12-12">

<title>KHstats - An Illustrated Guide to TMLE, Part III: Properties, Theory, and Learning More</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-136820093-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">KHstats</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../talks.html">
 <span class="menu-text">Talks</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-illustrations" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Illustrations</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-illustrations">    
        <li>
    <a class="dropdown-item" href="../../art/illustrations_viz.html">
 <span class="dropdown-text">Visual Guides for Causal Inference</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../art/illustrations_draw.html">
 <span class="dropdown-text">Educational Drawings and Comics</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-research" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Research</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-research">    
        <li>
    <a class="dropdown-item" href="../../pubs/pubs_methods.html">
 <span class="dropdown-text">Statistics Methods</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pubs/pubs_applications.html">
 <span class="dropdown-text">Clinical Applications</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://scholar.google.com/citations?user=73RvTUoAAAAJ&amp;hl=en">
 <span class="dropdown-text">Google Scholar Profile</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/kat_hoffman_"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kathoffman"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../static/KatherineHoffman_CV_Dec2022.pdf">
 <span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:kathoffman.stats@gmail.com"><i class="bi bi-envelope" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">An Illustrated Guide to TMLE, Part III: Properties, Theory, and Learning More</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">statistics</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Katherine Hoffman </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 12, 2020</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#properties-of-tmle" id="toc-properties-of-tmle" class="nav-link active" data-scroll-target="#properties-of-tmle">Properties of TMLE üìà</a></li>
  <li><a href="#why-does-tmle-work" id="toc-why-does-tmle-work" class="nav-link" data-scroll-target="#why-does-tmle-work">Why does TMLE work? ‚ú®</a></li>
  <li><a href="#resources-to-learn-more" id="toc-resources-to-learn-more" class="nav-link" data-scroll-target="#resources-to-learn-more">Resources to learn more</a>
  <ul class="collapse">
  <li><a href="#tmle" id="toc-tmle" class="nav-link" data-scroll-target="#tmle">TMLE</a></li>
  <li><a href="#semiparametric-theory-and-influence-functions" id="toc-semiparametric-theory-and-influence-functions" class="nav-link" data-scroll-target="#semiparametric-theory-and-influence-functions">Semiparametric Theory and Influence Functions</a></li>
  <li><a href="#causal-inference" id="toc-causal-inference" class="nav-link" data-scroll-target="#causal-inference">Causal Inference</a></li>
  </ul></li>
  <li><a href="#acknowledgements" id="toc-acknowledgements" class="nav-link" data-scroll-target="#acknowledgements">Acknowledgements</a></li>
  </ul>
</nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<blockquote class="blockquote">
<p>The is the third and final post in a three-part series to help beginners and/or visual learners understand Targeted Maximum Likelihood Estimation (TMLE). In this section, I discuss more <a href="#properties-of-tmle"><strong>statistical properties of TMLE</strong></a>, offer a brief <a href="#why-does-tmle-work"><strong>explanation for the theory behind TMLE</strong></a>, and provide <a href="#resources-to-learn-more"><strong>resources for learning more</strong></a>.</p>
</blockquote>
<hr>
<section id="properties-of-tmle" class="level1">
<h1>Properties of TMLE üìà</h1>
<p>To reiterate a point from <em>Parts I</em> and <em>II</em>, a main motivation for TMLE is that it <strong>allows the use of machine learning algorithms while still yielding asymptotic properties for inference</strong>. This is notably <em>not</em> true for many estimators.</p>
<p>For example, in <a href="https://khstats.com/blog/tmle/tutorial-pt2"><em>Part II</em></a> we walked through TMLE for the Average Treatment Effect (ATE). Two frequently used alternatives to estimating the ATE are G-computation and Inverse Probability of Treatment Weighting (see <a href="https://khstats.com/blog/tmle/tutorial-pt2/#step-1-estimate-the-outcome"><em>Part II, Step 1</em></a> and <a href="#resources-to-learn-more"><em>references</em></a>). In general, neither yield valid standard errors unless <em>a-priori</em> specified parametric models are used, and this reliance on parametric assumptions can bias results. There are many simulation studies that show this.</p>
<p>Another beneficial property of TMLE is that it is a <strong>doubly robust</strong> estimator. This means that if either the regression to estimate the expected outcome, or the regression to estimate the probability of treatment, are correctly specified (formally, their bias goes to zero as sample size grows large, meaning they are <strong>consistent</strong>), the final TMLE estimate will be consistent.</p>
<p>If both regressions are consistent, the <strong>final estimate will reach the smallest possible variance at a rate of</strong> <span class="math inline">\(\sqrt{n}\)</span>, which is the fastest possible rate of convergence and equivalent to parametric maximum likelihood estimation. The reason we use superlearning for estimating the outcome and treatment regressions is to give us the best possible chance of having two correctly specified models and obtaining an <strong>efficient estimate</strong>.</p>
<p><img src="tmle_props.png" class="img-fluid"></p>
<p>Even among other doubly robust estimators, TMLE is appealing because its estimates will always stay within the bounds of the original outcome. This is because it is part of a class of <strong>substitution estimators</strong>. There is another class of doubly robust, semiparametric estimation methods frequently used in causal inference that are referred to as <strong>one-step estimators</strong>, but they can sometimes yield final estimates that are outside the original outcome scale. The one-step estimator for the ATE is called <strong>Augmented Inverse Probability Weighting (AIPW)</strong>.</p>
</section>
<section id="why-does-tmle-work" class="level1">
<h1>Why does TMLE work? ‚ú®</h1>
<p>Truly understanding why TMLE works requires semiparametric theory that falls far outside the scope of this tutorial. However, the theory is interesting, so I‚Äôll give a brief, high-level explanation, and then you can look at the <a href="#resources-to-learn-more">references</a> if you‚Äôre curious to learn more. Importantly, the explanation I outline here is more than sufficient and certainly not necessary to appropriately implement TMLE as an analyst.</p>
<p><strong>TMLE relies on the following ideas:</strong></p>
<ol type="1">
<li><p>Some estimands allow for <strong>asymptotically linear estimation</strong>. This means that estimators can be represented as sample averages (plus a term that converges to zero).</p></li>
<li><p>The quantities being averaged for asymptotically linear estimators are called <strong>influence functions</strong>. An influence function is a function that quantifies how much influence each observation has on the estimator. For this reason, it is very <strong>useful to characterize the variance of the estimator</strong>. In parametric maximum likelihood estimation, the influence function is related the score function.</p></li>
<li><p>The <strong>efficient influence function</strong> (EIF) is the influence function that achieves the efficiency bound (think Cramer Rao Lower Bound from parametric maximum likelihood estimation) and <strong>can be used to create efficient estimators.</strong></p></li>
<li><p>If we want to <strong>construct an estimator that is efficient</strong>, we can take advantage of the EIF to endow the estimator with useful asymptotic properties.</p></li>
</ol>
<p>This is the reason TMLE allows us to use machine learning models ‚Äúunder the hood‚Äù while still obtaining asymptotic properties for inference: our <strong>estimand</strong> of interest admits <strong>asymptotically linear estimation</strong>, and we are <strong>using properties of the EIF</strong> to <strong>construct an estimator</strong> with <strong>optimal statistical properties</strong> (e.g.&nbsp;double robustness).</p>
</section>
<section id="resources-to-learn-more" class="level1">
<h1>Resources to learn more</h1>
<p>I could only cover so much in this post, but here are the resources I‚Äôve used the most to learn about TMLE, semiparametric estimation, and causal inference. If you are new to any or all of it, there is a good chance it will take <em>several</em> reads of these materials before the concepts begin to make any sense. Don‚Äôt get discouraged!</p>
<section id="tmle" class="level3">
<h3 class="anchored" data-anchor-id="tmle">TMLE</h3>
<ul>
<li><p>The paper I referred to most often while learning TMLE was <a href="https://academic.oup.com/aje/article/185/1/65/2662306"><em>Targeted Maximum Likelihood Estimation for Causal Inference in Observational Studies</em></a> by Megan S. Schuler and Sherri Rose. It has a nice step-by-step written explanation and details the statistical advantages of TMLE for an applied thinker.</p></li>
<li><p>I also really like the written explanations in the <a href="https://link.springer.com/book/10.1007/978-1-4419-9782-1"><em>Targeted Learning</em></a> book (Chapters 4 and 5) by Mark van der Laan and Sherri Rose. The notation was often too difficult for me to follow, but the words themselves make a lot of sense.</p></li>
<li><p>Miguel Angel Luque-Fernandez wrote an <a href="https://migariane.github.io/TMLE.nb.html">excellent bookdown tutorial on TMLE</a>, also with step-by-step <code>R</code> code. It is more technical and thorough than my post, but still aimed at an applied audience. He also has a tutorial on the <a href="https://migariane.github.io/DeltaMethodEpiTutorial.nb.html">functional delta method</a> which is part of the theory behind the way we compute the standard errors (see <a href="https://khstats.com/blog/tmle/tutorial-pt2/#step-6-calculate-the-standard-errors-for-confidence-intervals-and-p-values"><em>Part II, Step 6</em></a>).</p></li>
<li><p>Other code-based web-based tutorials on TMLE that are more recent (or recently discovered by me!) include:</p>
<ul>
<li><p>David Benkeser and Antoine Chambaz‚Äô <a href="https://achambaz.github.io/tlride/">A Ride in Targeted Learning Territory</a></p></li>
<li><p>The authors of the <code>R</code> package suite <a href="https://tlverse.org/"><code>tlverse</code></a>‚Äôs <a href="https://tlverse.org/tlverse-handbook/tmle3.html">Hitchhiker‚Äôs Guide to Targeted Learning: The TMLE Framework</a>.</p></li>
<li><p>Stitch Fix‚Äôs Jasmine Nettiksimmons AND Molly Davies‚Äô blog post: <a href="https://multithreaded.stitchfix.com/blog/2021/07/23/double-robust-estimator/">‚ÄúGimme a robust estimator - and make it a double!‚Äù</a></p></li>
</ul></li>
</ul>
</section>
<section id="semiparametric-theory-and-influence-functions" class="level3">
<h3 class="anchored" data-anchor-id="semiparametric-theory-and-influence-functions">Semiparametric Theory and Influence Functions</h3>
<ul>
<li><p>Edward Kennedy has several well-written pieces on semiparametric estimation in causal inference. I recommend starting with:</p>
<ul>
<li><p>His introductory paper on <a href="https://arxiv.org/pdf/1709.06418.pdf">Semiparametric Theory</a></p></li>
<li><p>His <a href="https://www.ehkennedy.com/uploads/5/8/4/5/58450265/tutorial.pdf">slideshow tutorial</a> <em>Nonparametric efficiency theory and machine learning in causal inference</em></p></li>
</ul></li>
<li><p>My favorite resource so far for learning specifically about influence functions has been <a href="https://www.tandfonline.com/doi/full/10.1080/00031305.2020.1717620">Visually Communicating Influence Functions</a> by Aaron Fisher and Edward Kennedy. However, this paper didn‚Äôt make sense to me until I worked through this <a href="https://observablehq.com/@herbps10/one-step-estimators-and-pathwise-derivatives">interactive tutorial</a> by Herb Susmann. I suggest playing around with the interactive examples first, and then trying to work through the paper.</p></li>
<li><p>Additonal walkthroughs to learn about EIFs include the following:</p>
<ul>
<li><p>Alejandro Schuler‚Äôs <a href="https://alejandroschuler.github.io/mci/index.html">Modern Causal Inference online book</a> which includes <a href="https://alejandroschuler.github.io/mci/0cb2ffe5e5cc4cf59a5fe6d896d221d1.html">a section on deriving EIFs</a>. His tutorial is a similar, shorter version of the next two resources:</p></li>
<li><p>Oliver Hines et al.‚Äôs <a href="https://arxiv.org/abs/2107.00681">Demystifying statistical learning based on efficient influence functions</a></p></li>
<li><p>Edward Kennedy‚Äôs comprehensive review <a href="https://arxiv.org/pdf/2203.06469.pdf">Semiparametric Doubly Robust Targeted Double Machine Learning: A Review</a></p></li>
</ul></li>
</ul>
<p>Although I did not personally use these last few when I was initially learning about EIFs (since they came out after I wrote this post in Fall 2020), they seem like great resources and intended to be approachable for beginner-to-intermediate learners.</p>
<!-- - The derivation of the Efficient Influence Function (EIF) in TMLE is in the Appendix of [*Targeted Learning*](https://link.springer.com/book/10.1007/978-1-4419-9782-1). -->
</section>
<section id="causal-inference" class="level3">
<h3 class="anchored" data-anchor-id="causal-inference">Causal Inference</h3>
<ul>
<li><p>As emphasized in <em>Part I</em>, TMLE is an estimation technique which <em>can</em> be used for causal inference. If you want to learn about the foundations of causal inference, I suggest two different introductory texts below. Note that these provide fairly different frameworks (notation, descriptions of assumptions) to reach the same conclusions, but both provide useful perspectives.</p>
<ul>
<li><p><a href="http://bayes.cs.ucla.edu/PRIMER/"><em>Causal Inference in Statistics: A Primer</em></a> by Judea Pearl. Pearl does not discuss estimation methods, but rather focuses on the assumptions, or identification, side of causal inference. Thus, you will not find TMLE mentioned in this text.</p></li>
<li><p><a href="https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/"><em>What If</em></a> by Miguel Hernan and James Robins. Notably, Hernan and Robins only discuss parametric estimation methods, so you will also not find TMLE or AIPW in this text.</p></li>
</ul></li>
<li><p>I also think the introductory chapters of the previously mentioned <a href="https://link.springer.com/book/10.1007/978-1-4419-9782-1"><em>Targeted Learning</em></a> book (Chapters 1 and 2) do an excellent job of setting up the ‚Äúroadmap‚Äù of causal inference.</p></li>
</ul>
<p>I‚Äôll continue to update this page with beginner‚Äôs resources as I discover them.</p>
<p>Feedback or clarifications on this post is welcome, either from the new learners of TMLE or experts in causal inference. The best way to reach me is through <a href="mailto:kathoffman.stats@gmail.com">email</a>.</p>
</section>
</section>
<section id="acknowledgements" class="level1">
<h1>Acknowledgements</h1>
<p>This tutorial would not have been possible without my colleague Iv√°n D√≠az patiently answering many, many questions on TMLE. I am also very appreciative of Miguel Angel Luque-Fernandez‚Äôs helpful feedback on the visual guide.</p>
<p>Lastly, many thanks to Axel Martin, Nick Williams, Anjile An, Adam Peterson, Alan Wu, and Will Simmons for providing suggestions on various drafts of this art project!</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp(/www\.khstats\.com/);
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
    var links = window.document.querySelectorAll('a:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "An Illustrated Guide to TMLE, Part III: Properties, Theory, and Learning More"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Katherine Hoffman"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> tmle_props.png</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">page-layout:</span><span class="co"> article</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> " "</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 2020-12-12</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> ["statistics"]</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="an">output:</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">  blogdown::html_page:</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: false</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">    toc_depth: 1</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="in">```{r setup, include=FALSE}</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span>opts_chunk<span class="sc">$</span><span class="fu">set</span>(<span class="at">echo =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The is the third and final post in a three-part series to help beginners and/or visual learners understand Targeted Maximum Likelihood Estimation (TMLE). In this section, I discuss more </span><span class="co">[</span><span class="ot">**statistical properties of TMLE**</span><span class="co">](#properties-of-tmle)</span><span class="at">, offer a brief </span><span class="co">[</span><span class="ot">**explanation for the theory behind TMLE**</span><span class="co">](#why-does-tmle-work)</span><span class="at">, and provide </span><span class="co">[</span><span class="ot">**resources for learning more**</span><span class="co">](#resources-to-learn-more)</span><span class="at">.</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="fu"># Properties of TMLE üìà {#properties-of-tmle}</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>To reiterate a point from *Parts I* and *II*, a main motivation for TMLE is that it **allows the use of machine learning algorithms while still yielding asymptotic properties for inference**. This is notably *not* true for many estimators.</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>For example, in <span class="co">[</span><span class="ot">*Part II*</span><span class="co">](https://khstats.com/blog/tmle/tutorial-pt2)</span> we walked through TMLE for the Average Treatment Effect (ATE). Two frequently used alternatives to estimating the ATE are G-computation and Inverse Probability of Treatment Weighting (see <span class="co">[</span><span class="ot">*Part II, Step 1*</span><span class="co">](https://khstats.com/blog/tmle/tutorial-pt2/#step-1-estimate-the-outcome)</span> and <span class="co">[</span><span class="ot">*references*</span><span class="co">](#resources-to-learn-more)</span>). In general, neither yield valid standard errors unless *a-priori* specified parametric models are used, and this reliance on parametric assumptions can bias results. There are many simulation studies that show this.</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>Another beneficial property of TMLE is that it is a **doubly robust** estimator. This means that if either the regression to estimate the expected outcome, or the regression to estimate the probability of treatment, are correctly specified (formally, their bias goes to zero as sample size grows large, meaning they are **consistent**), the final TMLE estimate will be consistent.</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>If both regressions are consistent, the **final estimate will reach the smallest possible variance at a rate of** $\sqrt{n}$, which is the fastest possible rate of convergence and equivalent to parametric maximum likelihood estimation. The reason we use superlearning for estimating the outcome and treatment regressions is to give us the best possible chance of having two correctly specified models and obtaining an **efficient estimate**.</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="al">![](tmle_props.png)</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>Even among other doubly robust estimators, TMLE is appealing because its estimates will always stay within the bounds of the original outcome. This is because it is part of a class of **substitution estimators**. There is another class of doubly robust, semiparametric estimation methods frequently used in causal inference that are referred to as **one-step estimators**, but they can sometimes yield final estimates that are outside the original outcome scale. The one-step estimator for the ATE is called **Augmented Inverse Probability Weighting (AIPW)**.</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="fu"># Why does TMLE work? ‚ú® {#why-does-tmle-work}</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>Truly understanding why TMLE works requires semiparametric theory that falls far outside the scope of this tutorial. However, the theory is interesting, so I'll give a brief, high-level explanation, and then you can look at the <span class="co">[</span><span class="ot">references</span><span class="co">](#resources-to-learn-more)</span> if you're curious to learn more. Importantly, the explanation I outline here is more than sufficient and certainly not necessary to appropriately implement TMLE as an analyst.</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>**TMLE relies on the following ideas:**</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Some estimands allow for **asymptotically linear estimation**. This means that estimators can be represented as sample averages (plus a term that converges to zero).</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>The quantities being averaged for asymptotically linear estimators are called **influence functions**. An influence function is a function that quantifies how much influence each observation has on the estimator. For this reason, it is very **useful to characterize the variance of the estimator**. In parametric maximum likelihood estimation, the influence function is related the score function.</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>The **efficient influence function** (EIF) is the influence function that achieves the efficiency bound (think Cramer Rao Lower Bound from parametric maximum likelihood estimation) and **can be used to create efficient estimators.**</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>If we want to **construct an estimator that is efficient**, we can take advantage of the EIF to endow the estimator with useful asymptotic properties.</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>This is the reason TMLE allows us to use machine learning models "under the hood" while still obtaining asymptotic properties for inference: our **estimand** of interest admits **asymptotically linear estimation**, and we are **using properties of the EIF** to **construct an estimator** with **optimal statistical properties** (e.g. double robustness).</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="fu"># Resources to learn more {#resources-to-learn-more}</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>I could only cover so much in this post, but here are the resources I've used the most to learn about TMLE, semiparametric estimation, and causal inference. If you are new to any or all of it, there is a good chance it will take *several* reads of these materials before the concepts begin to make any sense. Don't get discouraged!</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a><span class="fu">### TMLE</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The paper I referred to most often while learning TMLE was <span class="co">[</span><span class="ot">*Targeted Maximum Likelihood Estimation for Causal Inference in Observational Studies*</span><span class="co">](https://academic.oup.com/aje/article/185/1/65/2662306)</span> by Megan S. Schuler and Sherri Rose. It has a nice step-by-step written explanation and details the statistical advantages of TMLE for an applied thinker.</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>I also really like the written explanations in the <span class="co">[</span><span class="ot">*Targeted Learning*</span><span class="co">](https://link.springer.com/book/10.1007/978-1-4419-9782-1)</span> book (Chapters 4 and 5) by Mark van der Laan and Sherri Rose. The notation was often too difficult for me to follow, but the words themselves make a lot of sense.</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Miguel Angel Luque-Fernandez wrote an <span class="co">[</span><span class="ot">excellent bookdown tutorial on TMLE</span><span class="co">](https://migariane.github.io/TMLE.nb.html)</span>, also with step-by-step <span class="in">`R`</span> code. It is more technical and thorough than my post, but still aimed at an applied audience. He also has a tutorial on the <span class="co">[</span><span class="ot">functional delta method</span><span class="co">](https://migariane.github.io/DeltaMethodEpiTutorial.nb.html)</span> which is part of the theory behind the way we compute the standard errors (see <span class="co">[</span><span class="ot">*Part II, Step 6*</span><span class="co">](https://khstats.com/blog/tmle/tutorial-pt2/#step-6-calculate-the-standard-errors-for-confidence-intervals-and-p-values)</span>).</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Other code-based web-based tutorials on TMLE that are more recent (or recently discovered by me!) include:</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>David Benkeser and Antoine Chambaz' <span class="co">[</span><span class="ot">A Ride in Targeted Learning Territory</span><span class="co">](https://achambaz.github.io/tlride/)</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>The authors of the <span class="in">`R`</span> package suite <span class="co">[</span><span class="ot">`tlverse`</span><span class="co">](https://tlverse.org/)</span>'s <span class="co">[</span><span class="ot">Hitchhiker's Guide to Targeted Learning: The TMLE Framework</span><span class="co">](https://tlverse.org/tlverse-handbook/tmle3.html)</span>.</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>Stitch Fix's Jasmine Nettiksimmons AND Molly Davies' blog post: <span class="co">[</span><span class="ot">"Gimme a robust estimator - and make it a double!"</span><span class="co">](https://multithreaded.stitchfix.com/blog/2021/07/23/double-robust-estimator/)</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="fu">### Semiparametric Theory and Influence Functions</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Edward Kennedy has several well-written pieces on semiparametric estimation in causal inference. I recommend starting with:</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>His introductory paper on <span class="co">[</span><span class="ot">Semiparametric Theory</span><span class="co">](https://arxiv.org/pdf/1709.06418.pdf)</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>His <span class="co">[</span><span class="ot">slideshow tutorial</span><span class="co">](https://www.ehkennedy.com/uploads/5/8/4/5/58450265/tutorial.pdf)</span> *Nonparametric efficiency theory and machine learning in causal inference*</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>My favorite resource so far for learning specifically about influence functions has been <span class="co">[</span><span class="ot">Visually Communicating Influence Functions</span><span class="co">](https://www.tandfonline.com/doi/full/10.1080/00031305.2020.1717620)</span> by Aaron Fisher and Edward Kennedy. However, this paper didn't make sense to me until I worked through this <span class="co">[</span><span class="ot">interactive tutorial</span><span class="co">](https://observablehq.com/@herbps10/one-step-estimators-and-pathwise-derivatives)</span> by Herb Susmann. I suggest playing around with the interactive examples first, and then trying to work through the paper.</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Additonal walkthroughs to learn about EIFs include the following:</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>Alejandro Schuler's <span class="co">[</span><span class="ot">Modern Causal Inference online book</span><span class="co">](https://alejandroschuler.github.io/mci/index.html)</span> which includes <span class="co">[</span><span class="ot">a section on deriving EIFs</span><span class="co">](https://alejandroschuler.github.io/mci/0cb2ffe5e5cc4cf59a5fe6d896d221d1.html)</span>. His tutorial is a similar, shorter version of the next two resources:</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>Oliver Hines et al.'s <span class="co">[</span><span class="ot">Demystifying statistical learning based on efficient influence functions</span><span class="co">](https://arxiv.org/abs/2107.00681)</span></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>Edward Kennedy's comprehensive review <span class="co">[</span><span class="ot">Semiparametric Doubly Robust Targeted Double Machine Learning: A Review</span><span class="co">](https://arxiv.org/pdf/2203.06469.pdf)</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>Although I did not personally use these last few when I was initially learning about EIFs (since they came out after I wrote this post in Fall 2020), they seem like great resources and intended to be approachable for beginner-to-intermediate learners.</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - The derivation of the Efficient Influence Function (EIF) in TMLE is in the Appendix of [*Targeted Learning*](https://link.springer.com/book/10.1007/978-1-4419-9782-1). --&gt;</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a><span class="fu">### Causal Inference</span></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>As emphasized in *Part I*, TMLE is an estimation technique which *can* be used for causal inference. If you want to learn about the foundations of causal inference, I suggest two different introductory texts below. Note that these provide fairly different frameworks (notation, descriptions of assumptions) to reach the same conclusions, but both provide useful perspectives.</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span><span class="co">[</span><span class="ot">*Causal Inference in Statistics: A Primer*</span><span class="co">](http://bayes.cs.ucla.edu/PRIMER/)</span> by Judea Pearl. Pearl does not discuss estimation methods, but rather focuses on the assumptions, or identification, side of causal inference. Thus, you will not find TMLE mentioned in this text.</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span><span class="co">[</span><span class="ot">*What If*</span><span class="co">](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/)</span> by Miguel Hernan and James Robins. Notably, Hernan and Robins only discuss parametric estimation methods, so you will also not find TMLE or AIPW in this text.</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>I also think the introductory chapters of the previously mentioned <span class="co">[</span><span class="ot">*Targeted Learning*</span><span class="co">](https://link.springer.com/book/10.1007/978-1-4419-9782-1)</span> book (Chapters 1 and 2) do an excellent job of setting up the "roadmap" of causal inference.</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>I'll continue to update this page with beginner's resources as I discover them.</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>Feedback or clarifications on this post is welcome, either from the new learners of TMLE or experts in causal inference. The best way to reach me is through <span class="co">[</span><span class="ot">email</span><span class="co">](mailto:kathoffman.stats@gmail.com)</span>.</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a><span class="fu"># Acknowledgements</span></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>This tutorial would not have been possible without my colleague Iv√°n D√≠az patiently answering many, many questions on TMLE. I am also very appreciative of Miguel Angel Luque-Fernandez's helpful feedback on the visual guide.</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>Lastly, many thanks to Axel Martin, Nick Williams, Anjile An, Adam Peterson, Alan Wu, and Will Simmons for providing suggestions on various drafts of this art project!</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>