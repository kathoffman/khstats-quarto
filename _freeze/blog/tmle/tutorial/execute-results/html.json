{
  "hash": "93eb6eabb0c88441738575ec4443fc0c",
  "result": {
    "markdown": "---\ntitle: \"An Illustrated Guide to TMLE, Part I: Introduction and Motivation\"\nauthor: \"Katherine Hoffman\"\ndate: 2020-12-10\npage-layout: article\nimage: tmle/parametric_assumptions_comic.jpg\ncategories: [\"statistics\"]\nmath: true\ntags: [\"statistics\",\"causal inference\",\"R\",\"TMLE\",\"Targeted Maximum Likelihood Estimation\",\"introduction to TMLE\",\"targeted learning\"]\ndescription: \" \"\noutput:\n  blogdown::html_page:\n    toc: false\n    toc_depth: 1\n---\n\n\n\n\n> The **introductory post** of a three-part series to help beginners understand Targeted Maximum Likelihood Estimation (TMLE). This section contains a brief overview of the **targeted learning framework** and motivation for **semiparametric estimation methods for inference**, including causal inference.\n\n------------------------------------------------------------------------\n\n# Table of Contents {#table-of-contents}\n\n*This blog post series has three parts:*\n\n### Part I: Motivation\n\n1.  [TMLE in three sentences üéØ](#tmle-in-three-sentences)\n2.  [An Analyst's Motivation for Learning TMLE üë©üèº‚Äçüíª](#an-analysts-motivation-for-learning-tmle)\n3.  [Is TMLE Causal Inference? ü§î](#is-tmle-causal-inference)\n\n### [Part II: Algorithm](https://khstats.com/blog/tmle/tutorial-pt2)\n\n4.  [Why the Visual Guide? üé®](https://khstats.com/blog/tmle/tutorial-pt2/#why-the-visual-guide)\n5.  [TMLE, Step-by-Step üö∂üèΩ](https://khstats.com/blog/tmle/tutorial-pt2/#tmle-step-by-step)\n6.  [Using the `tmle` package üì¶](https://khstats.com/blog/tmle/tutorial-pt2/#using-the-tmle-package)\n\n### [Part III: Evaluation](https://khstats.com/blog/tmle/tutorial-pt3)\n\n7.  [Properties of TMLE üìà](https://khstats.com/blog/tmle/tutorial-pt3/#properties-of-tmle)\n8.  [Why does TMLE work? ‚ú®](https://khstats.com/blog/tmle/tutorial-pt3/#why-does-tmle-work)\n9.  [Resources to learn more ü§ì](https://khstats.com/blog/tmle/tutorial-pt3/#resources-to-learn-more)\n\n------------------------------------------------------------------------\n\n# TMLE in three sentences üéØ {#tmle-in-three-sentences}\n\nTargeted Maximum Likelihood Estimation (TMLE) is a semiparametric estimation framework to **estimate a statistical quantity of interest**. TMLE allows the use of **machine learning** (ML) models which place **minimal assumptions on the distribution of the data**. Unlike estimates normally obtained from ML, the **final TMLE estimate will still have valid standard errors for statistical inference**.\n\n# An Analyst's Motivation for Learning TMLE üë©üèº‚Äçüíª {#an-analysts-motivation-for-learning-tmle}\n\nWhen I graduated with my MS in Biostatistics two years ago, I had a mental framework of statistics and data science that I think is pretty common among new graduates. It went like this:\n\n1.  If the goal is [inference]{style=\"color: #3366ff;\"} (e.g., an effect size with a confidence interval), use an [interpretable, usually parametric, model]{style=\"color: #3366ff;\"} and explain what the coefficients and their standard errors mean.\n\n2.  If the goal is [prediction]{style=\"color: #cc0000;\"}, use [data-adaptive machine learning algorithms]{style=\"color: #cc0000;\"} and then look at performance metrics, with the understanding that standard errors, and sometimes even coefficients, no longer exist.\n\nThis mentality changed drastically when I started learning about semiparametric estimation methods like TMLE in the context of causal inference. I quickly realized two flaws in this mental framework.\n\nFirst, I was thinking about inference backwards: I was choosing a model based on my outcome type (binary, continuous, time-to-event, repeated measures) and then interpreting specific coefficients as my estimates of interest. Yet it makes way more sense to *first* determine the statistical quantity, or **estimand**, that best answers a scientific question, and *then* use the method, or **estimator**, best suited for estimating it. This is the paradigm TMLE is based upon: **we want to build an algorithm, or estimator, targeted to an estimand of interest**.\n\n<!-- <figure> -->\n\n<!-- <img src=\"tmle/estimator.png\" alt=\"Estimator and Estimand\" width=\"90%\"/> -->\n\n<!-- <figcaption>*An estimand is a quantity that answers a scientific question of interest. Once we figure out the estimand, we can build an estimator, or algorithm, to estimate it. Image courtesy of Dr. Laura Hatfield and [diff.healthpolicydatascience.org](https://diff.healthpolicydatascience.org/).*</figcaption> -->\n\n<!-- </figure> -->\n\nSecond, I thought flexible, data-adaptive models we commonly classify as statistical and/or **machine learning** (e.g. LASSO, random forests, gradient boosting, etc.) could only be used for prediction, since they don't have **asymptotic properties for inference** (i.e. standard errors). However, certain **semiparametric estimation methods** like TMLE can actually use these models to **obtain a final estimate that is closer to the target quantity** than would be obtained using classic parametric models (e.g. linear and logistic regression). This is because machine learning models are generally designed to accommodate **large numbers of covariates** with **complex, non-linear relationships**.\n\n<img src=\"tmle/parametric_assumptions_comic.jpg\" width=\"100%\"/>\n\n<figcaption>*Semiparametric estimation methods like TMLE can rely on machine learning to avoid making unrealistic parametric assumptions about the underlying distribution of the data (e.g. multivariate normality).*</figcaption>\n\nThe way we use the machine learning estimates in TMLE, surprisingly enough, yields **known asymptotic properties of bias and variance** -- just like we see in parametric maximum likelihood estimation -- for our target estimand.\n\nBesides allowing us to compute 95% confidence intervals and p-values for our estimates even after using flexible models, TMLE achieves other beneficial statistical properties, such as **double robustness**. These are discussed further in [*Part III*](https://khstats.com/blog/tmle/tutorial-pt3/).\n\n# Is TMLE Causal Inference? ü§î {#is-tmle-causal-inference}\n\nIf you've heard about TMLE before, it was likely in the context of **causal inference**. Although TMLE was developed for causal inference due to its many attractive properties, it cannot be considered causal inference by itself. Causal inference is a two-step process that first requires **causal assumptions**[^1] before a statistical estimand can be interpreted causally.\n\n[^1]: I won't discuss causal assumptions in these posts, but this is referring to fundamental assumptions in causal inference like consistency, exchangeability, and positivity. A primary motivation for using TMLE and other semiparametric estimation methods for causal inference is that if you've already taken the time to carefully evaluate *causal* assumptions, it does not make sense to then damage an otherwise well-designed analysis by making unrealistic *statistical* assumptions.\n\n**TMLE can be used to estimate various statistical estimands** (odds ratio, risk ratio, mean outcome difference, etc.) **even when causal assumptions are not met**. TMLE is, as its name implies, simply a tool for estimation.\n\n![](/img/ident-vs-est.png)\n\nIn [*Part II*](https://khstats.com/blog/tmle/tutorial-pt2/), I'll walk step-by-step through a basic version of the TMLE algorithm: estimating the mean difference in outcomes, adjusted for confounders, for a binary outcome and binary treatment. If causal assumptions are met, this is called the **Average Treatment Effect (ATE)**, or the mean difference in outcomes in a world in which everyone had received the treatment compared to a world in which everyone had not.\n\n‚§¥Ô∏è[*Back to the top*](#table-of-contents)\n\n‚û°Ô∏è[*Continue to Part II: The Algorithm*](https://khstats.com/blog/tmle/tutorial-pt2/)\n\n------------------------------------------------------------------------\n\n### *References*\n\nMy primary reference for all three posts is [*Targeted Learning*](https://link.springer.com/book/10.1007/978-1-4419-9782-1) by Mark van der Laan and Sherri Rose. I detail many other resources I've used to learn TMLE, semiparametric theory, and causal inference in [*Part III*](https://khstats.com/blog/tmle/tutorial-pt3/).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}